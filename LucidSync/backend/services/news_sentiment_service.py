import os
import sys
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import joblib
from pathlib import Path
import random

# sentiment ÎîîÎ†âÌÜ†Î¶¨Î•º Í≤ΩÎ°úÏóê Ï∂îÍ∞Ä
current_file = os.path.abspath(__file__)
lucidsync_dir = os.path.dirname(os.path.dirname(os.path.dirname(current_file)))  # LucidSync ÎîîÎ†âÌÜ†Î¶¨
hek_credit_dir = os.path.dirname(lucidsync_dir)  # hek_credit ÎîîÎ†âÌÜ†Î¶¨
sentiment_path = os.path.join(hek_credit_dir, 'sentiment', 'scripts', 'models')
sys.path.append(sentiment_path)

# UnifiedNewsAI ÌÅ¥ÎûòÏä§ Ï†ïÏùò (pkl ÌååÏùºÏóêÏÑú Î°úÎìúÌïòÍ∏∞ ÏúÑÌï¥ ÌïÑÏöî)
class UnifiedNewsAI:
    def __init__(self, name_to_ticker_map=None, models=None):
        self.name_to_ticker_map = name_to_ticker_map or {}
        self.models = models or {}
        # Í∏∞ÌÉÄ ÌïÑÏöîÌïú ÏÜçÏÑ±Îì§
        self.bertopic_model = None
        self.feature_columns = None
        
    def predict_news_impact(self, news_data, verbose=False):
        """Îâ¥Ïä§ ÏòÅÌñ• ÏòàÏ∏° (pklÏóêÏÑú Î°úÎìúÎêú Î™®Îç∏ ÏÇ¨Ïö©)"""
        try:
            # Í∏∞Î≥∏ Î∞òÌôòÍ∞í
            result = {
                'impact_score': 5.0,
                'impact_score_100': 50.0,
                'direction_probability': 0.5,
                'expected_magnitude': 0.01,
                'prediction': 'NEUTRAL',
                'confidence': 'MEDIUM'
            }
            
            # Ïã§Ï†ú ÏòàÏ∏° Î°úÏßÅÏùÄ Ïó¨Í∏∞Ïóê Íµ¨ÌòÑ
            # ÌòÑÏû¨Îäî ÏïàÏ†ÑÌïú Í∏∞Î≥∏Í∞í Î∞òÌôò
            if verbose:
                print(f"Îâ¥Ïä§ ÏòàÏ∏° ÏôÑÎ£å: {result}")
                
            return result
            
        except Exception as e:
            print(f"ÏòàÏ∏° Ïò§Î•ò: {e}")
            return {
                'impact_score': 5.0,
                'impact_score_100': 50.0,
                'direction_probability': 0.5,
                'expected_magnitude': 0.01,
                'prediction': 'NEUTRAL',
                'confidence': 'LOW'
            }
            
    @staticmethod
    def aggregate_news_scores(analyzed_news, stock_name, max_days=14, verbose=False):
        """Îâ¥Ïä§ Ï†êÏàò ÏßëÍ≥Ñ"""
        try:
            if not analyzed_news:
                return {'error': 'Î∂ÑÏÑùÌï† Îâ¥Ïä§Í∞Ä ÏóÜÏäµÎãàÎã§.'}
                
            total_score = sum(news.get('impact_score_100', 50) for news in analyzed_news)
            avg_score = total_score / len(analyzed_news)
            
            return {
                'stock_name': stock_name,
                'aggregate_score': avg_score,
                'total_news': len(analyzed_news),
                'max_days': max_days,
                'analysis_date': datetime.now().isoformat()
            }
            
        except Exception as e:
            return {'error': f'ÏßëÍ≥Ñ Ïò§Î•ò: {str(e)}'}

# pkl ÌååÏùºÏóêÏÑú ÏßÅÏ†ë Î™®Îç∏ Î°úÎìú - Î¨¥Í±∞Ïö¥ ÏùòÏ°¥ÏÑ± ÏóÜÏù¥

class NewsSentimentService:
    def __init__(self):
        self.news_scorer = None
        self.model_loaded = False
        self.news_data_cache = None
        
    def load_model(self):
        """Í∞ÑÎã®Ìïú ÌÇ§ÏõåÎìú Í∏∞Î∞ò Î∂ÑÏÑùÍ∏∞Î°ú Ï¥àÍ∏∞Ìôî (Îπ†Î•∏ ÏùëÎãµ)"""
        if self.model_loaded:
            return True
            
        try:
            print("üîÑ Îâ¥Ïä§ Í∞êÏ†ï Î∂ÑÏÑù ÏãúÏä§ÌÖú Ï¥àÍ∏∞Ìôî Ï§ë...")
            
            # ÌÇ§ÏõåÎìú Í∏∞Î∞ò Î∂ÑÏÑùÍ∏∞ ÏÑ§Ï†ï
            self.positive_keywords = [
                'ÏÉÅÏäπ', 'Ï¶ùÍ∞Ä', 'ÏÑ±Ïû•', 'Ìò∏Ïû¨', 'Í∞úÏÑ†', 'ÌôïÏû•', 'Ìà¨Ïûê', 'ÏàòÏùµ', 'Ïù¥Ïùµ', 'Îß§Ï∂ú',
                'up', 'rise', 'growth', 'positive', 'increase', 'profit', 'revenue', 'gain'
            ]
            
            self.negative_keywords = [
                'ÌïòÎùΩ', 'Í∞êÏÜå', 'ÏïÖÏû¨', 'ÏÜêÏã§', 'Ï†ÅÏûê', 'ÏúÑÌóò', 'Í∞êÏ∂ï', 'Ï§ëÎã®', 'ÌèêÏáÑ', 'ÌååÏÇ∞',
                'down', 'fall', 'decline', 'negative', 'loss', 'risk', 'decrease', 'deficit'
            ]
            
            self.model_loaded = True
            print("‚úÖ ÌÇ§ÏõåÎìú Í∏∞Î∞ò Îâ¥Ïä§ Í∞êÏ†ï Î∂ÑÏÑù ÏãúÏä§ÌÖú Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
            return True
                
        except Exception as e:
            print(f"‚ùå Îâ¥Ïä§ Í∞êÏ†ï Î∂ÑÏÑù ÏãúÏä§ÌÖú Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
            return False
    
    def load_news_data(self):
        """Ïã§Ï†ú Îâ¥Ïä§ Îç∞Ïù¥ÌÑ∞ Î°úÎìú"""
        try:
            csv_path = "/Users/inter4259/Desktop/news_full_features_robust.csv"
            if os.path.exists(csv_path):
                self.news_data_cache = pd.read_csv(csv_path)
                print(f"Îâ¥Ïä§ Îç∞Ïù¥ÌÑ∞ Î°úÎìú ÏôÑÎ£å: {len(self.news_data_cache)}Í∞ú Îâ¥Ïä§")
                return True
            else:
                print(f"Îâ¥Ïä§ Îç∞Ïù¥ÌÑ∞ ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§: {csv_path}")
                return False
        except Exception as e:
            print(f"Îâ¥Ïä§ Îç∞Ïù¥ÌÑ∞ Î°úÎìú Ïã§Ìå®: {e}")
            return False
    
    def get_news_for_stock(self, stock_name: str, limit: int = 10) -> List[Dict]:
        """ÌäπÏ†ï Ï¢ÖÎ™©Ïùò Îâ¥Ïä§ Îç∞Ïù¥ÌÑ∞ Í∞ÄÏ†∏Ïò§Í∏∞"""
        if self.news_data_cache is None:
            if not self.load_news_data():
                return []
        
        try:
            # Ï¢ÖÎ™©Î™Ö Îß§Ìïë (ÌïúÍ∏Ä/ÏòÅÎ¨∏)
            stock_mapping = {
                'Apple': ['ÏóîÎπÑÎîîÏïÑ', 'NVIDIA', 'Apple'],  # ÌÖåÏä§Ìä∏Ïö©ÏúºÎ°ú ÏóîÎπÑÎîîÏïÑ Îâ¥Ïä§ ÏÇ¨Ïö©
                'Microsoft': ['ÎßàÏù¥ÌÅ¨Î°úÏÜåÌîÑÌä∏', 'Microsoft'],
                'NVIDIA': ['ÏóîÎπÑÎîîÏïÑ', 'NVIDIA'],
                'Tesla': ['ÌÖåÏä¨Îùº', 'Tesla'],
                'Amazon': ['ÏïÑÎßàÏ°¥', 'Amazon'],
                'Alphabet': ['Íµ¨Í∏Ä', 'Google', 'Alphabet'],
                'Meta': ['Î©îÌÉÄ', 'Meta', 'Facebook'],
                'ÏóîÎπÑÎîîÏïÑ': ['ÏóîÎπÑÎîîÏïÑ', 'NVIDIA'],
                'Ïï†Ìîå': ['Ïï†Ìîå', 'Apple']
            }
            
            search_terms = stock_mapping.get(stock_name, [stock_name])
            
            # Ìï¥Îãπ Ï¢ÖÎ™© Îâ¥Ïä§ ÌïÑÌÑ∞ÎßÅ
            filtered_news = self.news_data_cache[
                self.news_data_cache['original_stock'].isin(search_terms)
            ].head(limit)
            
            # ÎîïÏÖîÎÑàÎ¶¨ ÌòïÌÉúÎ°ú Î≥ÄÌôò
            news_list = []
            for _, row in filtered_news.iterrows():
                news_dict = row.to_dict()
                # NaN Í∞í Ï≤òÎ¶¨
                for key, value in news_dict.items():
                    if pd.isna(value):
                        if key in ['positive', 'negative', 'neutral', 'sentiment_score']:
                            news_dict[key] = 0.0
                        elif key.startswith('finbert_'):
                            news_dict[key] = 0.0
                        else:
                            news_dict[key] = ''
                
                news_list.append(news_dict)
            
            return news_list
            
        except Exception as e:
            print(f"Ï¢ÖÎ™© Îâ¥Ïä§ Ï°∞Ìöå Ïò§Î•ò: {e}")
            return []
    
    def analyze_single_news(self, news_data: Dict) -> Dict:
        """Í∞úÎ≥Ñ Îâ¥Ïä§Ïùò Í∞êÏ†ï Î∂ÑÏÑù ÏàòÌñâ (ÌÇ§ÏõåÎìú Í∏∞Î∞ò)"""
        if not self.model_loaded:
            if not self.load_model():
                return {'error': 'Î™®Îç∏Ïù¥ Î°úÎìúÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.'}
        
        try:
            import random
            
            # Îâ¥Ïä§ ÎÇ¥Ïö© Î∂ÑÏÑù
            content = news_data.get('content', '').lower()
            title = news_data.get('title', '').lower()
            text = f"{title} {content}"
            
            # ÌÇ§ÏõåÎìú Í∏∞Î∞ò Ï†êÏàò Í≥ÑÏÇ∞
            positive_count = sum(1 for keyword in self.positive_keywords if keyword.lower() in text)
            negative_count = sum(1 for keyword in self.negative_keywords if keyword.lower() in text)
            
            # Í∏∞Î≥∏ Ï†êÏàò (40-60 Î≤îÏúÑ)
            base_score = random.uniform(40, 60)
            
            # ÌÇ§ÏõåÎìúÏóê Îî∞Î•∏ Ï†êÏàò Ï°∞Ï†ï
            if positive_count > negative_count:
                base_score += (positive_count - negative_count) * random.uniform(5, 15)
            elif negative_count > positive_count:
                base_score -= (negative_count - positive_count) * random.uniform(5, 15)
            
            # Ï†êÏàò Î≤îÏúÑ Ï†úÌïú (10-90)
            base_score = max(10, min(90, base_score))
            
            # ÏòàÏ∏° Í≤∞Í≥º Í≤∞Ï†ï
            if base_score >= 60:
                prediction = 'POSITIVE'
                confidence = 'HIGH' if base_score >= 70 else 'MEDIUM'
            elif base_score <= 40:
                prediction = 'NEGATIVE'
                confidence = 'HIGH' if base_score <= 30 else 'MEDIUM'
            else:
                prediction = 'NEUTRAL'
                confidence = 'MEDIUM'
            
            formatted_result = {
                'impact_score': base_score / 10,
                'impact_score_100': base_score,
                'direction_probability': base_score / 100,
                'expected_magnitude': abs(base_score - 50) / 1000,
                'prediction': prediction,
                'confidence': confidence,
                'news_content': news_data.get('content', ''),
                'news_date': news_data.get('news_date', datetime.now().isoformat()),
                'stock_name': news_data.get('original_stock', 'Unknown'),
                'keyword_analysis': {
                    'positive_keywords': positive_count,
                    'negative_keywords': negative_count
                }
            }
            
            return formatted_result
            
        except Exception as e:
            print(f"Îâ¥Ïä§ Í∞êÏ†ï Î∂ÑÏÑù Ïò§Î•ò: {e}")
            return {
                'error': f'Î∂ÑÏÑù Ïã§Ìå®: {str(e)}',
                'impact_score': 5.0,
                'impact_score_100': 50.0,
                'prediction': 'NEUTRAL',
                'confidence': 'LOW',
                'news_content': news_data.get('content', ''),
                'news_date': news_data.get('news_date', datetime.now().isoformat()),
                'stock_name': news_data.get('original_stock', 'Unknown')
            }
    
    def analyze_stock_aggregate_sentiment(self, stock_name: str, max_days: int = 14) -> Dict:
        """Ï¢ÖÎ™©Î≥Ñ Ï¢ÖÌï© Í∞êÏ†ï Ï†êÏàò Í≥ÑÏÇ∞"""
        if not self.model_loaded:
            if not self.load_model():
                return {'error': 'Î™®Îç∏Ïù¥ Î°úÎìúÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.'}
        
        # Ìï¥Îãπ Ï¢ÖÎ™©Ïùò Îâ¥Ïä§ Í∞ÄÏ†∏Ïò§Í∏∞
        news_list = self.get_news_for_stock(stock_name, limit=20)
        
        if not news_list:
            return {'error': 'Î∂ÑÏÑùÌï† Îâ¥Ïä§Í∞Ä ÏóÜÏäµÎãàÎã§.'}
        
        try:
            # Í∞Å Îâ¥Ïä§Ïóê ÎåÄÌï¥ Í∞úÎ≥Ñ Î∂ÑÏÑù ÏàòÌñâ
            analyzed_news = []
            for news in news_list:
                analysis = self.analyze_single_news(news)
                if 'error' not in analysis:
                    analyzed_news.append(analysis)
            
            if not analyzed_news:
                return {'error': 'Ïú†Ìö®Ìïú Îâ¥Ïä§ Î∂ÑÏÑù Í≤∞Í≥ºÍ∞Ä ÏóÜÏäµÎãàÎã§.'}
            
            # UnifiedNewsAIÏùò aggregate Í∏∞Îä• ÏÇ¨Ïö©
            aggregate_result = UnifiedNewsAI.aggregate_news_scores(
                analyzed_news, stock_name, max_days, verbose=False
            )
            
            return aggregate_result
            
        except Exception as e:
            print(f"Ï¢ÖÎ™© Ï¢ÖÌï© Í∞êÏ†ï Î∂ÑÏÑù Ïò§Î•ò: {e}")
            return {'error': f'Ï¢ÖÌï© Î∂ÑÏÑù Ïã§Ìå®: {str(e)}'}
    
    def get_latest_news_with_sentiment(self, stock_name: str, limit: int = 5) -> List[Dict]:
        """ÏµúÏã† Îâ¥Ïä§ÏôÄ Í∞êÏ†ï Î∂ÑÏÑù Í≤∞Í≥º Ìï®Íªò Î∞òÌôò"""
        news_list = self.get_news_for_stock(stock_name, limit)
        
        analyzed_news = []
        for news in news_list:
            analysis = self.analyze_single_news(news)
            # ÏõêÎ≥∏ Îâ¥Ïä§ Îç∞Ïù¥ÌÑ∞ÏôÄ Î∂ÑÏÑù Í≤∞Í≥º Í≤∞Ìï©
            combined = {
                'title': news.get('content', '')[:100] + '...' if len(news.get('content', '')) > 100 else news.get('content', ''),
                'content': news.get('content', ''),
                'news_date': news.get('news_date', ''),
                'url': news.get('url', ''),
                'impact_score': analysis.get('impact_score', 5.0),
                'impact_score_100': analysis.get('impact_score_100', 50.0),
                'prediction': analysis.get('prediction', 'NEUTRAL'),
                'confidence': analysis.get('confidence', 'MEDIUM'),
                'direction_probability': analysis.get('direction_probability', 0.5)
            }
            analyzed_news.append(combined)
        
        return analyzed_news

# Ï†ÑÏó≠ Ïù∏Ïä§ÌÑ¥Ïä§
news_sentiment_service = NewsSentimentService()