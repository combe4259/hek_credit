"""
2ë‹¨ê³„: BERT ì„ë² ë”©ì´ ì¶”ê°€ëœ íŒŒì¼ì— yfinance í”¼ì²˜ ì¶”ê°€ (ì™„ì „ ìˆ˜ì • ë²„ì „)
- FlexibleTechnicalScorer ì‚¬ìš©ìœ¼ë¡œ ë°ì´í„° ë¶€ì¡± ë¬¸ì œ í•´ê²°
- ì‹œê°„ëŒ€ ë¬¸ì œ ì™„ì „ í•´ê²°
- ë§¤í•‘ëœ í‹°ì»¤ ìºì‹œ ëˆ„ë½ ë¬¸ì œ í•´ê²°
- ìƒì„¸ ë””ë²„ê¹… ë¡œê·¸ë¡œ ë¬¸ì œì  ì •í™•íˆ íŒŒì•…
"""
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import yfinance as yf
from tqdm import tqdm
import time
import sys
import os
import joblib
from pandas.tseries.offsets import BDay

# Google Drive ë§ˆìš´íŠ¸ ë° ê²½ë¡œ ì„¤ì •
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

# FlexibleTechnicalScorer í´ë˜ìŠ¤ (ì‹œê°„ëŒ€ ë¬¸ì œ ì™„ì „ í•´ê²°)
class FlexibleTechnicalScorer:
    def __init__(self, data_cache=None):
        self.cached_data = data_cache if data_cache is not None else {}
        self.ticker_mapping = {'BRK': 'BRK-B', 'BF': 'BF-B'}
        self.weights = {'price_momentum': 0.60, 'volume_surge': 0.40}
        print("ğŸ¦ ìœ ì—°í•œ ê¸°ìˆ ì  ë¶„ì„ ì‹œìŠ¤í…œ (v2.5 - ì‹œê°„ëŒ€ ë¬¸ì œ í•´ê²°) ë¡œë”© ì™„ë£Œ")

    def _map_ticker(self, ticker: str) -> str:
        return self.ticker_mapping.get(ticker, ticker)

    def _get_stock_data(self, ticker: str, start_date: datetime, end_date: datetime) -> pd.DataFrame:
        """ì‹œê°„ëŒ€ ë¬¸ì œë¥¼ ì™„ì „íˆ í•´ê²°í•œ ë°ì´í„° ì¶”ì¶œ"""
        if ticker in self.cached_data:
            hist = self.cached_data[ticker]
            try:
                # ğŸ”§ ê°•ë ¥í•œ ì‹œê°„ëŒ€ ì²˜ë¦¬

                # 1. ìºì‹œ ë°ì´í„° ì‹œê°„ëŒ€ ì²˜ë¦¬
                if hist.index.tz is not None:
                    # ìºì‹œ ë°ì´í„°ê°€ tz-awareë©´ tz-naiveë¡œ ë³€í™˜
                    hist_index = hist.index.tz_convert('UTC').tz_localize(None)
                    hist_clean = hist.copy()
                    hist_clean.index = hist_index
                else:
                    hist_clean = hist

                # 2. ì…ë ¥ ë‚ ì§œë„ tz-naiveë¡œ í†µì¼
                start_clean = start_date
                end_clean = end_date

                if start_date.tzinfo is not None:
                    start_clean = start_date.tz_convert('UTC').tz_localize(None)
                if end_date.tzinfo is not None:
                    end_clean = end_date.tz_convert('UTC').tz_localize(None)

                # 3. ì•ˆì „í•œ í•„í„°ë§ (boolean indexing ì‚¬ìš©)
                mask = (hist_clean.index >= start_clean) & (hist_clean.index <= end_clean)
                filtered_data = hist_clean[mask]

                return filtered_data

            except Exception as e:
                print(f"âš ï¸ ë°ì´í„° í•„í„°ë§ ì‹¤íŒ¨ ({ticker}): {e}")
                # ìµœí›„ì˜ ìˆ˜ë‹¨: ìµœê·¼ ë°ì´í„° ë°˜í™˜
                try:
                    return hist.tail(min(252, len(hist)))  # ìµœëŒ€ 1ë…„ì¹˜
                except:
                    return pd.DataFrame()

        return pd.DataFrame()

    def _normalize_score(self, raw_score: float) -> float:
        return max(0, min(100, (raw_score + 100) / 2))

    def analyze_price_momentum(self, ticker: str, reference_date=None):
        try:
            mapped_ticker = self._map_ticker(ticker)
            end_date = reference_date or datetime.now()
            start_date = end_date - timedelta(days=365)

            daily_hist = self._get_stock_data(mapped_ticker, start_date, end_date)

            if len(daily_hist) < 10:
                return 50, {'error': f'ë°ì´í„° ë„ˆë¬´ ë¶€ì¡± ({mapped_ticker}, {len(daily_hist)}ì¼)', 'data_days': len(daily_hist)}

            score = 0
            analysis_details = {'factors': [], 'data_days': len(daily_hist)}

            current_price = daily_hist['Close'].iloc[-1]
            if current_price <= 0:
                return 50, {'error': 'í˜„ì¬ê°€ê°€ 0 ì´í•˜'}

            # ë‹¨ê³„ë³„ ë¶„ì„ (ë°ì´í„° ì–‘ì— ë”°ë¼ ì ì‘ì ìœ¼ë¡œ)

            # 1ì£¼ì¼ ìˆ˜ìµë¥  (ìµœì†Œ 5ì¼ í•„ìš”)
            if len(daily_hist) >= 5:
                week_start_idx = max(0, len(daily_hist) - 6)
                week_ago_price = daily_hist['Close'].iloc[week_start_idx]
                if week_ago_price > 0:
                    week_return = (current_price - week_ago_price) / week_ago_price
                    if week_return >= 0.05:
                        score += 15
                        analysis_details['factors'].append(f"ì£¼ê°„ ìƒìŠ¹ {week_return:.1%}")
                    elif week_return <= -0.05:
                        score -= 15
                        analysis_details['factors'].append(f"ì£¼ê°„ í•˜ë½ {week_return:.1%}")

            # 1ê°œì›” ìˆ˜ìµë¥  (ìµœì†Œ 15ì¼ í•„ìš”)
            if len(daily_hist) >= 15:
                month_start_idx = max(0, len(daily_hist) - 21)
                month_ago_price = daily_hist['Close'].iloc[month_start_idx]
                if month_ago_price > 0:
                    month_return = (current_price - month_ago_price) / month_ago_price
                    if month_return >= 0.15:
                        score += 25
                        analysis_details['factors'].append(f"ì›”ê°„ ê°•í•œ ìƒìŠ¹ {month_return:.1%}")
                    elif month_return >= 0.05:
                        score += 10
                        analysis_details['factors'].append(f"ì›”ê°„ ìƒìŠ¹ {month_return:.1%}")
                    elif month_return <= -0.15:
                        score -= 25
                        analysis_details['factors'].append(f"ì›”ê°„ ê°•í•œ í•˜ë½ {month_return:.1%}")
                    elif month_return <= -0.05:
                        score -= 10
                        analysis_details['factors'].append(f"ì›”ê°„ í•˜ë½ {month_return:.1%}")

            # 3ê°œì›” ìˆ˜ìµë¥  (ìµœì†Œ 40ì¼ í•„ìš”)
            if len(daily_hist) >= 40:
                quarter_start_idx = max(0, len(daily_hist) - 63)
                quarter_ago_price = daily_hist['Close'].iloc[quarter_start_idx]
                if quarter_ago_price > 0:
                    quarter_return = (current_price - quarter_ago_price) / quarter_ago_price
                    if quarter_return >= 0.30:
                        score += 30
                        analysis_details['factors'].append(f"ë¶„ê¸° ëŒ€í­ ìƒìŠ¹ {quarter_return:.1%}")
                    elif quarter_return <= -0.30:
                        score -= 30
                        analysis_details['factors'].append(f"ë¶„ê¸° ëŒ€í­ í•˜ë½ {quarter_return:.1%}")

            # ì¥ê¸° ìˆ˜ìµë¥  (ìµœì†Œ 100ì¼ í•„ìš”)
            if len(daily_hist) >= 100:
                long_start_idx = max(0, len(daily_hist) - 252)
                long_ago_price = daily_hist['Close'].iloc[long_start_idx]
                if long_ago_price > 0:
                    long_return = (current_price - long_ago_price) / long_ago_price
                    actual_days = len(daily_hist) - long_start_idx
                    analysis_details['factors'].append(f"{actual_days}ì¼ê°„ ìˆ˜ìµë¥  {long_return:.1%}")

                    if long_return >= 0.50:
                        score += 30
                    elif long_return >= 0.20:
                        score += 15
                    elif long_return <= -0.50:
                        score -= 30
                    elif long_return <= -0.20:
                        score -= 15

            # ë°ì´í„° ë¶€ì¡± í‘œì‹œ (í˜ë„í‹° ì—†ìŒ)
            if len(daily_hist) < 50:
                analysis_details['factors'].append(f"ì œí•œëœ ë°ì´í„° ({len(daily_hist)}ì¼)")

            normalized_score = self._normalize_score(score)
            return normalized_score, analysis_details

        except Exception as e:
            return 50, {'error': str(e)}

    def analyze_volume_surge(self, ticker: str, reference_date=None):
        try:
            mapped_ticker = self._map_ticker(ticker)
            end_date = reference_date or datetime.now()
            start_date = end_date - timedelta(days=60)

            hist = self._get_stock_data(mapped_ticker, start_date, end_date)

            if len(hist) < 5:
                return 50, {'error': f'ê±°ë˜ëŸ‰ ë°ì´í„° ë¶€ì¡± ({len(hist)}ì¼)', 'data_days': len(hist)}

            analysis_details = {'factors': [], 'data_days': len(hist)}
            current_volume = hist['Volume'].iloc[-1]

            if current_volume <= 0:
                return 50, {'error': 'í˜„ì¬ ê±°ë˜ëŸ‰ì´ 0'}

            score = 0

            # ë‹¨ê¸° ê±°ë˜ëŸ‰ ë¹„êµ (ìµœì†Œ 5ì¼)
            if len(hist) >= 5:
                recent_avg = hist['Volume'].iloc[-5:].mean()
                if recent_avg > 0:
                    short_ratio = current_volume / recent_avg
                    if short_ratio >= 2.0:
                        score += 25
                        analysis_details['factors'].append(f"5ì¼ í‰ê·  ëŒ€ë¹„ {short_ratio:.1f}ë°°")

            # ì¤‘ê¸° ê±°ë˜ëŸ‰ ë¹„êµ (ìµœì†Œ 15ì¼)
            if len(hist) >= 15:
                mid_avg = hist['Volume'].iloc[-15:].mean()
                if mid_avg > 0:
                    mid_ratio = current_volume / mid_avg
                    if mid_ratio >= 3.0:
                        score += 35
                        analysis_details['factors'].append(f"15ì¼ í‰ê·  ëŒ€ë¹„ {mid_ratio:.1f}ë°°")
                    elif mid_ratio >= 1.5:
                        score += 15
                        analysis_details['factors'].append(f"15ì¼ í‰ê·  ëŒ€ë¹„ {mid_ratio:.1f}ë°°")

            # ì¥ê¸° ê±°ë˜ëŸ‰ ë¹„êµ (ìµœì†Œ 30ì¼)
            if len(hist) >= 30:
                long_avg = hist['Volume'].iloc[-30:].mean()
                if long_avg > 0:
                    long_ratio = current_volume / long_avg
                    if long_ratio >= 5.0:
                        score += 40
                        analysis_details['factors'].append(f"30ì¼ í‰ê·  ëŒ€ë¹„ {long_ratio:.1f}ë°° ê¸‰ì¦")

            # ê±°ë˜ëŸ‰ ê¸‰ê° ê°ì§€
            if len(hist) >= 10:
                recent_avg = hist['Volume'].iloc[-10:].mean()
                if recent_avg > 0 and current_volume < recent_avg * 0.3:
                    score -= 10
                    analysis_details['factors'].append("ê±°ë˜ëŸ‰ ê¸‰ê°")

            normalized_score = self._normalize_score(score)
            return normalized_score, analysis_details

        except Exception as e:
            return 50, {'error': str(e)}

    def calculate_total_score(self, ticker: str, reference_date=None):
        """ì´ì  ê³„ì‚° ë° ìƒì„¸ ì •ë³´ ì œê³µ"""
        momentum_score, momentum_details = self.analyze_price_momentum(ticker, reference_date)
        volume_score, volume_details = self.analyze_volume_surge(ticker, reference_date)

        total_score = (
            momentum_score * self.weights['price_momentum'] +
            volume_score * self.weights['volume_surge']
        )

        # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
        if total_score >= 80: category = "ê°•ë ¥í˜¸ì¬"
        elif total_score >= 60: category = "í˜¸ì¬"
        elif total_score >= 40: category = "ì¤‘ë¦½"
        elif total_score >= 20: category = "ì•…ì¬"
        else: category = "ê°•ë ¥ì•…ì¬"

        return {
            'ticker': ticker,
            'total_score': round(total_score, 1),
            'category': category,
            'component_scores': {
                'price_momentum': round(momentum_score, 1),
                'volume_surge': round(volume_score, 1),
            },
            'details': {
                'momentum': momentum_details,
                'volume': volume_details,
            }
        }

# --- ë©”ì¸ í•¨ìˆ˜ë“¤ ---

def update_cache_with_mappings():
    """ìºì‹œì— ë§¤í•‘ëœ í‹°ì»¤ ì¶”ê°€ (BRK â†’ BRK-B, BF â†’ BF-B)"""
    cache_path = "/content/drive/MyDrive/yfinance_data_cache.joblib"

    if not os.path.exists(cache_path):
        print("âŒ ìºì‹œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    try:
        cache = joblib.load(cache_path)
        mappings = {'BRK': 'BRK-B', 'BF': 'BF-B'}

        updated = False
        for original, mapped in mappings.items():
            if original in cache and mapped not in cache:
                cache[mapped] = cache[original].copy()  # ë°ì´í„° ë³µì‚¬
                print(f"âœ… ë§¤í•‘ ì¶”ê°€: {original} â†’ {mapped}")
                updated = True

        if updated:
            joblib.dump(cache, cache_path)
            print(f"ğŸ’¾ ìºì‹œ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {len(cache)}ê°œ í‹°ì»¤")
        else:
            print("â„¹ï¸ ë§¤í•‘í•  í‹°ì»¤ê°€ ì—†ê±°ë‚˜ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")

    except Exception as e:
        print(f"âŒ ìºì‹œ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

def fetch_and_cache_yfinance_data(min_date, max_date, return_horizons):
    """ìºì‹œ íŒŒì¼ì„ ìš°ì„  ë¡œë“œí•˜ê³ , SPY ì‹œì¥ ë°ì´í„°ëŠ” í•­ìƒ ìµœì‹ ìœ¼ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    print("ğŸ“ˆ yfinance ë°ì´í„° ìºì‹± ë° ë¡œë“œ ì‹œì‘...")

    cache_path = "/content/drive/MyDrive/yfinance_data_cache.joblib"

    # ìºì‹œ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ ìƒì„¸ í™•ì¸
    print(f"ğŸ” ìºì‹œ íŒŒì¼ ê²½ë¡œ í™•ì¸: {cache_path}")
    print(f"   íŒŒì¼ ì¡´ì¬ ì—¬ë¶€: {os.path.exists(cache_path)}")

    if os.path.exists(cache_path):
        try:
            print("â³ ìºì‹œ íŒŒì¼ ë¡œë”© ì¤‘...")
            ticker_data_cache = joblib.load(cache_path)
            print(f"âœ… ìºì‹œ íŒŒì¼ì—ì„œ yfinance ë°ì´í„° ë¡œë“œ ì™„ë£Œ! ({len(ticker_data_cache)}ê°œ í‹°ì»¤)")

            # ìºì‹œ ìƒíƒœ ìƒì„¸ ë¶„ì„
            if ticker_data_cache:
                print(f"\nğŸ“Š ìºì‹œ ë°ì´í„° ìƒì„¸ ì •ë³´:")

                # ìƒ˜í”Œ í‹°ì»¤ ì •ë³´
                sample_ticker = list(ticker_data_cache.keys())[0]
                sample_data = ticker_data_cache[sample_ticker]
                print(f"   ğŸ“ˆ ìƒ˜í”Œ í‹°ì»¤ {sample_ticker}:")
                print(f"      - ë°ì´í„° í–‰ ìˆ˜: {len(sample_data)}")
                print(f"      - ë°ì´í„° ê¸°ê°„: {sample_data.index[0].date()} ~ {sample_data.index[-1].date()}")
                print(f"      - ì»¬ëŸ¼: {list(sample_data.columns)}")

                # ì „ì²´ í‹°ì»¤ ëª©ë¡ ìƒ˜í”Œ
                all_tickers = list(ticker_data_cache.keys())
                print(f"   ğŸ¯ ìºì‹œëœ í‹°ì»¤ ìƒ˜í”Œ (ì²˜ìŒ 10ê°œ): {all_tickers[:10]}")

                # ë°ì´í„° ê¸°ê°„ í†µê³„
                date_ranges = []
                for ticker, data in list(ticker_data_cache.items())[:5]:  # ì²˜ìŒ 5ê°œë§Œ ì²´í¬
                    if not data.empty:
                        date_ranges.append((data.index[0], data.index[-1]))

                if date_ranges:
                    earliest_start = min([dr[0] for dr in date_ranges])
                    latest_end = max([dr[1] for dr in date_ranges])
                    print(f"   ğŸ“… ì „ì²´ ë°ì´í„° ê¸°ê°„: {earliest_start.date()} ~ {latest_end.date()}")

                # ë‰´ìŠ¤ ë‚ ì§œì™€ì˜ ë¹„êµ
                print(f"   ğŸ—ï¸  ë‰´ìŠ¤ ë‚ ì§œ ë²”ìœ„: {min_date.date()} ~ {max_date.date()}")

                # ë‚ ì§œ ë²”ìœ„ í˜¸í™˜ì„± ì²´í¬ (ì‹œê°„ëŒ€ ë¬¸ì œ í•´ê²°)
                sample_start = sample_data.index[0]
                sample_end = sample_data.index[-1]

                # ì‹œê°„ëŒ€ ì •ë³´ í™•ì¸ ë° í†µì¼
                print(f"   ğŸ• ì‹œê°„ëŒ€ ì •ë³´:")
                print(f"      - ìºì‹œ ë°ì´í„° ì‹œê°„ëŒ€: {sample_data.index.tz}")
                print(f"      - ë‰´ìŠ¤ ë‚ ì§œ ì‹œê°„ëŒ€: {min_date.tzinfo}")

                # ì‹œê°„ëŒ€ í†µì¼í•˜ì—¬ ë¹„êµ
                try:
                    if sample_data.index.tz is not None:
                        # ìºì‹œê°€ tz-awareì¸ ê²½ìš°, ë‰´ìŠ¤ ë‚ ì§œë¥¼ ê°™ì€ ì‹œê°„ëŒ€ë¡œ ë³€í™˜
                        min_date_tz = min_date.tz_localize(sample_data.index.tz) if min_date.tzinfo is None else min_date.tz_convert(sample_data.index.tz)
                        max_date_tz = max_date.tz_localize(sample_data.index.tz) if max_date.tzinfo is None else max_date.tz_convert(sample_data.index.tz)
                    else:
                        # ìºì‹œê°€ tz-naiveì¸ ê²½ìš°, ê·¸ëŒ€ë¡œ ë¹„êµ
                        min_date_tz = min_date.tz_localize(None) if min_date.tzinfo is not None else min_date
                        max_date_tz = max_date.tz_localize(None) if max_date.tzinfo is not None else max_date

                    news_in_range = sample_start <= max_date_tz and sample_end >= min_date_tz
                    print(f"   âœ… ë‚ ì§œ ë²”ìœ„ í˜¸í™˜ì„±: {news_in_range}")

                except Exception as tz_error:
                    print(f"   âš ï¸ ì‹œê°„ëŒ€ ë¹„êµ ì‹¤íŒ¨: {tz_error}")
                    print(f"   ğŸ“… ìºì‹œ ë²”ìœ„: {sample_start} ~ {sample_end}")
                    print(f"   ğŸ“… ë‰´ìŠ¤ ë²”ìœ„: {min_date} ~ {max_date}")
                    # ì—ëŸ¬ê°€ ìˆì–´ë„ ê³„ì† ì§„í–‰

            else:
                print("âŒ ìºì‹œê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!")
                return None, None

        except Exception as e:
            print(f"âŒ ìºì‹œ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None, None
    else:
        print("âŒ ìºì‹œ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        print("   data_fetcher.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ ìºì‹œë¥¼ ìƒì„±í•˜ì„¸ìš”.")
        return None, None

    # SPY ì‹œì¥ ë°ì´í„° ë¡œë“œ
    market_hist = None
    market_ticker = 'SPY'
    try:
        hist_start_date = min_date - timedelta(days=400)
        market_hist = yf.Ticker(market_ticker).history(
            start=hist_start_date,
            end=max_date + timedelta(days=max(return_horizons) + 15)
        )
        print("âœ… SPY ì‹œì¥ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ SPY ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        market_hist = pd.DataFrame()

    return ticker_data_cache, market_hist

def calculate_technical_scores_with_debug(df, ticker_data_cache, debug_mode=True):
    """ë””ë²„ê¹… ê°•í™”ëœ ê¸°ìˆ ì  ë¶„ì„ ì ìˆ˜ ê³„ì‚°"""
    print("ğŸ“ˆ ê¸°ìˆ ì  ë¶„ì„ ì ìˆ˜ ê³„ì‚° ì¤‘ (FlexibleTechnicalScorer + ë””ë²„ê¹…)...")

    scorer = FlexibleTechnicalScorer(data_cache=ticker_data_cache)

    # ë””ë²„ê¹… ì •ë³´ ìˆ˜ì§‘
    results = []
    success_count = 0
    fail_count = 0
    fail_reasons = {}
    score_distribution = {'50ì ': 0, '50ì _ì™¸': 0}

    # ìºì‹œ ì •ë³´ ì¶œë ¥
    print(f"ğŸ“¦ ìºì‹œ ì •ë³´: {len(ticker_data_cache)}ê°œ í‹°ì»¤")
    print(f"ğŸ“° ë‰´ìŠ¤ ì •ë³´: {len(df)}ê°œ, ë‚ ì§œ ë²”ìœ„: {df['news_date'].min().date()} ~ {df['news_date'].max().date()}")
    print(f"ğŸ¯ ê³ ìœ  í‹°ì»¤: {df['ticker'].nunique()}ê°œ")

    # ì²˜ìŒ 5ê°œ í•­ëª©ë§Œ ìƒì„¸ ë””ë²„ê¹…
    debug_sample_size = 5 if debug_mode else 0

    for idx, row in tqdm(df.iterrows(), total=len(df), desc="ê¸°ìˆ ì  ë¶„ì„"):
        result = row.to_dict()
        ticker = row['ticker']
        news_date = row['news_date']

        is_debug_sample = idx < debug_sample_size

        try:
            # ë‚ ì§œ ë³´ì •
            adjusted_date = news_date - BDay(0)

            if is_debug_sample:
                print(f"\nğŸ” ë””ë²„ê¹… #{idx+1}: {ticker} ({news_date.date()})")
                print(f"   ë³´ì •ëœ ë‚ ì§œ: {adjusted_date.date()}")

            # í‹°ì»¤ ë§¤í•‘ í™•ì¸
            mapped_ticker = scorer._map_ticker(ticker)
            cache_exists = mapped_ticker in ticker_data_cache

            if is_debug_sample:
                print(f"   ë§¤í•‘: {ticker} â†’ {mapped_ticker}")
                print(f"   ìºì‹œ ì¡´ì¬: {cache_exists}")

            if not cache_exists:
                raise ValueError(f"í‹°ì»¤ {mapped_ticker}ê°€ ìºì‹œì— ì—†ìŒ")

            # ë°ì´í„° ê¸°ê°„ í™•ì¸
            hist_data = ticker_data_cache[mapped_ticker]
            hist_start = hist_data.index[0]
            hist_end = hist_data.index[-1]

            if is_debug_sample:
                print(f"   íˆìŠ¤í† ë¦¬: {hist_start.date()} ~ {hist_end.date()} ({len(hist_data)}í–‰)")
                # ì‹œê°„ëŒ€ ì•ˆì „ ë¹„êµëŠ” ìŠ¤í‚µ (FlexibleTechnicalScorerê°€ ì²˜ë¦¬)
                print(f"   ë°ì´í„° ì¶©ë¶„: {len(hist_data) >= 10}")

            # ìŠ¤ì½”ì–´ ê³„ì‚°
            score_data = scorer.calculate_total_score(ticker, reference_date=adjusted_date)

            if is_debug_sample:
                print(f"   ê²°ê³¼: rule={score_data['total_score']:.1f}, momentum={score_data['component_scores']['price_momentum']:.1f}, volume={score_data['component_scores']['volume_surge']:.1f}")
                if 'details' in score_data:
                    if 'momentum' in score_data['details'] and 'factors' in score_data['details']['momentum']:
                        print(f"   ëª¨ë©˜í…€ ìš”ì¸: {score_data['details']['momentum']['factors']}")
                    if 'volume' in score_data['details'] and 'factors' in score_data['details']['volume']:
                        print(f"   ê±°ë˜ëŸ‰ ìš”ì¸: {score_data['details']['volume']['factors']}")

            # ê²°ê³¼ ì €ì¥
            result.update({
                'rule_score': score_data['total_score'],
                'momentum_score': score_data['component_scores']['price_momentum'],
                'volume_score': score_data['component_scores']['volume_surge']
            })

            # ì ìˆ˜ ë¶„í¬ ì²´í¬
            if abs(score_data['total_score'] - 50.0) < 0.1:  # 50ì ì— ê°€ê¹Œìš´ ê²½ìš°
                score_distribution['50ì '] += 1
            else:
                score_distribution['50ì _ì™¸'] += 1

            success_count += 1

        except Exception as e:
            error_msg = str(e)
            fail_count += 1

            if error_msg not in fail_reasons:
                fail_reasons[error_msg] = 0
            fail_reasons[error_msg] += 1

            if is_debug_sample:
                print(f"   âŒ ì‹¤íŒ¨: {error_msg}")

            # NaNìœ¼ë¡œ ì²˜ë¦¬
            result.update({
                'rule_score': np.nan,
                'momentum_score': np.nan,
                'volume_score': np.nan
            })
            score_distribution['50ì '] += 1

        results.append(result)

    # ê²°ê³¼ ìš”ì•½
    print(f"\nğŸ“Š ê¸°ìˆ ì  ë¶„ì„ ê²°ê³¼ ìš”ì•½:")
    print(f"   ì„±ê³µ: {success_count}ê°œ")
    print(f"   ì‹¤íŒ¨: {fail_count}ê°œ")
    print(f"   ì„±ê³µë¥ : {success_count/(success_count+fail_count)*100:.1f}%")
    print(f"\nğŸ“ˆ ì ìˆ˜ ë¶„í¬:")
    print(f"   50ì  (ê¸°ë³¸ê°’): {score_distribution['50ì ']}ê°œ")
    print(f"   ê¸°íƒ€ ì ìˆ˜: {score_distribution['50ì _ì™¸']}ê°œ")

    if fail_reasons:
        print(f"\nâŒ ì£¼ìš” ì‹¤íŒ¨ ì›ì¸:")
        for reason, count in sorted(fail_reasons.items(), key=lambda x: x[1], reverse=True)[:5]:
            print(f"   {reason}: {count}ê±´")

    # âœ… score_distributionì„ ë°˜í™˜í•˜ë„ë¡ ìˆ˜ì •
    return pd.DataFrame(results), score_distribution

def calculate_composite_signal_cached(df, ticker_data_cache, market_hist, return_horizons):
    """ë³µí•© ì‹ í˜¸ ê³„ì‚° (ì‹œê°„ëŒ€ ë¬¸ì œ í•´ê²°)"""
    print("ğŸ¯ ìºì‹±ëœ ë°ì´í„°ë¡œ ë³µí•© ì‹ í˜¸ ê³„ì‚° ì¤‘...")
    results = []

    for idx, row in tqdm(df.iterrows(), total=len(df), desc="ë³µí•© ì‹ í˜¸ ê³„ì‚°"):
        result = row.to_dict()
        ticker = row['ticker']
        news_date = row['news_date']

        adjusted_date = news_date - BDay(0)
        mapped_ticker = {'BRK': 'BRK-B', 'BF': 'BF-B'}.get(ticker, ticker)

        if mapped_ticker in ticker_data_cache and not market_hist.empty:
            hist = ticker_data_cache[mapped_ticker]
            try:
                # ì‹œê°„ëŒ€ í†µì¼
                target_date = adjusted_date

                # ìºì‹œ ë°ì´í„° ì‹œê°„ëŒ€ ì²˜ë¦¬
                if hist.index.tz is not None:
                    if target_date.tzinfo is None:
                        target_date = target_date.tz_localize(hist.index.tz)
                    else:
                        target_date = target_date.tz_convert(hist.index.tz)

                # SPY ë°ì´í„° ì‹œê°„ëŒ€ ì²˜ë¦¬
                market_target_date = target_date
                if market_hist.index.tz is not None:
                    if market_target_date.tzinfo is None:
                        market_target_date = market_target_date.tz_localize(market_hist.index.tz)
                    else:
                        market_target_date = market_target_date.tz_convert(market_hist.index.tz)

                # asofë¥¼ ì‚¬ìš©í•œ ì•ˆì „í•œ ì¸ë±ì‹±
                hist_slice_start_index = hist.index.asof(target_date)
                market_slice_start_index = market_hist.index.asof(market_target_date)

                if pd.isna(hist_slice_start_index) or pd.isna(market_slice_start_index):
                    raise IndexError("Cannot find matching date in historical data")

                relevant_hist = hist.loc[hist_slice_start_index:]
                market_relevant_hist = market_hist.loc[market_slice_start_index:]

                if len(relevant_hist) > max(return_horizons) and len(market_relevant_hist) > max(return_horizons):
                    news_day_close = relevant_hist.iloc[0]['Close']
                    market_news_day_close = market_relevant_hist.iloc[0]['Close']

                    returns = {}
                    for horizon in return_horizons:
                        if len(relevant_hist) > horizon and len(market_relevant_hist) > horizon:
                            future_close = relevant_hist.iloc[horizon]['Close']
                            market_future_close = market_relevant_hist.iloc[horizon]['Close']
                            if news_day_close > 0 and market_news_day_close > 0:
                                stock_return = (future_close - news_day_close) / news_day_close
                                market_return = (market_future_close - market_news_day_close) / market_news_day_close
                                returns[f'relative_return_{horizon}d'] = stock_return - market_return

                    if returns:
                        result['composite_signal'] = np.mean(list(returns.values()))
                    else:
                        result['composite_signal'] = np.nan
                else:
                    result['composite_signal'] = np.nan
            except Exception:
                result['composite_signal'] = np.nan
        else:
            result['composite_signal'] = np.nan
        results.append(result)

    return pd.DataFrame(results)

def main():
    print("ğŸš€ 2ë‹¨ê³„: FlexibleTechnicalScorer ê¸°ë°˜ yfinance í”¼ì²˜ ìƒì„± (ì™„ì „ ìˆ˜ì • ë²„ì „)!")

    # 0. ìºì‹œì— ë§¤í•‘ëœ í‹°ì»¤ ì¶”ê°€ (í•œ ë²ˆë§Œ ì‹¤í–‰)
    print("ğŸ”§ ìºì‹œ ë§¤í•‘ ì—…ë°ì´íŠ¸ ì¤‘...")
    update_cache_with_mappings()

    # 1. ë°ì´í„° ë¡œë“œ
    DRIVE_DATA_PATH = "/content/drive/MyDrive/news_with_embeddings.csv"
    try:
        df = pd.read_csv(DRIVE_DATA_PATH, engine='python')
        df['news_date'] = pd.to_datetime(df['news_date'])
        print(f"âœ… ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ: {len(df)}ê°œ")
    except Exception as e:
        print(f"âŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return

    # 2. ìºì‹œ ë° ì‹œì¥ ë°ì´í„° ì¤€ë¹„
    min_date = df['news_date'].min()
    max_date = df['news_date'].max()
    return_horizons = [1, 3, 7]

    ticker_data_cache, market_hist = fetch_and_cache_yfinance_data(min_date, max_date, return_horizons)
    if ticker_data_cache is None:
        return

    # 3. ê¸°ìˆ ì  ë¶„ì„ ì ìˆ˜ ê³„ì‚° (ë””ë²„ê¹… ëª¨ë“œ)
    print("\n" + "="*60)
    print("ğŸ“ˆ ê¸°ìˆ ì  ë¶„ì„ ì ìˆ˜ ê³„ì‚° ì‹œì‘")
    print("="*60)

    # âœ… score_distributionì„ ë°›ë„ë¡ ìˆ˜ì •
    df_scores, score_distribution = calculate_technical_scores_with_debug(df.copy(), ticker_data_cache, debug_mode=True)
    df_with_scores = pd.merge(df, df_scores[['rule_score', 'momentum_score', 'volume_score']], left_index=True, right_index=True)

    # 4. ì‹¤íŒ¨ ë°ì´í„° ì œê±°
    original_count = len(df_with_scores)
    df_processed = df_with_scores.dropna(subset=['rule_score']).reset_index(drop=True)
    print(f"\nâœ… ê¸°ìˆ ì  ë¶„ì„ ì„±ê³µ ë°ì´í„°: {len(df_processed)}ê°œ (ì‹¤íŒ¨: {original_count - len(df_processed)}ê°œ)")

    if df_processed.empty:
        print("âŒ ì²˜ë¦¬í•  ìœ íš¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 5. ì ìˆ˜ ë¶„í¬ ìƒì„¸ ë¶„ì„
    print(f"\nğŸ“Š ì ìˆ˜ ë¶„í¬ ìƒì„¸ ë¶„ì„:")

    rule_scores = df_processed['rule_score'].dropna()
    momentum_scores = df_processed['momentum_score'].dropna()
    volume_scores = df_processed['volume_score'].dropna()

    print(f"ğŸ“ˆ rule_score ë¶„í¬:")
    print(f"   í‰ê· : {rule_scores.mean():.1f}, í‘œì¤€í¸ì°¨: {rule_scores.std():.1f}")
    print(f"   ë²”ìœ„: {rule_scores.min():.1f} ~ {rule_scores.max():.1f}")
    print(f"   50ì  ì •í™•íˆ: {(rule_scores == 50.0).sum()}ê°œ")
    print(f"   50ì  ê·¼ì²˜(49~51): {((rule_scores >= 49) & (rule_scores <= 51)).sum()}ê°œ")

    print(f"\nğŸ“ˆ momentum_score ë¶„í¬:")
    print(f"   í‰ê· : {momentum_scores.mean():.1f}, í‘œì¤€í¸ì°¨: {momentum_scores.std():.1f}")
    print(f"   ë²”ìœ„: {momentum_scores.min():.1f} ~ {momentum_scores.max():.1f}")

    print(f"\nğŸ“ˆ volume_score ë¶„í¬:")
    print(f"   í‰ê· : {volume_scores.mean():.1f}, í‘œì¤€í¸ì°¨: {volume_scores.std():.1f}")
    print(f"   ë²”ìœ„: {volume_scores.min():.1f} ~ {volume_scores.max():.1f}")

    # 6. ì„¹í„° ë° ì‹œê°€ì´ì•¡ ì •ë³´ ìˆ˜ì§‘
    print("\nğŸ¢ ì„¹í„° ë° ì‹œê°€ì´ì•¡ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
    unique_tickers_list = df_processed['ticker'].unique().tolist()
    sector_map = {}
    market_cap_map = {}

    for t in tqdm(unique_tickers_list, desc="í‹°ì»¤ ì •ë³´ ìˆ˜ì§‘"):
        try:
            # ë§¤í•‘ëœ í‹°ì»¤ ì‚¬ìš©
            mapped_t = {'BRK': 'BRK-B', 'BF': 'BF-B'}.get(t, t)
            info = yf.Ticker(mapped_t).info
            sector_map[t] = info.get('sector', 'Unknown')
            market_cap_map[t] = info.get('marketCap', None)
        except Exception:
            sector_map[t] = 'Unknown'
            market_cap_map[t] = None
        time.sleep(0.5)

    df_processed['sector'] = df_processed['ticker'].map(sector_map)
    df_processed['market_cap'] = df_processed['ticker'].map(market_cap_map)

    # 7. ë³µí•© ì‹ í˜¸ ê³„ì‚°
    print("\n" + "="*60)
    print("ğŸ¯ ë³µí•© ì‹ í˜¸ ê³„ì‚° ì‹œì‘")
    print("="*60)

    df_composite = calculate_composite_signal_cached(df_processed.copy(), ticker_data_cache, market_hist, return_horizons)
    df_processed['composite_signal'] = df_composite['composite_signal']

    # ë³µí•© ì‹ í˜¸ ì„±ê³µ ë°ì´í„°ë§Œ í•„í„°ë§
    df_final = df_processed.dropna(subset=['composite_signal']).reset_index(drop=True)
    print(f"âœ… ë³µí•© ì‹ í˜¸ ê³„ì‚° ì„±ê³µ: {len(df_final)}ê°œ")

    # 8. ìµœì¢… ì ìˆ˜ ê³„ì‚° ë° ë¶„í¬ í™•ì¸
    df_final['final_score_10'] = ((df_final['composite_signal'] + 1) * 5).clip(0, 10)

    print(f"\nğŸ“Š ìµœì¢… ë°ì´í„° í’ˆì§ˆ í™•ì¸:")
    print(f"   composite_signal ë¶„í¬:")
    print(f"     í‰ê· : {df_final['composite_signal'].mean():.3f}")
    print(f"     í‘œì¤€í¸ì°¨: {df_final['composite_signal'].std():.3f}")
    print(f"     ë²”ìœ„: {df_final['composite_signal'].min():.3f} ~ {df_final['composite_signal'].max():.3f}")

    print(f"   final_score_10 ë¶„í¬:")
    print(f"     í‰ê· : {df_final['final_score_10'].mean():.1f}")
    print(f"     í‘œì¤€í¸ì°¨: {df_final['final_score_10'].std():.1f}")
    print(f"     ë²”ìœ„: {df_final['final_score_10'].min():.1f} ~ {df_final['final_score_10'].max():.1f}")

    # 9. ìµœì¢… íŒŒì¼ ì €ì¥
    output_file = "/content/drive/MyDrive/news_full_features_robust.csv"
    df_final.to_csv(output_file, index=False, encoding='utf-8-sig')

    print(f"\nğŸ‰ ìµœì¢… íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_file}")
    print(f"ğŸ“Š ìµœì¢… ì»¬ëŸ¼ ìˆ˜: {len(df_final.columns)}")
    print(f"ğŸ“Š ìµœì¢… ì»¬ëŸ¼: {list(df_final.columns)}")
    print(f"ğŸ“ˆ ìµœì¢… ë°ì´í„°: {len(df_final)}ê°œ")

    # 10. ì„±ê³¼ ìš”ì•½
    print(f"\n" + "="*60)
    print("ğŸ‰ 2ë‹¨ê³„ ì™„ë£Œ - ì„±ê³¼ ìš”ì•½")
    print("="*60)
    print(f"âœ… ì›ë³¸ ë‰´ìŠ¤: {len(df)}ê°œ")
    print(f"âœ… ê¸°ìˆ ì  ë¶„ì„ ì„±ê³µ: {len(df_processed)}ê°œ ({len(df_processed)/len(df)*100:.1f}%)")
    print(f"âœ… ë³µí•© ì‹ í˜¸ ì„±ê³µ: {len(df_final)}ê°œ ({len(df_final)/len(df)*100:.1f}%)")
    print(f"âœ… 50ì  ì™¸ ì ìˆ˜ ë¹„ìœ¨: {score_distribution.get('50ì _ì™¸', 0)/(score_distribution.get('50ì ', 1) + score_distribution.get('50ì _ì™¸', 0))*100:.1f}%")
    print(f"ğŸš€ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í›ˆë ¨ ì¤€ë¹„ ì™„ë£Œ!")

if __name__ == "__main__":
    main()