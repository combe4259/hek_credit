"""
ë‰´ìŠ¤ ì „ì²˜ë¦¬ + í•œêµ­ì–´ ì¢…ëª©ëª… â†’ ì˜ì–´ í‹°ì»¤ ë§¤í•‘
"""

import json
import pandas as pd
import re
import os
from typing import Dict, List, Optional
from rapidfuzz import fuzz, process
from datetime import datetime, timedelta
import warnings
from dotenv import load_dotenv
from concurrent.futures import ThreadPoolExecutor, as_completed
from datasketch import MinHash, MinHashLSH
warnings.filterwarnings('ignore')

# .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ (ë£¨íŠ¸ ë””ë ‰í† ë¦¬)
load_dotenv("../.env")

class NewsPreprocessor:
    """ë‰´ìŠ¤ ì „ì²˜ë¦¬ & ì¢…ëª© ë§¤í•‘ í´ë˜ìŠ¤"""

    def __init__(self, excel_file: str = "data/src/sp500_korean_stocks_with_symbols.xlsx", debug: bool = False):
        self.debug = debug
        self.stock_mapping = {}
        self.load_stock_mapping(excel_file)

        # <<< í•„í„°ë§ ê·œì¹™ìš© í‚¤ì›Œë“œ ë° íŒ¨í„´ ì •ì˜ >>>
        self.PROMOTION_KEYWORDS = ["í• ì¸", "íŠ¹ê°€", "ì´ë²¤íŠ¸", "í”„ë¡œëª¨ì…˜", "ë¬´ë£Œ", "ì¦ì •"]
        self.INVESTMENT_KEYWORDS = [
            "ì‹¤ì ", "ì „ë§", "ëª©í‘œì£¼ê°€", "íˆ¬ìì˜ê²¬", "ë§¤ìˆ˜", "ë§¤ë„", "ì¤‘ë¦½", "ìƒìŠ¹", "í•˜ë½",
            "ìˆ˜ìµë¥ ", "ì• ë„ë¦¬ìŠ¤íŠ¸", "ë¦¬í¬íŠ¸", "ì»¨ì„¼ì„œìŠ¤", "ì–´ë‹", "ê°€ì´ë˜ìŠ¤", "ê³µê¸‰", "ê³„ì•½", "M&A",
            "ì‹¤ì ë°œí‘œ", "ì£¼ê°€", "íˆ¬ì", "ì¸ìˆ˜", "ì„±ì¥", "ìˆ˜ì£¼",
            "ë°°ë‹¹", "ìë³¸ê¸ˆ", "ì‹œì¥ì ìœ ìœ¨", "ë¶„ê¸°ì‹¤ì ", "ì˜ì—…ì´ìµ", "ìˆœì´ìµ", "ë§¤ì¶œ",
            "ì£¼ì‹", "ì¦ê¶Œ", "ê±°ë˜ëŸ‰", "ìœ ìƒì¦ì", "ì¬ë¬´ì œí‘œ", "ì¬ë¬´êµ¬ì¡°", "IPO", "ìƒì¥",
            "íˆ¬ìì", "í€ë“œ", "í—¤ì§€í€ë“œ", "ì‹ ê·œì‚¬ì—…", "ì‚¬ì—…í™•ì¥", "ì‹ ì œí’ˆ", "ë¦¬ìŠ¤í¬", "ìœ„í—˜",
            "ê²½ìŸì‚¬", "í™˜ìœ¨", "ê¸ˆë¦¬", "ê²½ì œì§€í‘œ", "ì£¼ì´", "ë°°ë‹¹ê¸ˆ", "ìœ í†µì£¼ì‹ìˆ˜", "ëŒ€ì£¼ì£¼",
            "ì§€ë¶„ë³€ë™", "ìíšŒì‚¬", "í•©ì‘ë²•ì¸", "ì‹ ìš©ë“±ê¸‰", "ì±„ê¶Œ", "ë¶€ì±„ë¹„ìœ¨"
]
        self.POLITICAL_KEYWORDS = [
            "ì •ë¶€", "ì •ì±…", "ê·œì œ", "ê´€ì„¸", "ì„ ê±°", "ëŒ€í†µë ¹", "êµ­íšŒ", "ë²•ì•ˆ", "ê³µí™”ë‹¹", "ë¯¼ì£¼ë‹¹",
            "íŠ¸ëŸ¼í”„", "ë°”ì´ë“ ", "ì˜íšŒ"
        ]
        self.ENUMERATION_PATTERNS = [r"ë“±ì„", r"ë“±ì´", r"ë¹„ë¡¯í•œ"]

    def load_stock_mapping(self, excel_file: str):
        """ì—‘ì…€ íŒŒì¼ì—ì„œ ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ ìƒì„±"""
        try:
            df = pd.read_excel(excel_file)
            df.columns = ['korean_name', 'symbol']  # ì²« ì—´: í•œêµ­ì–´, ë‘ ë²ˆì§¸ ì—´: ì˜ì–´

            # NaN ì œê±° í›„ strip
            df = df.dropna()
            for _, row in df.iterrows():
                korean = str(row['korean_name']).strip()
                symbol = str(row['symbol']).strip()
                if korean and symbol:
                    self.stock_mapping[korean] = symbol

            print(f"âœ… ë§¤í•‘ ë¡œë“œ ì™„ë£Œ: {len(self.stock_mapping)}ê°œ")

        except Exception as e:
            print(f"âŒ ì—‘ì…€ ë¡œë“œ ì‹¤íŒ¨: {e}")

    def clean_content(self, text: str) -> str:
        """1ì°¨ ì „ì²˜ë¦¬: ë¶ˆí•„ìš”í•œ ê³µë°±/íŠ¹ìˆ˜ë¬¸ì + ê´‘ê³ /ì¡ë‹¤í•œ ë¬¸êµ¬ ì œê±°"""
        if not isinstance(text, str) or not text:
            return ""

        # ì¤„ë°”ê¿ˆ ë° ê³µë°± ì •ë¦¬
        text = text.replace("\n", " ").replace("\r", " ")
        text = re.sub(r'\s+', ' ', text)

        # ê²Œí‹°ì´ë¯¸ì§€/ê´‘ê³ /ë²„íŠ¼ ë“± ë¶ˆí•„ìš”í•œ ë¬¸êµ¬ íŒ¨í„´ ì œê±°
        unwanted_pattern = re.compile(
            r"ì‚¬ì§„\s*=\s*ê²Œí‹°ì´ë¯¸ì§€\w*|"            # ì‚¬ì§„=ê²Œí‹°ì´ë¯¸ì§€ë±…í¬ ë“±
            r"ê²Œí‹°ì´ë¯¸ì§€ë±…í¬|"
            r"êµ¬ë…\s*\d+[ëª…]*|"                    # êµ¬ë… 6,565ëª…
            r"ì‘ì›\s*\d+|"                        # ì‘ì› 76,297
            r"(ì¢‹ì•„ìš”|ìŠ¬í¼ìš”|í™”ë‚˜ìš”|íŒ¬ì´ì—ìš”|í›„ì†ê¸°ì‚¬ ì›í•´ìš”)\s*\d*|"  # ê°ì • ë²„íŠ¼
            r"ë¬´ë£Œë§Œí™”\s*ë³´ëŸ¬ê°€ê¸°|"
            r"(í”„ë¡œì•¼êµ¬|ë°”ë¡œê°€ê¸°)|"
            r"ì–¸ë¡ ì‚¬\s*í™ˆ\s*ë°”ë¡œê°€ê¸°|"
            r"ê¸°ì\s*êµ¬ë…|"
            r"\*?\s*ì¬íŒë§¤\s*ë°\s*DB\s*ê¸ˆì§€\s*\*?|"  # *ì¬íŒë§¤ ë° DB ê¸ˆì§€*
            r"\*?\s*ì¬íŒë§¤\s*ê¸ˆì§€\s*\*?|"            # *ì¬íŒë§¤ ê¸ˆì§€*
            r"ì‚¬ì§„\s*ì œê³µ\s*=\s*[^\s]+|"            # ì‚¬ì§„ ì œê³µ=íšŒì‚¬ëª…
            r"ì´\s*ê¸°ì‚¬ëŠ”\s*\d{4}ë…„\d{2}ì›”\d{2}ì¼.*?ì„ ê³µê°œ.*?|"   # í”„ë¦¬ë¯¸ì—„ ì½˜í…ì¸  ê´€ë ¨
            r"ì¬ìƒ\s*\d+[,\d]*\s*\d{1,2}:\d{2}|"   # ì¬ìƒ 7,188 04:26
            r"\d{1,2}:\d{2}\s*-\s*|"                # 04:26 - 
            r"\|?\s*[ê°€-í£]{2,4}\s*ê¸°ì|"           # ìµœë³‘íƒœ ê¸°ì, ê¹€ìˆ˜í˜„ ê¸°ì ë“±
            r"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}"  # ì´ë©”ì¼ íŒ¨í„´
            r"(ê³µì‹ê³„ì •\s*[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})"
        )
        
        text = unwanted_pattern.sub("", text)

        # ëŒ€ê´„í˜¸ [] ì•ˆì˜ ë‚´ìš© ì œê±°
        text = re.sub(r"\[[^\]]*\]", "", text)

        # ì €ì‘ê¶Œ í‘œì‹œ ì œê±° (â“’ì¶œì²˜ëª…)
        text = re.sub(r"â“’[^\s]+", "", text)

        # íŠ¹ìˆ˜ ê¸°í˜¸ ì œê±°
        text = re.sub(r"[â‘ â‘¦â‘©â—†â—‡â– â—â–¶â–·â–³â—€â–²â–¼â˜…â€»]", "", text)

        # ì‚¬ì§„ ê´€ë ¨ íƒœê·¸ ì œê±°
        text = re.sub(r"<ì‚¬ì§„=.*?>", "", text)
        text = re.sub(r'/(ì‚¬ì§„|ê·¸ë˜í”½|ì˜ìƒ)=.*?ê¸°ì', '', text)
        text = re.sub(r"(ê·¸ë˜í”½|ì‚¬ì§„|ê¸°ì‚¬|ì¸í„°ë·°|Cover Story)\s*=\s*[^,.\s]+", "", text)
        text = re.sub(r"ì‚¬ì§„:.*?(\.|$)", "", text)
        text = re.sub(r"ã€ˆì‚¬ì§„=.*?ã€‰", "", text)

        # ì €ì‘ê¶Œ ì¶œì²˜ + ê¸°ìëª… + ë‚ ì§œ ì‹œê°„ íŒ¨í„´ ì œê±° (ì¶”ê°€)
        text = re.sub(
            r"â“’\s*[^\d\s]+(?:\s*[ê°€-í£]{2,4})?\s*\d{1,2}ì¼\s*ì˜¤?ì „?\s*\d{1,2}ì‹œ\s*\d{1,2}ë¶„",
            "", text
        )

        # URL ì œê±°
        text = re.sub(r"https?://[^\s]+", "", text)

        # ì–‘ ì˜† ===== 5ê°œ ì´ìƒìœ¼ë¡œ ë‘˜ëŸ¬ìŒ“ì¸ í…ìŠ¤íŠ¸
        text = re.sub(r"={5,}.*?={5,}", "", text, flags=re.DOTALL)

        # 'ì¸ì‚¬ì´íŠ¸' í¬í•¨ ë‹¨ì–´ ì œê±°
        text = re.sub(r"\b[ê°€-í£]+ì¸ì‚¬ì´íŠ¸\b", "", text)

        # ë”°ì˜´í‘œ í†µì¼
        text = re.sub(r"[â€œâ€â€˜â€™\"']", '"', text)

        # ê°ì„±ë¶„ì„ì— ë¶ˆí•„ìš”í•œ íŠ¹ìˆ˜ë¬¸ì ì œê±° (ë‹¨, ë¬¸ì¥ë¶€í˜¸ ìœ ì§€)
        text = re.sub(r"[^\w\s.,!?\"']", " ", text)

        # ê¸°ì‚¬ ê²Œì¬ì¼, ì¶œì²˜ ë¬¸êµ¬ ì œê±°
        text = re.sub(r"ì´ ê¸°ì‚¬ëŠ” .*?ì— ê²Œì¬ëœ ê¸°ì‚¬ì…ë‹ˆë‹¤\.", "", text)

        # 1) ê¹¨ì§„ ë¬¸ì(ï¿½) UTF-8 ì¸ì½”ë”©/ë””ì½”ë”©í•´ì„œ ì œê±°
        text = text.encode('utf-8', 'ignore').decode('utf-8', 'ignore')

        text = text.replace("\\", "")

        # ìµœì¢… ê³µë°± ì •ë¦¬
        text = re.sub(r"\s+", " ", text)

        return text.strip()

    def map_stock(self, stock_name: str) -> Optional[str]:
        """í•œêµ­ì–´ ì¢…ëª©ëª… â†’ ì˜ì–´ í‹°ì»¤ (ì •í™• ë§¤ì¹­ â†’ í¼ì§€ ë§¤ì¹­ ìˆœì„œ)"""
        if stock_name in self.stock_mapping:
            return self.stock_mapping[stock_name]

        # í¼ì§€ ë§¤ì¹­ (cutoff 85 ì´ìƒë§Œ)
        match = process.extractOne(stock_name, self.stock_mapping.keys(), scorer=fuzz.partial_ratio)
        if match and match[1] >= 85:
            if self.debug:
                print(f"ğŸ“ í¼ì§€ ë§¤ì¹­: '{stock_name}' â†’ '{match[0]}' ({self.stock_mapping[match[0]]})")
            return self.stock_mapping[match[0]]

        return None
    
    def get_minhash(self, text, num_perm=128):
        m = MinHash(num_perm=num_perm)
        tokens = set(text.split())  # ê°„ë‹¨ í† í°í™”
        for token in tokens:
            m.update(token.encode('utf8'))
        return m
    
    def filter_similar_news(self, news_list, threshold=0.85):
        """ë‰´ìŠ¤ ë³¸ë¬¸ ìœ ì‚¬ë„ ê¸°ë°˜ ì¤‘ë³µ ë‰´ìŠ¤ ì œê±°"""
        lsh = MinHashLSH(threshold=threshold, num_perm=128)
        unique_news = []
        minhashes = []
        
        for i, news in enumerate(news_list):
            m = self.get_minhash(news['content'])
            minhashes.append(m)
            
        for i, m in enumerate(minhashes):
            # í˜„ì¬ LSHì— ì‚½ì…ëœ ê²ƒ ì¤‘ ìœ ì‚¬í•œ ë‰´ìŠ¤ ê²€ìƒ‰
            similar_keys = lsh.query(m)

            # ìê¸° ìì‹  key iê°€ ìˆë‹¤ë©´ ì œê±° (ì•ˆ ë“¤ì–´ê°€ìˆìœ¼ë©´ ë³´í†µ ì—†ì§€ë§Œ ì•ˆì „ì¥ì¹˜)
            similar_keys = [k for k in similar_keys if k != i]

            if similar_keys:
                # ìœ ì‚¬í•œ ë‰´ìŠ¤ ì´ë¯¸ ìˆìŒ â†’ ì¤‘ë³µì´ë¯€ë¡œ skip
                continue

            lsh.insert(i, m)
            unique_news.append(news_list[i])
                
        return unique_news

    # <<< ë‰´ìŠ¤ ê´€ë ¨ì„± í•„í„°ë§ ë¡œì§ >>>
    def is_relevant(self, title: str, content: str, stock_kor: str, ticker: Optional[str]) -> (bool, str):
        """ë‰´ìŠ¤ ê¸°ì‚¬ì˜ íˆ¬ì ê´€ë ¨ì„±ì„ íŒë‹¨í•˜ì—¬ (True/False, ì‚¬ìœ )ë¥¼ ë°˜í™˜"""
        if not title and not content:
            return False, "ë‚´ìš©_ì—†ìŒ"
            
        # 1. ì¦‰ì‹œ ì œê±° ê·œì¹™ (Fast Fail)
        if any(keyword in title for keyword in self.PROMOTION_KEYWORDS):
            return False, "ê´‘ê³ ì„±_ì œëª©"

        for pattern in self.ENUMERATION_PATTERNS:
            if re.search(f"{re.escape(stock_kor)}\s*{pattern}", content):
                # ì£¼ë³€ 50ì ì´ë‚´ì— íˆ¬ì í‚¤ì›Œë“œê°€ ì—†ë‹¤ë©´ ë‹¨ìˆœ ë‚˜ì—´ë¡œ ê°„ì£¼
                match = re.search(f"{re.escape(stock_kor)}\s*{pattern}", content)
                if match:
                    start, end = max(0, match.start() - 50), match.end() + 50
                    context_window = content[start:end]
                    if not any(kw in context_window for kw in self.INVESTMENT_KEYWORDS):
                        return False, "ë‹¨ìˆœ_ë‚˜ì—´"
        
        # 2. ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
        score = 0
        stock_name_in_title = stock_kor in title or (ticker and ticker in title)
        if stock_name_in_title:
            score += 3

        stock_mentions = content.count(stock_kor) + (content.count(ticker) if ticker else 0)
        score += stock_mentions

        found_invest_keywords = {keyword for keyword in self.INVESTMENT_KEYWORDS if keyword in content}
        score += len(found_invest_keywords) * 2

        stock_name_in_title = stock_kor in title or (ticker and ticker in title)

        if stock_mentions < 2 and not stock_name_in_title:
            # ì£¼ë³€ ë§¥ë½ ê²€ì‚¬ ëŒ€ì‹  íˆ¬ì í‚¤ì›Œë“œë§Œ ê²€ì‚¬
            if len(found_invest_keywords) >= 2:
                return True, "íˆ¬ìí‚¤ì›Œë“œ_ìˆì–´ì„œ_í†µê³¼"
            else:
                return False, "ì–¸ê¸‰_ë¶€ì¡±_ë°_ë§¥ë½_ì—†ìŒ"

        if score < 2:
            return False, f"ê´€ë ¨ì„±_ì ìˆ˜_ë‚®ìŒ({score}ì )"

        return True, f"ê´€ë ¨ì„±_ë†’ìŒ({score}ì )"
    
    def validate_cleaned_text(self, original: str, cleaned: str) -> Dict:
        """ì •ì œëœ í…ìŠ¤íŠ¸ í’ˆì§ˆ ê²€ì¦"""
        if not original or not cleaned:
            return {"status": "empty", "length_reduction": 0}
        
        original_len = len(original)
        cleaned_len = len(cleaned)
        length_reduction = (original_len - cleaned_len) / original_len
        
        # ë„ˆë¬´ ë§ì´ ì¤„ì–´ë“¤ë©´ ê²½ê³ 
        if length_reduction > 0.7:  # 70% ì´ìƒ ì¤„ì–´ë“¤ë©´
            status = "over_cleaned"
        elif length_reduction < 0.1:  # 10% ë¯¸ë§Œìœ¼ë¡œ ì¤„ì–´ë“¤ë©´
            status = "under_cleaned"
        else:
            status = "good"
        
        return {
            "status": status,
            "length_reduction": round(length_reduction, 2),
            "original_length": original_len,
            "cleaned_length": cleaned_len
        }
    
    def _filter_by_ticker_count(self, data: List[Dict], min_news_count: int = 5) -> List[Dict]:
        """í‹°ì»¤ë³„ ìµœì†Œ ë‰´ìŠ¤ ê°œìˆ˜ í•„í„°ë§"""
        df = pd.DataFrame(data)
        counts = df['ticker'].value_counts()
        valid_tickers = counts[counts >= min_news_count].index.tolist()
        if self.debug:
            dropped = counts[counts < min_news_count]
            print(f"ğŸš« ì œê±°ëœ í‹°ì»¤: {list(dropped.index)} (ë‰´ìŠ¤ ìˆ˜: {dropped.to_dict()})")
        filtered = df[df['ticker'].isin(valid_tickers)]
        return filtered.to_dict(orient='records') 

    def load_news_data(self, file_path: str) -> List[Dict]:
        """ë‰´ìŠ¤ ë°ì´í„° íŒŒì¼ ë¡œë“œ"""
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)
            print(f"âœ… ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(data)}ê±´")
            return data
        except Exception as e:
            print(f"âŒ ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []

    def save_results(self, processed_data: List[Dict]):
        """ê²°ê³¼ ì €ì¥ (JSON + CSV + í‹°ì»¤ë³„ ì¹´ìš´íŠ¸)"""
        ts = pd.Timestamp.now().strftime("%Y%m%d_%H%M")

        # ì €ì¥í•  í´ë” ê²½ë¡œ
        folder_path = "data"
        os.makedirs(folder_path, exist_ok=True)
        
        # ì „ì²´ JSON ì €ì¥
        json_file = os.path.join(folder_path, f"news_processed_{ts}.json")
        with open(json_file, "w", encoding="utf-8") as f:
            json.dump(processed_data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ ì „ì²´ JSON ì €ì¥ â†’ {json_file}")

        # ë§¤í•‘ ì„±ê³µ ë°ì´í„°ë§Œ CSV ì €ì¥
        mapped_data = [d for d in processed_data if d['ticker'] in self.stock_mapping.values()]
        df = pd.DataFrame(mapped_data)
        csv_file = os.path.join(folder_path, f"news_mapped_{ts}.csv")
        df.to_csv(csv_file, index=False, encoding="utf-8-sig")
        print(f"ğŸ“Š ë§¤í•‘ ì„±ê³µ CSV ì €ì¥ â†’ {csv_file} (ì´ {len(df)}ê°œ)")

        # í‹°ì»¤ë³„ ë‰´ìŠ¤ ê°œìˆ˜ ì €ì¥
        ticker_counts = df['ticker'].value_counts()
        count_file = os.path.join(folder_path, f"ticker_counts_{ts}.csv")
        ticker_counts.to_csv(count_file, header=['news_count'])
        print(f"ğŸ“ˆ í‹°ì»¤ë³„ ì¹´ìš´íŠ¸ ì €ì¥ â†’ {count_file}")

    def process_news_parallel(self, raw_data: List[Dict], max_workers: int = 5) -> tuple[List[Dict], Dict]:
        """ë©€í‹°ìŠ¤ë ˆë“œë¥¼ ì‚¬ìš©í•œ ë‰´ìŠ¤ ë³‘ë ¬ ì²˜ë¦¬"""
        results = []
        status_counter = {"good": 0, "over_cleaned": 0, "under_cleaned": 0, "empty": 0}
        
        print(f"ğŸš€ ë©€í‹°ìŠ¤ë ˆë“œ ì²˜ë¦¬ ì‹œì‘ (ì›Œì»¤: {max_workers}ê°œ, ì´ {len(raw_data)}ê±´)")
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [executor.submit(process_single_news, item, self) for item in raw_data]
            
            for i, future in enumerate(as_completed(futures), 1):
                result, status = future.result()
                if status in status_counter:
                    status_counter[status] += 1
                else:
                    status_counter[status] = 1
                
                if result:
                    results.append(result)
                
                if i % 10 == 0 or i == len(raw_data):
                    progress = (i / len(raw_data)) * 100
                    print(f"  ì§„í–‰ìƒí™©: {i}/{len(raw_data)} ({progress:.1f}%) ì²˜ë¦¬ ì™„ë£Œ", flush=True)
        
        print(f"âœ… ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ: {len(results)}ê±´")
        print(f"   í…ìŠ¤íŠ¸ í’ˆì§ˆ: {status_counter}")
        
        return results, status_counter
    

def process_single_news(item, processor: NewsPreprocessor):
    if processor.debug:
        print(f"â–¶ ë‰´ìŠ¤ ì²˜ë¦¬ ì‹œì‘: {item.get('stock', '')[:10]}...", flush=True)
    stock_kor = item.get("stock", "").strip()
    content_original = item.get("content", "")
    title_original = item.get("title", "")

    # ê¸°ë³¸ ìœ íš¨ì„± ê²€ì‚¬
    if not stock_kor or not content_original or not title_original:
        return None, "missing_data"
    
    published_raw = item.get("published_at", {}).get("$date", None)
    try:
        news_date = datetime.fromisoformat(published_raw.replace("Z", "+00:00")).date()
        if news_date < datetime(2024, 1, 1).date():
            return None, "date_filtered"
        news_date_str = news_date.isoformat()
    except:
        return None, "date_error"
    
    # 1ì°¨ ë£°ì…‹ ì •ì œ
    content_cleaned = processor.clean_content(content_original)
    title_cleaned = processor.clean_content(title_original)

    if not stock_kor or not content_cleaned or len(content_cleaned.strip()) < 50 or len(content_cleaned.split('.')) < 2:
        return None, "content_too_short"
    
    ticker = processor.map_stock(stock_kor) or stock_kor
    
    # ê´€ë ¨ì„± í•„í„°ë§
    is_rel, reason = processor.is_relevant(title_cleaned, content_cleaned, stock_kor, ticker)
    if not is_rel:
        if processor.debug:
            print(f" Â âŒ í•„í„°ë§ë¨: '{title_cleaned[:30]}...' (ì‚¬ìœ : {reason})", flush=True)
        return None, f"filtered_{reason}"
    
    quality = processor.validate_cleaned_text(content_original, content_cleaned)

    return {
        "original_stock": stock_kor,
        "ticker": ticker,
        "news_date": news_date_str,
        "content": content_cleaned,
        "url": item.get("url", "").strip()
    }, quality["status"]


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜: ë‰´ìŠ¤ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
    # 1. ì „ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
    processor = NewsPreprocessor(
        excel_file="data/src/sp500_korean_stocks_with_symbols.xlsx", 
        debug=True,
    )
    
    # 2. ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ
    raw_data = processor.load_news_data("data/src/newsDB.news.json")
    if not raw_data:
        return
    
    # 3. ë³‘ë ¬ ì²˜ë¦¬ë¡œ ë‰´ìŠ¤ ì „ì²˜ë¦¬
    results, status_counter = processor.process_news_parallel(raw_data, max_workers=5)

    # ì¤‘ë³µ ë‰´ìŠ¤ í•„í„°ë§
    print(f"ğŸ” ì¤‘ë³µ ë‰´ìŠ¤ í•„í„°ë§ ì „: {len(results)}ê±´")
    deduped_results = processor.filter_similar_news(results, threshold=0.85)
    print(f"ğŸ” ì¤‘ë³µ ë‰´ìŠ¤ í•„í„°ë§ í›„: {len(deduped_results)}ê±´")

    
    # 4. í‹°ì»¤ë³„ ìµœì†Œ ë‰´ìŠ¤ ê°œìˆ˜ í•„í„°ë§ & ê²°ê³¼ ì €ì¥
    filtered_results = processor._filter_by_ticker_count(deduped_results, min_news_count=5)
    processor.save_results(filtered_results)
    
if __name__ == "__main__":
    main()
