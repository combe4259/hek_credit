import pandas as pd
from tqdm import tqdm
import nltk
from nltk.tokenize import sent_tokenize
import multiprocessing

# NLTK ë°ì´í„° í™•ì¸ ë° ë‹¤ìš´ë¡œë“œ (í•„ìš”ì‹œë§Œ)
try:
    nltk.data.find('tokenizers/punkt_tab')
    nltk.data.find('tokenizers/punkt')
except LookupError:
    print("NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘...")
    nltk.download('punkt_tab', quiet=True)
    nltk.download('punkt', quiet=True)

def process_article(content, nlp_pipeline, batch_size=64):
    """í•˜ë‚˜ì˜ ê¸°ì‚¬ ë³¸ë¬¸(content)ì„ ë¬¸ì¥ë³„ë¡œ ë‚˜ëˆ  ê°ì„±ë¶„ì„í•˜ê³  í‰ê· ê°’ ë°˜í™˜"""
    
    # ë¹ ë¥¸ ì „ì²˜ë¦¬
    if not content or len(str(content).strip()) < 10:
        return {'positive': 0.0, 'negative': 0.0, 'neutral': 0.0, 'sentence_count': 0}
    
    sentences = sent_tokenize(str(content))
    if len(sentences) == 0:
        return {'positive': 0.0, 'negative': 0.0, 'neutral': 0.0, 'sentence_count': 0}

    sentence_scores = []

    # ë¬¸ì¥ ë‹¨ìœ„ batch ì²˜ë¦¬
    for i in range(0, len(sentences), batch_size):
        batch_sents = sentences[i:i + batch_size]
        try:
            batch_results = nlp_pipeline(batch_sents)
            for res in batch_results:
                scores = {x['label']: x['score'] for x in res}
                positive = scores.get('positive', 0.0)
                negative = scores.get('negative', 0.0)
                neutral = scores.get('neutral', 0.0)

                total = positive + negative + neutral
                if total > 0:
                    positive /= total
                    negative /= total
                    neutral /= total

                sentence_scores.append({
                    'positive': positive,
                    'negative': negative,
                    'neutral': neutral
                })
        except Exception as e:
            print(f"Error in batch: {e}")
            sentence_scores.extend([{'positive': 0.0, 'negative': 0.0, 'neutral': 0.0}] * len(batch_sents))

    # ë¬¸ì¥ë³„ ì ìˆ˜ í‰ê· ë‚´ê¸°
    pos_avg = sum(s['positive'] for s in sentence_scores) / len(sentence_scores)
    neg_avg = sum(s['negative'] for s in sentence_scores) / len(sentence_scores)
    neu_avg = sum(s['neutral'] for s in sentence_scores) / len(sentence_scores)
    sent_count = len(sentences)

    return {'positive': pos_avg, 
            'negative': neg_avg, 
            'neutral': neu_avg,
            'sentence_count': sent_count
            }

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # 1) ëª¨ë¸ ë¡œë“œ (í•œ ë²ˆë§Œ)
    print("ğŸ¤– ëª¨ë¸ ë¡œë”© ì¤‘...")
    from load_model import nlp_pipeline
    print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    # 2) ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
    import glob
    import os
    
    csv_files = glob.glob("data/news_mapped_*.csv")
    if csv_files:
        latest_file = max(csv_files, key=os.path.getctime)
        print(f"ğŸ“‚ ìµœì‹  ì „ì²˜ë¦¬ íŒŒì¼ ì‚¬ìš©: {latest_file}")
        df = pd.read_csv(latest_file)
    else:
        raise FileNotFoundError("ì „ì²˜ë¦¬ëœ ë‰´ìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    print(f"ğŸ“Š ì´ {len(df)}ê°œ ë‰´ìŠ¤ ì²˜ë¦¬ ì‹œì‘")
    
    # 3) ìˆœì°¨ ì²˜ë¦¬ (ì•ˆì •ì„± ìš°ì„ )
    all_results = []
    contents = df["content"].tolist()
    
    # ì§„í–‰ë¥  í‘œì‹œ ê°œì„ 
    for i, content in enumerate(tqdm(contents, desc="ê°ì„±ë¶„ì„ ì§„í–‰")):
        result = process_article(content, nlp_pipeline, batch_size=64)
        all_results.append(result)
        
        # 1000ê°œë§ˆë‹¤ ì§„í–‰ìƒí™© ì¶œë ¥
        if (i + 1) % 1000 == 0:
            print(f"ì§„í–‰: {i+1}/{len(contents)} ì™„ë£Œ")
    
    print("âœ… ê°ì„±ë¶„ì„ ì™„ë£Œ")
    
    # 4) ê²°ê³¼ í•©ì¹˜ê¸°
    scores_df = pd.DataFrame(all_results)
    df = pd.concat([df, scores_df], axis=1)
    
    # ë‹¨ì¼ ì ìˆ˜í™”
    df['sentiment_score'] = df['positive'] - df['negative']
    
    # 5) ì €ì¥
    df.to_csv("news_sentiment_probabilities.csv", index=False, encoding="utf-8-sig")
    print("ğŸ“ news_sentiment_probabilities.csv ì €ì¥ ì™„ë£Œ!")

if __name__ == "__main__":
    main()