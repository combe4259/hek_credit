import pandas as pd
from tqdm import tqdm
import nltk
from nltk.tokenize import sent_tokenize
from concurrent.futures import ProcessPoolExecutor, as_completed
import os

# NLTK ë°ì´í„° í™•ì¸ ë° ë‹¤ìš´ë¡œë“œ (í•„ìš”ì‹œë§Œ)
try:
    nltk.data.find('tokenizers/punkt_tab')
    nltk.data.find('tokenizers/punkt')
except LookupError:
    print("NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘...")
    nltk.download('punkt_tab', quiet=True)
    nltk.download('punkt', quiet=True)

def process_article(content, nlp_pipeline, batch_size=64):
    """í•˜ë‚˜ì˜ ê¸°ì‚¬ ë³¸ë¬¸(content)ì„ ë¬¸ì¥ë³„ë¡œ ë‚˜ëˆ  ê°ì„±ë¶„ì„í•˜ê³  í‰ê· ê°’ ë°˜í™˜"""
    
    # ë¹ ë¥¸ ì „ì²˜ë¦¬
    if not content or len(str(content).strip()) < 10:
        return {'positive': 0.0, 'negative': 0.0, 'neutral': 0.0, 'sentence_count': 0}
    
    sentences = sent_tokenize(str(content))
    if len(sentences) == 0:
        return {'positive': 0.0, 'negative': 0.0, 'neutral': 0.0, 'sentence_count': 0}

    sentence_scores = []

    # ë¬¸ì¥ ë‹¨ìœ„ batch ì²˜ë¦¬
    for i in range(0, len(sentences), batch_size):
        batch_sents = sentences[i:i + batch_size]
        try:
            batch_results = nlp_pipeline(batch_sents)
            for res in batch_results:
                scores = {x['label']: x['score'] for x in res}
                positive = scores.get('positive', 0.0)
                negative = scores.get('negative', 0.0)
                neutral = scores.get('neutral', 0.0)

                total = positive + negative + neutral
                if total > 0:
                    positive /= total
                    negative /= total
                    neutral /= total

                sentence_scores.append({
                    'positive': positive,
                    'negative': negative,
                    'neutral': neutral
                })
        except Exception as e:
            print(f"Error in batch: {e}")
            sentence_scores.extend([{'positive': 0.0, 'negative': 0.0, 'neutral': 0.0}] * len(batch_sents))

    # ë¬¸ì¥ë³„ ì ìˆ˜ í‰ê· ë‚´ê¸°
    pos_avg = sum(s['positive'] for s in sentence_scores) / len(sentence_scores)
    neg_avg = sum(s['negative'] for s in sentence_scores) / len(sentence_scores)
    neu_avg = sum(s['neutral'] for s in sentence_scores) / len(sentence_scores)
    sent_count = len(sentences)

    return {'positive': pos_avg, 
            'negative': neg_avg, 
            'neutral': neu_avg,
            'sentence_count': sent_count
            }

nlp_pipeline = None

def init_worker():
    global nlp_pipeline
    from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
    model_name = "snunlp/KR-FinBert-SC"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(model_name)
    nlp_pipeline = pipeline(
        "text-classification",
        model=model,
        tokenizer=tokenizer,
        device=-1,  # CPU ì‚¬ìš© ëª…ì‹œ
        top_k=None,
        max_length=512,
        truncation=True
    )

def worker_process(content):
    global nlp_pipeline
    try:
        return process_article(content, nlp_pipeline)
    except Exception as e:
        print(f"Worker process error: {e}")
        return {'positive': 0, 'negative': 0, 'neutral': 0, 'sentence_count': 0}

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # 1) ëª¨ë¸ ë¡œë“œ (í•œ ë²ˆë§Œ)
    print("ğŸ¤– ê°ì„±ë¶„ì„ ì‹œì‘...")
    
    # 2) ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
    import glob
    
    csv_files = glob.glob("data/news_mapped_*.csv")
    if csv_files:
        latest_file = max(csv_files, key=os.path.getctime)
        print(f"ğŸ“‚ ìµœì‹  ì „ì²˜ë¦¬ íŒŒì¼ ì‚¬ìš©: {latest_file}")
        df = pd.read_csv(latest_file)
    else:
        raise FileNotFoundError("ì „ì²˜ë¦¬ëœ ë‰´ìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    print(f"ğŸ“Š ì´ {len(df)}ê°œ ë‰´ìŠ¤ ì²˜ë¦¬ ì‹œì‘")
    
    contents = df["content"].tolist()
    print(f"ğŸ“Š ì´ {len(contents)}ê°œ ë‰´ìŠ¤ ì²˜ë¦¬ ì‹œì‘ (ë³‘ë ¬ ì²˜ë¦¬)")

    # cpu_count = os.cpu_count() or 4
    # worker_num = max(1, cpu_count - 2)
    worker_num = 2

    # ProcessPoolExecutor ì‚¬ìš©: CPU ì½”ì–´ ìˆ˜ì— ë§ê²Œ ì›Œì»¤ ì§€ì • ê°€ëŠ¥
    with ProcessPoolExecutor(max_workers=worker_num, initializer=init_worker) as executor:
        futures = [executor.submit(worker_process, content) for content in contents]
        
        all_results = []
        for i, future in enumerate(tqdm(as_completed(futures), total=len(futures), desc="ê°ì„±ë¶„ì„ ì§„í–‰")):
            try:
                res = future.result()
            except Exception as e:
                print(f"ì—ëŸ¬ ë°œìƒ: {e}")
                res = {'positive': 0, 'negative': 0, 'neutral': 0, 'sentence_count': 0}
            all_results.append(res)
            
            # 1000ê°œë§ˆë‹¤ ì§„í–‰ìƒí™© ì¶œë ¥
            if (i + 1) % 1000 == 0:
                print(f"ì§„í–‰: {i+1}/{len(contents)} ì™„ë£Œ")
    
    print("âœ… ê°ì„±ë¶„ì„ ì™„ë£Œ")
    
    # 4) ê²°ê³¼ í•©ì¹˜ê¸°
    scores_df = pd.DataFrame(all_results)
    df = pd.concat([df, scores_df], axis=1)
    
    # ë‹¨ì¼ ì ìˆ˜í™”
    df['sentiment_score'] = df['positive'] - df['negative']
    
    # 5) ì €ì¥
    df.to_csv("news_sentiment_probabilities.csv", index=False, encoding="utf-8-sig")
    print("ğŸ“ news_sentiment_probabilities.csv ì €ì¥ ì™„ë£Œ!")

if __name__ == "__main__":
    main()