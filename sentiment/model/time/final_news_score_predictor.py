# =============================================================================
# ìµœì¢… ë‰´ìŠ¤ ì ìˆ˜ ì˜ˆì¸¡ ëª¨ë¸ (ìµœì¢… ê²€ì¦ ê°•í™” ë²„ì „)
# ê¸°ê°„ ê¸°ë°˜ ë¡¤ë§ ìœˆë„ìš°, KR-FinBERT, LightGBM, Optuna ì ìš©
# =============================================================================
import pandas as pd
import numpy as np
import optuna
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_absolute_error, r2_score
import lightgbm as lgb
import joblib
import warnings
from pandas.tseries.offsets import DateOffset
from sklearn.decomposition import PCA
import sys
import os
import traceback
from google.colab import drive
import matplotlib.pyplot as plt

warnings.filterwarnings('ignore')

class FinalNewsScorePredictor:
    """
    ğŸ¯ ê¸°ê°„ ê¸°ë°˜ ë¡¤ë§ ìœˆë„ìš° í‰ê°€ë¥¼ í†µí•´ ëª¨ë¸ì˜ ì•ˆì •ì„±ì„ ê²€ì¦í•˜ëŠ” ìµœì¢… ëª¨ë¸
    """

    def __init__(self):
        self.model = None
        self.feature_names = None
        self.best_params = None
        self.pca = None
        print("âœ… FinalNewsScorePredictor í´ë˜ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ!")

    def _composite_signal_to_score(self, composite_signal):
        composite_signal = np.clip(composite_signal, -1.0, 1.0)
        score = 7.5 * (composite_signal + 1)
        # ë¶„ì‚° ì¦ê°€ë¥¼ ìœ„í•œ ë³€í™˜ - í´ë¦¬í•‘ ì „ì— ì ìš©
        score = np.sqrt(score) * 2.5  # ì œê³±ê·¼ ë³€í™˜ìœ¼ë¡œ ë¶„ì‚° ì¦ê°€
        return score  # í´ë¦¬í•‘ ì œê±°í•˜ì—¬ ë” ë„“ì€ ë¶„í¬ í—ˆìš©

    def _score_to_category(self, score):
        if score >= 8.5: return "ê°•ë ¥í˜¸ì¬"
        elif score >= 7.0: return "í˜¸ì¬"
        elif score >= 6.0: return "ì•½ê°„í˜¸ì¬"
        elif score >= 4.0: return "ì¤‘ë¦½"
        elif score >= 3.0: return "ì•½ê°„ì•…ì¬"
        elif score >= 1.5: return "ì•…ì¬"
        else: return "ê°•ë ¥ì•…ì¬"

    def _create_features(self, df):
        """ì‚¬ì „ ìƒì„±ëœ í”¼ì²˜ë¥¼ ë¶ˆëŸ¬ì™€ ëª¨ë¸ ì…ë ¥ ë°ì´í„°ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜"""
        print("ğŸ› ï¸ ì‚¬ì „ ìƒì„±ëœ í”¼ì²˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì¢… ë°ì´í„°ì…‹ êµ¬ì„± ì¤‘...")
        final_feature_parts = []

        # ğŸ¯ ê°œì„ : BERT ì„ë² ë”© ì°¨ì› ì¶•ì†Œ (PCA)
        bert_cols = [f'finbert_{i}' for i in range(768)]
        X_bert = df[bert_cols]
        if self.pca is None:
            self.pca = PCA(n_components=0.95)  # 95% ë¶„ì‚° ì„¤ëª…í•˜ëŠ” ì»´í¬ë„ŒíŠ¸ ìë™ ì„ íƒ
            X_bert_pca = self.pca.fit_transform(X_bert)
        else:
            X_bert_pca = self.pca.transform(X_bert)
        X_bert_pca = pd.DataFrame(X_bert_pca, index=df.index, columns=[f'pca_bert_{i}' for i in range(X_bert_pca.shape[1])])
        final_feature_parts.append(X_bert_pca)

        # ê°ì„± í”¼ì²˜
        sentiment_features = df[['positive', 'negative', 'neutral', 'sentence_count']].copy()
        total_emotion = (sentiment_features['positive'] + sentiment_features['negative']).replace(0, 1e-8)
        sentiment_features['emotion_balance'] = (sentiment_features['positive'] - sentiment_features['negative']) / total_emotion
        final_feature_parts.append(sentiment_features)

        # ğŸ¯ ìˆ˜ì •: rule_score í”¼ì²˜ë¥¼ ì œê±°í•˜ì—¬ ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€
        ruleset_cols = ['momentum_score', 'volume_score']
        ruleset_features = df[ruleset_cols].fillna(50).copy()
        final_feature_parts.append(ruleset_features)

        # ë§¥ë½ í”¼ì²˜
        context_features = df[['sector', 'market_cap']].copy()
        cap_q1, cap_q3 = context_features['market_cap'].quantile([0.25, 0.75])
        context_features['market_cap_group'] = pd.cut(
            context_features['market_cap'], bins=[0, cap_q1, cap_q3, np.inf],
            labels=['Small', 'Mid', 'Large'], right=False
        )
        context_features_encoded = pd.get_dummies(context_features[['sector', 'market_cap_group']], dummy_na=True)
        final_feature_parts.append(context_features_encoded)

        X = pd.concat(final_feature_parts, axis=1)

        # ğŸ¯ ê°œì„ : í”¼ì²˜ ìƒê´€ì„± ì œê±°
        corr_matrix = X.corr().abs()
        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
        to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]
        X = X.drop(columns=to_drop)

        # íƒ€ê²Ÿ ë³€ìˆ˜ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ìƒê´€ì„± ê²€ì‚¬ ìˆ˜í–‰
        # (í›ˆë ¨ ì‹œì—ë§Œ ì‹¤í–‰, ì˜ˆì¸¡ ì‹œì—ëŠ” ìŠ¤í‚µ)

        return X

    def train(self, df, model_save_path='final_model.pkl'):
        print("\nğŸ¤– ìµœì¢… ëª¨ë¸ í›ˆë ¨ (ì‹œê³„ì—´ êµì°¨ê²€ì¦) ì‹œì‘...")

        df['news_date'] = pd.to_datetime(df['news_date'])
        df = df.sort_values('news_date').reset_index(drop=True)

        required_cols = ['composite_signal'] + [f'finbert_{i}' for i in range(768)]
        train_df = df.dropna(subset=required_cols).copy()

        y = self._composite_signal_to_score(train_df['composite_signal'])
        X = self._create_features(train_df)
        self.feature_names = X.columns.tolist()
        
        # ë°ì´í„° ëˆ„ìˆ˜ ê²€ì‚¬ (í›ˆë ¨ ì‹œì—ë§Œ)
        combined_data = pd.concat([X, pd.Series(y, name='target', index=X.index)], axis=1)
        corr_with_target = combined_data.corr()['target'].abs()
        high_corr = corr_with_target[corr_with_target > 0.95].drop('target', errors='ignore').index.tolist()
        if high_corr:
            print(f"âš ï¸ ë†’ì€ ìƒê´€ í”¼ì²˜ (ëˆ„ìˆ˜ ì˜ì‹¬): {high_corr}")
            X = X.drop(columns=high_corr)
            self.feature_names = X.columns.tolist()

        split_index = int(len(X) * 0.8)
        X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]
        y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]

        print(f"ğŸ“ˆ ì „ì²´ ë°ì´í„°: {len(X)}ê°œ")
        print(f"   - í›ˆë ¨/íŠœë‹ ë°ì´í„°: {len(X_train)}ê°œ (ê³¼ê±° 80%)")
        print(f"   - ìµœì¢… í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(X_test)}ê°œ (ìµœì‹  20%)")

        # ğŸ¯ ìˆ˜ì •: íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„ì‚°ì´ ì¶©ë¶„í•œì§€ ê²€ì¦
        print("\nğŸ“Š íƒ€ê²Ÿ ë¶„í¬ ìš”ì•½:")
        print(y_train.describe())
        if len(y_train.unique()) <= 1:
            print("âŒ íƒ€ê²Ÿ ë³€ìˆ˜ì— ìœ ì˜ë¯¸í•œ ë¶„ì‚°ì´ ì—†ì–´ ëª¨ë¸ í•™ìŠµì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. ë°ì´í„°ì…‹ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            return

        def objective(trial):
            params = {
                'objective': 'regression_l1', 'metric': 'mae', 'verbosity': -1,
                'n_estimators': trial.suggest_int('n_estimators', 200, 1000, step=100),
                'max_depth': trial.suggest_int('max_depth', 5, 11),
                'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1, log=True),
                'num_leaves': trial.suggest_int('num_leaves', 31, 71),
                'subsample': trial.suggest_float('subsample', 0.7, 0.9),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 0.9),
                'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
                'reg_alpha': trial.suggest_float('reg_alpha', 0, 1),
                'reg_lambda': trial.suggest_float('reg_lambda', 0, 1),
                'random_state': 42, 'n_jobs': -1
            }

            tscv = TimeSeriesSplit(n_splits=3)
            mae_scores = []
            for train_idx, val_idx in tscv.split(X_train):
                X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]
                y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]

                model = lgb.LGBMRegressor(**params)
                model.fit(
                    X_train_fold, y_train_fold,
                    eval_set=[(X_val_fold, y_val_fold)],
                    callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)]
                )
                y_pred = model.predict(X_val_fold)
                mae_scores.append(mean_absolute_error(y_val_fold, y_pred))

            return np.mean(mae_scores)

        print("\nâ³ Optuna íŠœë‹ (ì‹œê³„ì—´ êµì°¨ê²€ì¦ ê¸°ë°˜)...")
        study = optuna.create_study(direction='minimize')
        study.optimize(objective, n_trials=100, show_progress_bar=True)

        print("\nâœ… ìµœì  íŒŒë¼ë¯¸í„°:", study.best_params)
        self.best_params = study.best_params
        self.model = lgb.LGBMRegressor(**self.best_params, random_state=42, n_jobs=-1)
        self.model.fit(X_train, y_train)

        y_pred = self.model.predict(X_test)
        mae = mean_absolute_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        direction_accuracy = np.mean((y_test > 5) == (y_pred > 5))
        print(f"\nğŸ“Š ìµœì¢… ì„±ëŠ¥ (Hold-out Test) - MAE: {mae:.3f}, RÂ²: {r2:.3f}, ë°©í–¥ ì •í™•ë„: {direction_accuracy:.1%}")

        # ğŸ¯ ìˆ˜ì •: í”¼ì²˜ ì¤‘ìš”ë„ ì‹œê°í™” ì˜¤ë¥˜ ë°©ì§€
        if hasattr(self.model, 'feature_importances_') and len(self.model.feature_importances_) > 0:
            lgb.plot_importance(self.model, max_num_features=20)
            plt.show()
        else:
            print("âš ï¸ ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ê±°ë‚˜ í”¼ì²˜ê°€ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„°ì™€ í›ˆë ¨ ê³¼ì •ì„ í™•ì¸í•˜ì„¸ìš”.")

        self.save_model(model_save_path)

    def evaluate_rolling_window(self, df, train_period_months=18, test_period_months=3):
        """
        âœ¨ ê¸°ê°„ ê¸°ë°˜ ë¡¤ë§ ìœˆë„ìš°ë¡œ ëª¨ë¸ì˜ ì„±ëŠ¥ ì•ˆì •ì„±ì„ í‰ê°€í•©ë‹ˆë‹¤.
        """
        if self.best_params is None:
            print("âš ï¸ ë¡¤ë§ ìœˆë„ìš° í‰ê°€ë¥¼ ìœ„í•´ ë¨¼ì € train() í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•˜ì—¬ ìµœì  íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ì•„ì•¼ í•©ë‹ˆë‹¤.")
            return

        print("\n" + "="*60)
        print("ğŸ” ê¸°ê°„ ê¸°ë°˜ ë¡¤ë§ ìœˆë„ìš° í‰ê°€ ì‹œì‘...")
        print(f"(í›ˆë ¨ ê¸°ê°„: {train_period_months}ê°œì›”, í…ŒìŠ¤íŠ¸ ê¸°ê°„: {test_period_months}ê°œì›”)")
        print("="*60)

        df['news_date'] = pd.to_datetime(df['news_date'])
        df = df.sort_values('news_date').reset_index(drop=True)

        required_cols = ['composite_signal'] + [f'finbert_{i}' for i in range(768)]
        eval_df = df.dropna(subset=required_cols).copy()

        if self.pca is None:
            print("âŒ PCA ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•„ ë¡¤ë§ ìœˆë„ìš° í‰ê°€ë¥¼ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. train()ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
            return

        X = self._create_features(eval_df)
        y = self._composite_signal_to_score(eval_df['composite_signal'])

        results = []

        start_date = eval_df['news_date'].min()
        end_date = eval_df['news_date'].max()

        current_date = start_date + DateOffset(months=train_period_months)

        while current_date + DateOffset(months=test_period_months) <= end_date:
            train_start_date = current_date - DateOffset(months=train_period_months)
            train_end_date = current_date
            test_end_date = current_date + DateOffset(months=test_period_months)

            train_mask = (eval_df['news_date'] >= train_start_date) & (eval_df['news_date'] < train_end_date)
            test_mask = (eval_df['news_date'] >= train_end_date) & (eval_df['news_date'] < test_end_date)

            if train_mask.sum() < 50 or test_mask.sum() < 10:
                current_date += DateOffset(months=test_period_months)
                continue

            X_train, X_test = X[train_mask], X[test_mask]
            y_train, y_test = y[train_mask], y[test_mask]

            print(f"\nğŸ—“ï¸  í…ŒìŠ¤íŠ¸ êµ¬ê°„: {train_end_date.date()} ~ {test_end_date.date()-DateOffset(days=1)}")

            model = lgb.LGBMRegressor(**self.best_params, random_state=42, n_jobs=-1)
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)

            mae = mean_absolute_error(y_test, y_pred)
            r2 = r2_score(y_test, y_pred)
            direction_accuracy = np.mean((y_test > 5) == (y_pred > 5))

            results.append({'test_period': f"{train_end_date.date()}", 'mae': mae, 'r2': r2, 'direction_accuracy': direction_accuracy})
            print(f"   - MAE: {mae:.3f}, RÂ²: {r2:.3f}, ë°©í–¥ ì •í™•ë„: {direction_accuracy:.1%}")

            current_date += DateOffset(months=test_period_months)

        if not results:
            print("âš ï¸ ë¡¤ë§ ìœˆë„ìš° í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ê¸°ì— ë°ì´í„° ê¸°ê°„ì´ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return

        results_df = pd.DataFrame(results)
        print("\n" + "-"*60)
        print("ğŸ“Š ë¡¤ë§ ìœˆë„ìš° í‰ê°€ ìµœì¢… ìš”ì•½:")
        print(results_df.round(3))
        print("-" * 60)
        print(f"   - í‰ê·  MAE: {results_df['mae'].mean():.3f}")
        print(f"   - í‰ê·  RÂ²: {results_df['r2'].mean():.3f}")
        print(f"   - í‰ê·  ë°©í–¥ ì •í™•ë„: {results_df['direction_accuracy'].mean():.1%}")
        return results_df

    def save_model(self, path):
        """âœ¨ ëª¨ë¸, í”¼ì²˜ ì´ë¦„, ìµœì  íŒŒë¼ë¯¸í„°ë¥¼ í•¨ê»˜ ì €ì¥"""
        model_payload = {
            'model': self.model,
            'feature_names': self.feature_names,
            'best_params': self.best_params,
            'pca': self.pca
        }
        joblib.dump(model_payload, path)
        print(f"ğŸ’¾ ëª¨ë¸ê³¼ ì„¤ì • ì €ì¥ ì™„ë£Œ: {path}")

    @classmethod
    def load_model(cls, path):
        """âœ¨ í´ë˜ìŠ¤ ë©”ì„œë“œë¡œ ëª¨ë¸ê³¼ ì„¤ì •ì„ ë¡œë“œ"""
        try:
            model_payload = joblib.load(path)
            instance = cls()
            instance.model = model_payload['model']
            instance.feature_names = model_payload['feature_names']
            instance.best_params = model_payload['best_params']
            instance.pca = model_payload.get('pca')
            print(f"ğŸ’¾ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {path}")
            return instance
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

# =============================================================================
# ì‹¤í–‰
# =============================================================================
if __name__ == "__main__":
    drive.mount('/content/drive', force_remount=True)

    try:
        DRIVE_DATA_PATH = "/content/drive/MyDrive/news_full_features_robust.csv"
        df_news = pd.read_csv(DRIVE_DATA_PATH, engine='python')

        predictor = FinalNewsScorePredictor()
        predictor.train(df_news)
        predictor.evaluate_rolling_window(df_news, train_period_months=18, test_period_months=3)

    except Exception as e:
        print(f"\nâŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        traceback.print_exc()