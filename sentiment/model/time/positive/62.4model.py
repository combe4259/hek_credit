# ==================================================================================
# ì•™ìƒë¸” ë° ê³ ê¸‰ í”¼ì²˜ ê¸°ë°˜ ë‰´ìŠ¤ AI ì‹œìŠ¤í…œ
# ==================================================================================
import pandas as pd
import numpy as np
import lightgbm as lgb
import xgboost as xgb
import catboost as cb
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import RobustScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
import warnings
from datetime import datetime, timedelta
import yfinance as yf
from typing import Dict, Optional
import joblib
from google.colab import drive

warnings.filterwarnings('ignore')

# í—¬í¼ í•¨ìˆ˜: ì¢…ëª©ë³„ ì„¹í„° ì •ë³´ ê°€ì ¸ì˜¤ê¸°
def get_sector_map(ticker_map: Dict) -> Dict:
    print("ğŸŒ ì¢…ëª©ë³„ ì„¹í„° ì •ë³´ ìˆ˜ì§‘ ì¤‘... (ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    sector_map = {}
    processed = 0
    total = len(ticker_map)
    for name, ticker in ticker_map.items():
        processed += 1
        if processed % 50 == 0:
            print(f"  - {processed}/{total}ê°œ ì¢…ëª© ì²˜ë¦¬ ì¤‘")
        try:
            stock_info = yf.Ticker(ticker).info
            sector = stock_info.get('sector', 'Unknown')
            sector_map[name] = sector
        except Exception:
            sector_map[name] = 'Unknown'
    print("  âœ… ì„¹í„° ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ!")
    return sector_map

class AdvancedNewsAI:
    """ì•™ìƒë¸” ë° ê³ ê¸‰ í”¼ì²˜ ê¸°ë°˜ íˆ¬ì AI ì‹œìŠ¤í…œ"""
    
    def __init__(self, name_to_ticker_map: Dict, name_to_sector_map: Dict):
        self.name_to_ticker_map = name_to_ticker_map
        self.name_to_sector_map = name_to_sector_map
        self.base_models = {}
        self.meta_model = LogisticRegression()
        self.magnitude_model = lgb.LGBMRegressor(objective='regression_l1', random_state=42, verbose=-1)
        self.scaler = RobustScaler()
        self.feature_names = None
        self.stock_cache = {}
        self.bert_pca = None
        print("ğŸš€ ì•™ìƒë¸” ê¸°ë°˜ AI ì‹œìŠ¤í…œ ì´ˆê¸°í™”")

    def create_improved_targets(self, df_news: pd.DataFrame, verbose: bool = True) -> pd.DataFrame:
        if verbose:
            print("\nğŸ“Š ê°œì„ ëœ íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± ì¤‘...")
            print("=" * 50)
        
        results = []
        unique_stocks = df_news['original_stock'].unique()
        for i, stock_name in enumerate(unique_stocks):
            if verbose and (i + 1) % 50 == 0:
                print(f"  - íƒ€ê²Ÿ ìƒì„± ì§„í–‰: {i+1}/{len(unique_stocks)} ì¢…ëª©")

            stock_name_clean = stock_name.strip('$')
            ticker = self.name_to_ticker_map.get(stock_name_clean)
            if not ticker:
                continue

            stock_news = df_news[df_news['original_stock'] == stock_name].sort_values('news_date')
            min_date = stock_news['news_date'].min()
            max_date = stock_news['news_date'].max()
            stock_data = self._get_stock_data(ticker, min_date - timedelta(days=30), max_date + timedelta(days=10))

            if stock_data is None:
                continue
            
            stock_news_with_price = pd.merge_asof(
                stock_news,
                stock_data[['Close']].rename(columns={'Close': 'price_at_news_time'}),
                left_on='news_date',
                right_index=True,
                direction='nearest',
                tolerance=pd.Timedelta('1 day')
            )

            for _, news_row in stock_news_with_price.iterrows():
                targets = self._calculate_advanced_targets(news_row, stock_data)
                result_row = news_row.to_dict()
                result_row.update(targets)
                results.append(result_row)
        
        result_df = pd.DataFrame(results).dropna(subset=['direction_24h'])
        if verbose and not result_df.empty:
            print(f"âœ… {len(result_df)}ê°œ ë‰´ìŠ¤ì— ëŒ€í•œ íƒ€ê²Ÿ ìƒì„± ì™„ë£Œ")
            self._print_target_stats(result_df)
        return result_df

    def _calculate_advanced_targets(self, news_row: pd.Series, stock_data: pd.DataFrame) -> Dict:
        news_date = pd.to_datetime(news_row['news_date'])
        base_price = self._get_price_at_time(stock_data, news_date)
        if base_price is None:
            return self._get_default_targets()

        prices = {f'{h}h': self._get_price_at_time(stock_data, news_date + timedelta(hours=h)) for h in [24, 72, 120]}
        targets = {}

        for timeframe, price in prices.items():
            if price and base_price and base_price > 0:
                return_rate = (price - base_price) / base_price
                targets[f'direction_{timeframe}'] = 1 if return_rate > 0.001 else 0
                targets[f'return_{timeframe}'] = return_rate
            else:
                targets.update({f'direction_{timeframe}': np.nan, f'return_{timeframe}': 0})
        
        return targets

    def create_advanced_features(self, df: pd.DataFrame, verbose: bool = True) -> pd.DataFrame:
        if verbose:
            print("\nğŸ› ï¸ ê³ ê¸‰ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§...")
        
        feature_parts = []
        df_copy = df.copy()

        # 1. ê°ì„± í”¼ì²˜
        sentiment_cols = ['positive', 'negative', 'neutral', 'sentiment_score']
        sentiment_df = df_copy[sentiment_cols].fillna(0)
        sentiment_df['sentiment_intensity'] = sentiment_df['positive'] + sentiment_df['negative']
        sentiment_df['sentiment_balance'] = (sentiment_df['positive'] - sentiment_df['negative']) / (sentiment_df['sentiment_intensity'] + 1e-8)
        feature_parts.append(sentiment_df)

        # 2. ì‹œê°„ í”¼ì²˜
        news_dates = pd.to_datetime(df_copy['news_date'])
        time_df = pd.DataFrame(index=df_copy.index)
        time_df['hour'] = news_dates.dt.hour
        time_df['day_of_week'] = news_dates.dt.dayofweek
        feature_parts.append(time_df)

        # 3. ì‹œì¥ í™˜ê²½ í”¼ì²˜
        market_df = pd.DataFrame(index=df_copy.index)
        if 'vix_close' in df_copy.columns:
            market_df['vix_close'] = df_copy['vix_close'].fillna(20)
            market_df['vix_high'] = (market_df['vix_close'] > 25).astype(int)
        else:
            market_df['vix_close'] = 20
            market_df['vix_high'] = 0
        feature_parts.append(market_df)

        # 4. ì¢…ëª©/ì„¹í„° í”¼ì²˜
        df_copy['sector'] = df_copy['original_stock'].str.strip('$').map(self.name_to_sector_map).fillna('Unknown')
        sector_dummies = pd.get_dummies(df_copy['sector'], prefix='sector', dummy_na=True)
        feature_parts.append(sector_dummies)

        # 5. ë‰´ìŠ¤ ë¹ˆë„ í”¼ì²˜ (Expanding Window)
        df_sorted = df_copy.sort_values('news_date').copy()
        df_sorted['days_since_last_news'] = df_sorted.groupby('original_stock')['news_date'].diff().dt.days.fillna(0)
        df_sorted['news_frequency_cumulative'] = df_sorted.groupby('original_stock').cumcount() + 1
        feature_parts.append(df_sorted[['days_since_last_news', 'news_frequency_cumulative']])

        # 6. ê³¼ê±° ì£¼ê°€ ì •ë³´ (Lagged Features)
        if 'price_at_news_time' in df_copy.columns:
            lagged_df = pd.DataFrame(index=df_copy.index)
            lagged_df['return_5d_before_news'] = df_sorted.groupby('original_stock')['price_at_news_time'].pct_change(periods=5).fillna(0)
            feature_parts.append(lagged_df)

        # 7. ìƒí˜¸ì‘ìš© í”¼ì²˜
        interaction_df = pd.DataFrame(index=df_copy.index)
        interaction_df['vix_x_sentiment'] = market_df['vix_close'] * sentiment_df['sentiment_balance']
        feature_parts.append(interaction_df)

        # 8. FinBERT í”¼ì²˜
        bert_cols = [f'finbert_{i}' for i in range(768) if f'finbert_{i}' in df_copy.columns]
        if bert_cols:
            bert_data = df_copy[bert_cols].fillna(0)
            if self.bert_pca is None:
                self.bert_pca = PCA(n_components=30, random_state=42)
                bert_reduced = self.bert_pca.fit_transform(bert_data)
            else:
                bert_reduced = self.bert_pca.transform(bert_data)
            bert_df = pd.DataFrame(bert_reduced, index=df_copy.index, columns=[f'bert_pc_{i}' for i in range(30)])
            feature_parts.append(bert_df)
        
        X_final = pd.concat(feature_parts, axis=1)
        
        if self.feature_names is None:
            self.feature_names = X_final.columns.tolist()
        else:
            for col in self.feature_names:
                if col not in X_final.columns:
                    X_final[col] = 0
            X_final = X_final[self.feature_names]
            
        return X_final.fillna(0)

    def train_models(self, df_news: pd.DataFrame, verbose: bool = True):
        if verbose:
            print("\nğŸš€ ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨ ì‹œì‘")
        
        df_with_targets = self.create_improved_targets(df_news, verbose=verbose)
        if df_with_targets.empty:
            return None
        
        min_date = df_with_targets['news_date'].min() - timedelta(days=5)
        max_date = df_with_targets['news_date'].max() + timedelta(days=5)
        vix_data = self._get_stock_data('^VIX', min_date, max_date)
        if vix_data is not None:
            df_with_targets = pd.merge_asof(
                df_with_targets.sort_values('news_date'),
                vix_data[['Close']].rename(columns={'Close':'vix_close'}),
                left_on='news_date',
                right_index=True,
                direction='backward'
            ).sort_index()
        else:
            df_with_targets['vix_close'] = 20

        X = self.create_advanced_features(df_with_targets, verbose=verbose)
        y = df_with_targets['direction_24h'].astype(int)
        
        tscv = TimeSeriesSplit(n_splits=3)
        oof_preds = np.zeros((len(X), 3))
        
        if verbose:
            print("\nğŸ“ˆ ìŠ¤íƒœí‚¹ ì•™ìƒë¸” êµì°¨ ê²€ì¦...")
        for fold, (train_idx, test_idx) in enumerate(tscv.split(X)):
            print(f"  - Fold {fold+1}/3 í›ˆë ¨ ì¤‘...")
            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

            scaler_cv = RobustScaler()
            X_train_scaled = scaler_cv.fit_transform(X_train)
            X_test_scaled = scaler_cv.transform(X_test)

            models_cv = {
                'lgbm': lgb.LGBMClassifier(random_state=42, verbose=-1),
                'xgb': xgb.XGBClassifier(random_state=42, eval_metric='logloss', use_label_encoder=False),
                'cat': cb.CatBoostClassifier(random_state=42, verbose=0)
            }

            for i, (name, model) in enumerate(models_cv.items()):
                model.fit(X_train_scaled, y_train)
                oof_preds[test_idx, i] = model.predict_proba(X_test_scaled)[:, 1]
        
        last_fold_test_idx = test_idx
        meta_X = oof_preds[last_fold_test_idx]
        meta_y = y.iloc[last_fold_test_idx]
        
        print("  - ë©”íƒ€ ëª¨ë¸ í›ˆë ¨ ì¤‘...")
        self.meta_model.fit(meta_X, meta_y)
        meta_preds = self.meta_model.predict(meta_X)
        accuracy = accuracy_score(meta_y, meta_preds)
        print(f"  - ìµœì¢… Fold ë©”íƒ€ ëª¨ë¸ ì •í™•ë„: {accuracy:.1%}")

        print("  - ì „ì²´ ë°ì´í„°ë¡œ ìµœì¢… ê¸°ë³¸ ëª¨ë¸ë“¤ í›ˆë ¨...")
        self.scaler.fit(X)
        X_scaled = self.scaler.transform(X)
        
        self.base_models['lgbm'] = lgb.LGBMClassifier(random_state=42, verbose=-1).fit(X_scaled, y)
        self.base_models['xgb'] = xgb.XGBClassifier(random_state=42, eval_metric='logloss', use_label_encoder=False).fit(X_scaled, y)
        self.base_models['cat'] = cb.CatBoostClassifier(random_state=42, verbose=0).fit(X_scaled, y)
        
        base_preds_final = np.array([model.predict_proba(X_scaled)[:, 1] for model in self.base_models.values()]).T
        self.meta_model.fit(base_preds_final, y)
        
        print("  - ìµœì¢… ë³€ë™í­ ì˜ˆì¸¡ ëª¨ë¸ í›ˆë ¨...")
        y_magnitude = np.abs(df_with_targets['return_24h']).fillna(0)
        self.magnitude_model.fit(X_scaled, y_magnitude)

        if verbose:
            print(f"\nâœ… ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!")
        return {'ensemble_accuracy': accuracy}

    def predict_news_impact(self, news_data: Dict, verbose: bool = True) -> Dict:
        if not self.base_models:
            raise ValueError("ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
        
        df_input = pd.DataFrame([news_data])
        df_input['news_date'] = pd.to_datetime(df_input['news_date'])
        
        vix_data = self._get_stock_data('^VIX', df_input['news_date'].min() - timedelta(days=5), df_input['news_date'].max() + timedelta(days=1))
        if vix_data is not None:
            df_input['vix_close'] = vix_data['Close'].asof(df_input['news_date'].iloc[0])
        else:
            df_input['vix_close'] = 20
            
        ticker = self.name_to_ticker_map.get(news_data['original_stock'].strip('$'))
        if ticker:
            price_data = self._get_stock_data(ticker, df_input['news_date'].min() - timedelta(days=10), df_input['news_date'].max() + timedelta(days=1))
            if price_data is not None:
                 df_input['price_at_news_time'] = self._get_price_at_time(price_data, df_input['news_date'].iloc[0])
        if 'price_at_news_time' not in df_input.columns:
             df_input['price_at_news_time'] = 100

        X = self.create_advanced_features(df_input, verbose=False)
        X_scaled = self.scaler.transform(X)
        
        base_predictions = np.array([model.predict_proba(X_scaled)[:, 1] for model in self.base_models.values()]).T
        final_prob = self.meta_model.predict_proba(base_predictions)[0, 1]
        magnitude = self.magnitude_model.predict(X_scaled)[0]
        
        impact_score = 5 + (final_prob - 0.5) * 10 * min(magnitude * 50, 1)
        impact_score = np.clip(impact_score, 0, 10)
        
        prediction = 'POSITIVE' if final_prob > 0.7 else 'NEGATIVE' if final_prob < 0.3 else 'NEUTRAL'
        confidence = 'HIGH' if abs(final_prob - 0.5) > 0.2 else 'MEDIUM'
        
        result = {'impact_score': round(float(impact_score), 2), 'direction_probability': round(float(final_prob), 3), 'expected_magnitude': round(float(magnitude), 4), 'prediction': prediction, 'confidence': confidence}
        
        if verbose:
            print(f"ğŸ“° ë‰´ìŠ¤ ì˜í–¥ë„ ë¶„ì„ (ì•™ìƒë¸” + ì ìˆ˜):")
            print(f" Â ğŸ¯ ìµœì¢… ì˜í–¥ë„ ì ìˆ˜: {result['impact_score']}/10")
            print(f" Â ğŸ“ˆ ìµœì¢… ìƒìŠ¹ í™•ë¥ : {result['direction_probability']:.1%}")
            print(f" Â ğŸ“Š ì˜ˆìƒ ë³€ë™í­: {result['expected_magnitude']:.2%}")
            print(f" Â ğŸ” ì˜ˆì¸¡: {result['prediction']} ({result['confidence']} ì‹ ë¢°ë„)")
        
        return result

    def _get_stock_data(self, ticker: str, start_date: datetime, end_date: datetime) -> Optional[pd.DataFrame]:
        cache_key = f"{ticker}_{start_date.date()}_{end_date.date()}"
        if cache_key in self.stock_cache:
            return self.stock_cache[cache_key]
        try:
            data = yf.Ticker(ticker).history(start=start_date, end=end_date, auto_adjust=True)
            if not data.empty:
                data.index = data.index.tz_localize(None)
                self.stock_cache[cache_key] = data
                return data
        except Exception:
            pass
        return None

    def _get_price_at_time(self, stock_data: Optional[pd.DataFrame], target_time: datetime) -> Optional[float]:
        if stock_data is None:
            return None
        try:
            target_time_naive = pd.to_datetime(target_time).tz_localize(None)
            return stock_data['Close'].asof(target_time_naive)
        except:
            return None

    def _get_default_targets(self) -> Dict:
        return {'direction_24h': np.nan, 'return_24h': 0, 'direction_72h': np.nan, 'return_72h': 0, 'direction_120h': np.nan, 'return_120h': 0}

    def _print_target_stats(self, df: pd.DataFrame):
        print("\nğŸ“Š íƒ€ê²Ÿ ë³€ìˆ˜ í†µê³„:")
        for timeframe in ['24h', '72h', '120h']:
            direction_col = f'direction_{timeframe}'
            return_col = f'return_{timeframe}'
            if direction_col in df.columns and return_col in df.columns:
                up_ratio = df[direction_col].mean()
                avg_return = df[return_col].mean()
                print(f" Â {timeframe}: ìƒìŠ¹ë¹„ìœ¨ {up_ratio:.1%}, í‰ê· ìˆ˜ìµë¥  {avg_return:.2%}")
        
    def _print_feature_importance(self):
        if self.base_models.get('lgbm'):
            print("\nğŸ“Š LGBM ëª¨ë¸ì˜ ì£¼ìš” í”¼ì²˜ ì¤‘ìš”ë„ Top 10:")
            importance_df = pd.DataFrame({'feature': self.feature_names, 'importance': self.base_models['lgbm'].feature_importances_}).sort_values('importance', ascending=False)
            for i, (_, row) in enumerate(importance_df.head(10).iterrows()):
                print(f" Â {i+1:2d}. {row['feature']:<30}: {row['importance']}")

    def save_model(self, filepath: str):
        joblib.dump(self, filepath)
        print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {filepath}")

    @classmethod
    def load_model(cls, filepath: str):
        instance = joblib.load(filepath)
        print(f"ğŸ“‚ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {filepath}")
        return instance

# ==================================================================================
# ì‹¤í–‰ë¶€
# ==================================================================================
if __name__ == "__main__":
    try:
        drive.mount('/content/drive', force_remount=True)
        print("ğŸš€ ì•™ìƒë¸” AI ì‹œìŠ¤í…œ í›ˆë ¨ ë° ì‹œì—° ì‹œì‘")
        print("=" * 60)

        mapping_xlsx_path = "/content/drive/MyDrive/sp500_korean_stocks_with_symbols.xlsx"
        news_csv_path = "/content/drive/MyDrive/news_full_features_robust.csv"
        model_save_path = "/content/drive/MyDrive/advanced_news_ai_model.pkl"

        df_mapping = pd.read_excel(mapping_xlsx_path, header=1)
        df_mapping.dropna(subset=['Symbol'], inplace=True)
        name_ticker_map = pd.Series(df_mapping.Symbol.values, index=df_mapping['Korean Name'].str.strip()).to_dict()

        name_sector_map = {name: 'Unknown' for name in name_ticker_map.keys()}
        
        df_news = pd.read_csv(news_csv_path)
        df_news['news_date'] = pd.to_datetime(df_news['news_date'])

        ai_system = AdvancedNewsAI(name_ticker_map, name_sector_map)
        training_results = ai_system.train_models(df_news, verbose=True)

        if training_results:
            ai_system.save_model(model_save_path)
            
            print("\n" + "="*60)
            print("ğŸ¬ ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ ë° ì˜ˆì¸¡ ì‹œì—°")
            print("="*60)
            
            loaded_ai = AdvancedNewsAI.load_model(model_save_path)
            
            sample_stock_name = 'ì—”ë¹„ë””ì•„'
            if sample_stock_name in loaded_ai.name_to_ticker_map:
                sample_news = df_news[df_news['original_stock'] == sample_stock_name].iloc[-1].to_dict()
                loaded_ai.predict_news_impact(sample_news)
            else:
                print(f"'{sample_stock_name}'ì´ ë‰´ìŠ¤ ë°ì´í„°ì— ì—†ì–´ ì‹œì—°ì„ ê±´ë„ˆëœë‹ˆë‹¤.")

    except Exception as e:
        print(f"âŒ ìµœì¢… ì‹¤í–‰ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()