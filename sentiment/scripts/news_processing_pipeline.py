# ==================================================================================
# ë‰´ìŠ¤ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
# ìƒˆë¡œ í¬ë¡¤ë§ëœ ë‰´ìŠ¤ â†’ ê°ì„±ë¶„ì„ â†’ FinBERT ì„ë² ë”© â†’ í•™ìŠµ ë°ì´í„° í˜•íƒœë¡œ ë³€í™˜
# ==================================================================================

import pandas as pd
import numpy as np
import os
import sys
import re
from datetime import datetime
from pymongo import MongoClient
import json
from tqdm import tqdm
from concurrent.futures import ProcessPoolExecutor, as_completed
import warnings
warnings.filterwarnings('ignore')

# ìƒìœ„ ë””ë ‰í† ë¦¬ ì¶”ê°€ (sentiment_analysis.py ì‚¬ìš©)
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

class NewsProcessor:
    def __init__(self):
        self.mongo_uri = os.getenv('MONGODB_URI', 'mongodb+srv://julk0206:%23Sooyeon2004@hek.yqi7d9x.mongodb.net')
        self.client = None
        
    def connect_mongodb(self):
        """MongoDB ì—°ê²°"""
        try:
            self.client = MongoClient(self.mongo_uri)
            self.db = self.client.newsDB
            self.collection = self.db.news
            print("MongoDB ì—°ê²° ì™„ë£Œ")
            return True
        except Exception as e:
            print(f"MongoDB ì—°ê²° ì‹¤íŒ¨: {e}")
            return False
    
    # ì „ì²˜ë¦¬
    def clean_content(self, content):
        if not content:
            return ""
        
        content_str = str(content)
        
        # í”„ë¡œëª¨ì…˜/ê´‘ê³ ì„± í‚¤ì›Œë“œ ì œê±°
        promo_keywords = [
            r'.*ë¬´ë£Œì²´í—˜.*', r'.*ì²´í—˜ì‹ ì²­.*', r'.*ê°€ì…í•˜ê¸°.*', r'.*í´ë¦­.*',
            r'.*ë°”ë¡œê°€ê¸°.*', r'.*ì‹ ì²­.*', r'.*ì´ë²¤íŠ¸.*', r'.*í• ì¸.*',
            r'.*í˜œíƒ.*', r'.*íŠ¹ê°€.*', r'.*ê¸°íšŒ.*'
        ]
        
        for keyword in promo_keywords:
            content_str = re.sub(keyword, '', content_str, flags=re.IGNORECASE)
        
        # ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ íŒ¨í„´ ì œê±°
        unwanted_patterns = [
            r'\[.*?ê¸°ì\]',  # [ê¸°ìëª…]
            r'\[.*?íŠ¹íŒŒì›\]',  # [íŠ¹íŒŒì›ëª…] 
            r'\[.*?=.*?\]',  # [ì§€ì—­=ê¸°ì]
            r'Copyright.*',  # ì €ì‘ê¶Œ í‘œì‹œ
            r'copyrights?.*',  # ì €ì‘ê¶Œ í‘œì‹œ (ì†Œë¬¸ì)
            r'ë¬´ë‹¨.*ì „ì¬.*',  # ë¬´ë‹¨ ì „ì¬ ê¸ˆì§€
            r'ì¬ë°°í¬.*ê¸ˆì§€.*',  # ì¬ë°°í¬ ê¸ˆì§€
            r'ë°°í¬.*ê¸ˆì§€.*',  # ë°°í¬ ê¸ˆì§€  
            r'â€».*',  # ì£¼ì„
            r'â–¶.*',  # í™”ì‚´í‘œë¡œ ì‹œì‘í•˜ëŠ” ë§í¬
            r'â–².*ì‚¬ì§„.*',  # ì‚¬ì§„ ì„¤ëª…
            r'â–¼.*ì‚¬ì§„.*',  # ì‚¬ì§„ ì„¤ëª…
            r'ì‚¬ì§„.*=.*',  # ì‚¬ì§„ ìº¡ì…˜
            r'ê·¸ë˜í”½.*=.*',  # ê·¸ë˜í”½ ìº¡ì…˜
            r'ìë£Œ.*=.*',  # ìë£Œ ìº¡ì…˜
            r'\(ì‚¬ì§„.*\)',  # (ì‚¬ì§„ì„¤ëª…)
            r'\(ìë£Œ.*\)',  # (ìë£Œì„¤ëª…)
            r'\(ê·¸ë˜í”½.*\)',  # (ê·¸ë˜í”½ì„¤ëª…)
            r'â–·.*ë°”ë¡œê°€ê¸°',  # ë§í¬ ì•ˆë‚´
            r'â–·.*ìì„¸íˆ',  # ë§í¬ ì•ˆë‚´
            r'ê´€ë ¨ê¸°ì‚¬.*',  # ê´€ë ¨ê¸°ì‚¬ ì•ˆë‚´
            r'ì´ì „ê¸°ì‚¬.*',  # ì´ì „ê¸°ì‚¬ ì•ˆë‚´
            r'ë‹¤ìŒê¸°ì‚¬.*',  # ë‹¤ìŒê¸°ì‚¬ ì•ˆë‚´
            r'ê¸°ì.*@.*\..*',  # ì´ë©”ì¼ ì£¼ì†Œ
            r'ì—°ë½ì²˜.*\d{3}-\d{3,4}-\d{4}',  # ì „í™”ë²ˆí˜¸
            r'.*êµ¬ë….*',  # êµ¬ë… ì•ˆë‚´
            r'.*íŒ”ë¡œìš°.*',  # íŒ”ë¡œìš° ì•ˆë‚´
            r'.*ì¢‹ì•„ìš”.*',  # ì¢‹ì•„ìš” ì•ˆë‚´
            r'.*ê³µìœ í•˜ê¸°.*',  # ê³µìœ  ì•ˆë‚´
            r'.*í˜ì´ìŠ¤ë¶.*',  # SNS ê´€ë ¨
            r'.*íŠ¸ìœ„í„°.*',  # SNS ê´€ë ¨
            r'.*ì¸ìŠ¤íƒ€ê·¸ë¨.*',  # SNS ê´€ë ¨
            r'â€».*ì´ ê¸°ì‚¬ëŠ”.*',  # ê¸°ì‚¬ ì¶œì²˜ í‘œì‹œ
            r'â–¶.*ì´ ê¸°ì‚¬ëŠ”.*',  # ê¸°ì‚¬ ì¶œì²˜ í‘œì‹œ
        ]
        
        for pattern in unwanted_patterns:
            content_str = re.sub(pattern, '', content_str, flags=re.IGNORECASE)
        
        # HTML íƒœê·¸ ë° íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
        content_str = re.sub(r'<[^>]+>', '', content_str)  # HTML íƒœê·¸
        content_str = re.sub(r'&[a-z]+;', ' ', content_str)  # HTML
        content_str = re.sub(r'[^\w\sê°€-í£]', ' ', content_str)  # íŠ¹ìˆ˜ë¬¸ì (í•œê¸€, ì˜ì–´, ìˆ«ì, ê³µë°±ë§Œ ìœ ì§€)
        content_str = re.sub(r'\s+', ' ', content_str)  # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        content_str = content_str.strip()
        
        # ë„ˆë¬´ ì§§ì€ ì»¨í…ì¸  í•„í„°ë§
        if len(content_str) < 20:
            return ""
        
        return content_str

    def get_new_news_from_mongo(self, limit=None):
        """MongoDBì—ì„œ ìƒˆë¡œìš´ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
        try:
            if not self.client:
                if not self.connect_mongodb():
                    return pd.DataFrame()
            
            # ìµœì‹  ë‰´ìŠ¤ë¶€í„° ê°€ì ¸ì˜¤ê¸°
            query = {}
            if limit:
                cursor = self.collection.find(query).sort("created_at", -1).limit(limit)
            else:
                cursor = self.collection.find(query).sort("created_at", -1)
            
            news_data = list(cursor)
            
            if not news_data:
                print("ìƒˆë¡œìš´ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return pd.DataFrame()
            
            df = pd.DataFrame(news_data)
            print(f"MongoDBì—ì„œ {len(df)}ê°œ ë‰´ìŠ¤ ë¡œë“œ ì™„ë£Œ")
            
            # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒí•˜ê³  ì´ë¦„ ë³€ê²½
            required_columns = {
                'stock': 'original_stock',
                'title': 'title',
                'content': 'content',
                'url': 'url',
                'press': 'press',
                'published_at': 'news_date',
                'created_at': 'crawled_at'
            }
            
            # ì»¬ëŸ¼ ì´ë¦„ ë³€ê²½
            for old_name, new_name in required_columns.items():
                if old_name in df.columns:
                    df = df.rename(columns={old_name: new_name})
            
            # í•„ìˆ˜ ì»¬ëŸ¼ ì²´í¬
            missing_cols = [col for col in required_columns.values() if col not in df.columns]
            if missing_cols:
                print(f"ëˆ„ë½ëœ ì»¬ëŸ¼: {missing_cols}")
            
            # ì»¨í…ì¸  ì •ì œ ì ìš©
            print("ë‰´ìŠ¤ ì»¨í…ì¸  ì •ì œ ì¤‘")
            df['content'] = df['content'].apply(self.clean_content)
            
            # ì •ì œ í›„ ë¹ˆ ì»¨í…ì¸  ì œê±°
            original_count = len(df)
            df = df[df['content'].str.len() > 0]
            filtered_count = len(df)
            
            if original_count != filtered_count:
                print(f"ì»¨í…ì¸  ì •ì œ í›„ {original_count - filtered_count}ê°œ ë‰´ìŠ¤ í•„í„°ë§ë¨")
            
            return df[list(required_columns.values())].copy()
            
        except Exception as e:
            print(f"MongoDB ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return pd.DataFrame()
    
    # ê°ì„±ë¶„ì„
    def run_sentiment_analysis(self, df):
        print("ê°ì„±ë¶„ì„ ì‹œì‘...")
        
        try:
            # ê°ì„±ë¶„ì„ì„ ìœ„í•œ ì›Œì»¤ í•¨ìˆ˜ë“¤
            def init_worker():
                global nlp_pipeline
                from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
                model_name = "snunlp/KR-FinBert-SC"
                tokenizer = AutoTokenizer.from_pretrained(model_name)
                model = AutoModelForSequenceClassification.from_pretrained(model_name)
                nlp_pipeline = pipeline(
                    "text-classification",
                    model=model,
                    tokenizer=tokenizer,
                    device=-1,  # CPU ì‚¬ìš©
                    top_k=None,
                    max_length=512,
                    truncation=True
                )
            
            def worker_process(content):
                global nlp_pipeline
                try:
                    if not content or len(str(content).strip()) < 10:
                        return {'positive': 0.0, 'negative': 0.0, 'neutral': 0.0}
                    
                    # ì§§ì€ í…ìŠ¤íŠ¸ëŠ” ì „ì²´ë¥¼ í•œë²ˆì— ì²˜ë¦¬
                    if len(str(content)) < 500:
                        result = nlp_pipeline(str(content))
                        scores = {x['label']: x['score'] for x in result}
                    else:
                        # ê¸´ í…ìŠ¤íŠ¸ëŠ” ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì²˜ë¦¬ í›„ í‰ê· 
                        from nltk.tokenize import sent_tokenize
                        sentences = sent_tokenize(str(content))[:5]  # ìµœëŒ€ 5ë¬¸ì¥ë§Œ
                        all_scores = []
                        
                        for sent in sentences:
                            if len(sent.strip()) > 5:
                                result = nlp_pipeline(sent)
                                scores = {x['label']: x['score'] for x in result}
                                all_scores.append(scores)
                        
                        if all_scores:
                            # í‰ê·  ê³„ì‚°
                            scores = {}
                            for label in ['positive', 'negative', 'neutral']:
                                scores[label] = np.mean([s.get(label, 0) for s in all_scores])
                        else:
                            scores = {'positive': 0.0, 'negative': 0.0, 'neutral': 0.0}
                    
                    positive = scores.get('positive', 0.0)
                    negative = scores.get('negative', 0.0)
                    neutral = scores.get('neutral', 0.0)
                    
                    # ì •ê·œí™”
                    total = positive + negative + neutral
                    if total > 0:
                        positive /= total
                        negative /= total
                        neutral /= total
                    
                    return {
                        'positive': positive,
                        'negative': negative, 
                        'neutral': neutral
                    }
                    
                except Exception as e:
                    print(f"ê°ì„±ë¶„ì„ ì˜¤ë¥˜: {e}")
                    return {'positive': 0.0, 'negative': 0.0, 'neutral': 0.0}
            
            # NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ
            try:
                import nltk
                nltk.data.find('tokenizers/punkt_tab')
            except:
                import nltk
                nltk.download('punkt_tab', quiet=True)
                nltk.download('punkt', quiet=True)
            
            # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ê°ì„±ë¶„ì„ ì‹¤í–‰
            contents = df['content'].fillna('').tolist()
            
            with ProcessPoolExecutor(max_workers=2, initializer=init_worker) as executor:
                futures = [executor.submit(worker_process, content) for content in contents]
                
                results = []
                for future in tqdm(as_completed(futures), total=len(futures), desc="ê°ì„±ë¶„ì„"):
                    try:
                        result = future.result()
                        results.append(result)
                    except Exception as e:
                        print(f"ê°ì„±ë¶„ì„ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
                        results.append({'positive': 0.0, 'negative': 0.0, 'neutral': 0.0})
            
            # ê²°ê³¼ë¥¼ DataFrameì— ì¶”ê°€
            sentiment_df = pd.DataFrame(results)
            for col in ['positive', 'negative', 'neutral']:
                df[col] = sentiment_df[col]
            
            # ê°ì„± ì ìˆ˜ ê³„ì‚°
            df['sentiment_score'] = df['positive'] - df['negative']
            
            print("ê°ì„±ë¶„ì„ ì™„ë£Œ")
            return df
            
        except Exception as e:
            print(f"ê°ì„±ë¶„ì„ ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ì‹œ ê¸°ë³¸ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
            df['positive'] = 0.5
            df['negative'] = 0.3
            df['neutral'] = 0.2
            df['sentiment_score'] = 0.2
            return df
    
    def add_finbert_embeddings(self, df):
        print("FinBERT ì„ë² ë”© ìƒì„± ì¤‘")
        
        try:
            from transformers import AutoTokenizer, AutoModel
            import torch
            
            # FinBERT ëª¨ë¸ ë¡œë“œ
            model_name = "jhgan/ko-sroberta-multitask"
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            model = AutoModel.from_pretrained(model_name)
            model.eval()
            
            embeddings = []
            
            for content in tqdm(df['content'].fillna(''), desc="ì„ë² ë”© ìƒì„±"):
                try:
                    if not content or len(str(content).strip()) < 10:
                        # ë¹ˆ ì»¨í…ì¸ ëŠ” ì œë¡œ ë²¡í„°
                        embedding = np.zeros(768)
                    else:
                        # í† í°í™” ë° ì„ë² ë”© ìƒì„±
                        inputs = tokenizer(str(content)[:512], return_tensors="pt", 
                                         truncation=True, padding=True, max_length=512)
                        
                        with torch.no_grad():
                            outputs = model(**inputs)
                            # [CLS] í† í°ì˜ ì„ë² ë”© ì‚¬ìš©
                            embedding = outputs.last_hidden_state[0][0].numpy()
                    
                    embeddings.append(embedding)
                    
                except Exception as e:
                    print(f"ì„ë² ë”© ìƒì„± ì˜¤ë¥˜: {e}")
                    embeddings.append(np.zeros(768))
            
            # FinBERT ì»¬ëŸ¼ ì¶”ê°€
            for i in range(768):
                df[f'finbert_{i}'] = [emb[i] if len(emb) > i else 0 for emb in embeddings]
            
            print("FinBERT ì„ë² ë”© ì™„ë£Œ")
            return df
            
        except Exception as e:
            print(f"FinBERT ì„ë² ë”© ì‹¤íŒ¨: {e}")
            print("ê¸°ë³¸ê°’ìœ¼ë¡œ ì„ë² ë”© ìƒì„±")
            
            # ì‹¤íŒ¨ì‹œ ëœë¤ ì„ë² ë”© ìƒì„±
            for i in range(768):
                df[f'finbert_{i}'] = np.random.normal(0, 0.01, len(df))
            
            return df
    
    def add_technical_scores(self, df):
        """ê¸°ìˆ ì  ë¶„ì„ ì ìˆ˜ ê³„ì‚° (yfinance ì‚¬ìš©)"""
        print("ğŸ“ˆ ê¸°ìˆ ì  ë¶„ì„ ì ìˆ˜ ê³„ì‚° ì¤‘...")
        
        try:
            import yfinance as yf
            
            # ì¢…ëª© ë§¤í•‘ íŒŒì¼ì—ì„œ ticker ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            mapping_file = os.path.join(os.path.dirname(__file__), 'data/sp500_korean_stocks_with_symbols.xlsx')
            if os.path.exists(mapping_file):
                df_mapping = pd.read_excel(mapping_file, header=1)
                df_mapping.dropna(subset=['Symbol'], inplace=True)
                name_to_ticker = pd.Series(df_mapping.Symbol.values, 
                                         index=df_mapping['Korean Name'].str.strip()).to_dict()
            else:
                # ê¸°ë³¸ ë§¤í•‘
                name_to_ticker = {
                    'ì—”ë¹„ë””ì•„': 'NVDA', 'ì‚¼ì„±ì „ì': '005930.KS', 'ì• í”Œ': 'AAPL',
                    'ë§ˆì´í¬ë¡œì†Œí”„íŠ¸': 'MSFT', 'êµ¬ê¸€': 'GOOGL', 'í…ŒìŠ¬ë¼': 'TSLA'
                }
            
            # ticker ì»¬ëŸ¼ ì¶”ê°€
            df['ticker'] = df['original_stock'].str.strip('$').map(name_to_ticker).fillna('UNKNOWN')
            
            # ê° ì¢…ëª©ë³„ë¡œ ê¸°ìˆ ì  ì ìˆ˜ ê³„ì‚°
            technical_scores = []
            unique_tickers = df['ticker'].unique()
            
            for ticker in tqdm(unique_tickers, desc="ê¸°ìˆ ì  ë¶„ì„"):
                if ticker == 'UNKNOWN':
                    ticker_scores = {'rule_score': 50.0, 'momentum_score': 50.0, 'volume_score': 50.0}
                else:
                    try:
                        # 30ì¼ê°„ ì£¼ê°€ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
                        stock = yf.Ticker(ticker)
                        hist = stock.history(period="1mo")
                        
                        if len(hist) > 10:
                            # RSI ê³„ì‚° (ê°„ë‹¨ ë²„ì „)
                            closes = hist['Close'].values
                            gains = []
                            losses = []
                            for i in range(1, len(closes)):
                                change = closes[i] - closes[i-1]
                                if change > 0:
                                    gains.append(change)
                                    losses.append(0)
                                else:
                                    gains.append(0)
                                    losses.append(-change)
                            
                            avg_gain = np.mean(gains[-14:]) if len(gains) >= 14 else np.mean(gains)
                            avg_loss = np.mean(losses[-14:]) if len(losses) >= 14 else np.mean(losses)
                            
                            if avg_loss != 0:
                                rs = avg_gain / avg_loss
                                rsi = 100 - (100 / (1 + rs))
                            else:
                                rsi = 50
                            
                            # ëª¨ë©˜í…€ ì ìˆ˜ (ìµœê·¼ 5ì¼ vs ì´ì „ 5ì¼)
                            if len(closes) >= 10:
                                recent_avg = np.mean(closes[-5:])
                                previous_avg = np.mean(closes[-10:-5])
                                momentum = ((recent_avg - previous_avg) / previous_avg) * 100 + 50
                                momentum = np.clip(momentum, 0, 100)
                            else:
                                momentum = 50
                            
                            # ê±°ë˜ëŸ‰ ì ìˆ˜ (ìµœê·¼ ê±°ë˜ëŸ‰ vs í‰ê·  ê±°ë˜ëŸ‰)
                            volumes = hist['Volume'].values
                            if len(volumes) >= 5:
                                recent_volume = np.mean(volumes[-5:])
                                avg_volume = np.mean(volumes)
                                if avg_volume > 0:
                                    volume_score = min((recent_volume / avg_volume) * 50, 100)
                                else:
                                    volume_score = 50
                            else:
                                volume_score = 50
                            
                            ticker_scores = {
                                'rule_score': rsi,
                                'momentum_score': momentum,
                                'volume_score': volume_score
                            }
                        else:
                            ticker_scores = {'rule_score': 50.0, 'momentum_score': 50.0, 'volume_score': 50.0}
                            
                    except Exception as e:
                        print(f"âš ï¸ {ticker} ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
                        ticker_scores = {'rule_score': 50.0, 'momentum_score': 50.0, 'volume_score': 50.0}
                
                technical_scores.append({'ticker': ticker, **ticker_scores})
            
            # ì ìˆ˜ë¥¼ DataFrameì— ë§¤í•‘
            scores_df = pd.DataFrame(technical_scores)
            df = df.merge(scores_df, on='ticker', how='left')
            
            print("ê¸°ìˆ ì  ë¶„ì„ ì ìˆ˜ ê³„ì‚° ì™„ë£Œ")
            return df
            
        except Exception as e:
            print(f"ê¸°ìˆ ì  ë¶„ì„ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            
            df['ticker'] = df['original_stock'].str.strip('$').str[:4]
            df['rule_score'] = 50.0
            df['momentum_score'] = 50.0
            df['volume_score'] = 50.0
            return df
    
    # ì„¹í„° ë° ì‹œê°€ì´ì•¡ ì •ë³´ ì¶”ê°€
    def add_sector_and_market_cap(self, df):
        print("ì„¹í„° ë° ì‹œê°€ì´ì•¡ ì •ë³´ ì¶”ê°€ ì¤‘")
        
        try:
            import yfinance as yf
            
            unique_tickers = df['ticker'].unique()
            sector_data = []
            
            for ticker in tqdm(unique_tickers, desc="ì„¹í„°/ì‹œì´ ì •ë³´"):
                if ticker == 'UNKNOWN':
                    sector_info = {'ticker': ticker, 'sector': 'Unknown', 'market_cap': 100000000000}
                else:
                    try:
                        stock = yf.Ticker(ticker)
                        info = stock.info
                        
                        sector_info = {
                            'ticker': ticker,
                            'sector': info.get('sector', 'Unknown'),
                            'market_cap': info.get('marketCap', 100000000000)
                        }
                    except Exception as e:
                        print(f"{ticker} ì •ë³´ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
                        sector_info = {'ticker': ticker, 'sector': 'Unknown', 'market_cap': 100000000000}
                
                sector_data.append(sector_info)
            
            # ì •ë³´ë¥¼ DataFrameì— ë§¤í•‘
            sector_df = pd.DataFrame(sector_data)
            df = df.merge(sector_df, on='ticker', how='left')
            
            print("âœ… ì„¹í„° ë° ì‹œê°€ì´ì•¡ ì •ë³´ ì¶”ê°€ ì™„ë£Œ")
            return df
            
        except Exception as e:
            print(f"ì„¹í„° ì •ë³´ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            df['sector'] = 'Unknown'
            df['market_cap'] = 100000000000
            return df
    
    def save_processed_data(self, df):
        """ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M')
        
        # ê¸°ì¡´ data í´ë”ì— ì €ì¥
        data_dir = './data'
        os.makedirs(data_dir, exist_ok=True)
        
        filename = f"news_full_features_robust_{timestamp}.csv"
        filepath = os.path.join(data_dir, filename)
        
        df.to_csv(filepath, index=False, encoding='utf-8-sig')
        print(f"ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {filepath}")
        
        # ìµœì‹  íŒŒì¼ë¡œ ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„±
        latest_filepath = os.path.join(data_dir, "news_full_features_robust.csv")
        try:
            if os.path.exists(latest_filepath):
                os.remove(latest_filepath)
            df.to_csv(latest_filepath, index=False, encoding='utf-8-sig')
            print(f"ìµœì‹  ë°ì´í„° ë§í¬ ì—…ë°ì´íŠ¸: {latest_filepath}")
        except Exception as e:
            print(f"ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„± ì‹¤íŒ¨: {e}")
        
        return filepath

def main():
    print("=" * 60)
    print("ë‰´ìŠ¤ ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    print("=" * 60)
    
    processor = NewsProcessor()
    
    try:
        # 1. MongoDBì—ì„œ ìƒˆë¡œìš´ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
        print("ğŸ“¡ MongoDBì—ì„œ ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ ì¤‘...")
        df = processor.get_new_news_from_mongo()
        
        if df.empty:
            print("ì²˜ë¦¬í•  ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"{len(df)}ê°œ ë‰´ìŠ¤ ë¡œë“œ ì™„ë£Œ")
        
        # 2. ê°ì„±ë¶„ì„ ì‹¤í–‰
        df = processor.run_sentiment_analysis(df)
        
        # 3. FinBERT ì„ë² ë”© ì¶”ê°€
        df = processor.add_finbert_embeddings(df)
        
        # 4. ê¸°ìˆ ì  ë¶„ì„ ì ìˆ˜ ì¶”ê°€ (ticker í¬í•¨)
        df = processor.add_technical_scores(df)
        
        # 5. ì„¹í„° ë° ì‹œê°€ì´ì•¡ ì •ë³´ ì¶”ê°€
        df = processor.add_sector_and_market_cap(df)
        
        # 6. sentence_count ì»¬ëŸ¼ ì¶”ê°€ (ëˆ„ë½ëœ ê²½ìš°)
        if 'sentence_count' not in df.columns:
            df['sentence_count'] = df['content'].fillna('').str.split('.').str.len()
        
        # 7. ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥
        saved_path = processor.save_processed_data(df)
        
        print("=" * 60)
        print("ë‰´ìŠ¤ ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
        print(f"ì €ì¥ëœ íŒŒì¼: {saved_path}")
        print(f"ì´ ì²˜ë¦¬ëœ ë‰´ìŠ¤: {len(df)}ê°œ")
        print("=" * 60)
        
    except Exception as e:
        print(f"íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
    finally:
        if processor.client:
            processor.client.close()

if __name__ == "__main__":
    main()