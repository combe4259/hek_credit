#!/usr/bin/env python3
"""
ìˆœìˆ˜ ë¹„ì§€ë„í•™ìŠµ íŒ¨í„´ ë°œê²¬
ê·œì¹™ì´ë‚˜ ë¼ë²¨ ì—†ì´ ë°ì´í„°ì—ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ë‚˜íƒ€ë‚˜ëŠ” íŒ¨í„´ì„ ë°œê²¬
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict
import json
import os

class PureUnsupervisedDiscovery:
    """ì™„ì „í•œ ë¹„ì§€ë„í•™ìŠµìœ¼ë¡œ íŒ¨í„´ì„ ë°œê²¬í•˜ëŠ” ì‹œìŠ¤í…œ"""
    
    def __init__(self, transactions_path, customers_path):
        print("ğŸ§  ìˆœìˆ˜ ë¹„ì§€ë„í•™ìŠµ ì‹œìŠ¤í…œ ì´ˆê¸°í™”...")
        self.transactions_df = pd.read_csv(transactions_path)
        self.customers_df = pd.read_csv(customers_path)
        self.transactions_df['timestamp'] = pd.to_datetime(self.transactions_df['timestamp'])
        
    def extract_pure_features(self):
        """ë¼ë²¨ì´ë‚˜ ê·œì¹™ ì—†ì´ ìˆœìˆ˜í•˜ê²Œ íŠ¹ì§• ì¶”ì¶œ"""
        print("\nğŸ“Š ê±°ë˜ íŠ¹ì§• ì¶”ì¶œ ì¤‘...")
        
        all_features = []
        
        # ê° ê³ ê°ì˜ ê±°ë˜ íŒ¨í„´ì„ ì‹œê³„ì—´ë¡œ ì¶”ì¶œ
        for customer_id in self.transactions_df['customerID'].unique():
            customer_trades = self.transactions_df[
                self.transactions_df['customerID'] == customer_id
            ].sort_values('timestamp')
            
            if len(customer_trades) < 10:
                continue
            
            # Buy-Sell ë§¤ì¹­ìœ¼ë¡œ ì‹¤í˜„ ì†ìµ ê³„ì‚°
            realized_trades = self._match_trades(customer_trades)
            
            # ê° ì‹¤í˜„ ê±°ë˜ì— ëŒ€í•œ ìˆœìˆ˜ íŠ¹ì§• ì¶”ì¶œ
            for trade in realized_trades:
                features = self._extract_trade_features(trade, customer_trades, realized_trades)
                if features:
                    all_features.append(features)
        
        self.features_df = pd.DataFrame(all_features)
        print(f"âœ… {len(self.features_df)}ê°œ ê±°ë˜ íŠ¹ì§• ì¶”ì¶œ ì™„ë£Œ")
        return self.features_df
    
    def _match_trades(self, trades):
        """Buy-Sell ë§¤ì¹­"""
        results = []
        positions = defaultdict(list)
        
        for _, trade in trades.iterrows():
            if trade['transactionType'] == 'Buy':
                positions[trade['ISIN']].append({
                    'buy_date': trade['timestamp'],
                    'buy_price': trade['totalValue'] / trade['units'],
                    'units': trade['units'],
                    'isin': trade['ISIN']
                })
            elif trade['transactionType'] == 'Sell' and positions[trade['ISIN']]:
                sell_price = trade['totalValue'] / trade['units']
                sell_date = trade['timestamp']
                
                position = positions[trade['ISIN']].pop(0)
                holding_days = (sell_date - position['buy_date']).days
                return_rate = (sell_price - position['buy_price']) / position['buy_price'] * 100
                
                results.append({
                    'customer_id': trade['customerID'],
                    'isin': trade['ISIN'],
                    'buy_date': position['buy_date'],
                    'sell_date': sell_date,
                    'holding_days': holding_days,
                    'return_rate': return_rate,
                    'trade_value': position['units'] * sell_price
                })
        
        return results
    
    def _extract_trade_features(self, trade, all_trades, all_results):
        """ê±°ë˜ì˜ ìˆœìˆ˜ íŠ¹ì§• ì¶”ì¶œ (ê·œì¹™ ì—†ì´)"""
        
        # ì´ì „ ê±°ë˜ë“¤
        past_results = [r for r in all_results if r['sell_date'] < trade['sell_date']]
        if len(past_results) < 3:
            return None
        
        # 1. í˜„ì¬ ê±°ë˜ì˜ ê¸°ë³¸ íŠ¹ì§•
        features = {
            # ìˆ˜ìµë¥ ê³¼ ë³´ìœ ê¸°ê°„
            'return_rate': trade['return_rate'],
            'holding_days': trade['holding_days'],
            'daily_return': trade['return_rate'] / trade['holding_days'] if trade['holding_days'] > 0 else 0,
            
            # ìˆ˜ìµë¥ ì˜ ì œê³±, ì„¸ì œê³± (ë¹„ì„ í˜• ê´€ê³„ í¬ì°©)
            'return_squared': trade['return_rate'] ** 2,
            'return_cubed': trade['return_rate'] ** 3,
            
            # ë¡œê·¸ ë³€í™˜ (ì™œë„ ë³´ì •)
            'log_holding_days': np.log1p(trade['holding_days']),
            'abs_return': abs(trade['return_rate']),
        }
        
        # 2. ê³¼ê±° í†µê³„ì™€ì˜ ê´€ê³„ (ìˆœìˆ˜ í†µê³„)
        past_returns = [r['return_rate'] for r in past_results]
        past_holdings = [r['holding_days'] for r in past_results]
        
        features.update({
            # ì´ë™ í‰ê· 
            'return_ma3': np.mean(past_returns[-3:]) if len(past_returns) >= 3 else 0,
            'holding_ma3': np.mean(past_holdings[-3:]) if len(past_holdings) >= 3 else 0,
            
            # ì´ë™ í‘œì¤€í¸ì°¨
            'return_std3': np.std(past_returns[-3:]) if len(past_returns) >= 3 else 0,
            'holding_std3': np.std(past_holdings[-3:]) if len(past_holdings) >= 3 else 0,
            
            # í˜„ì¬ê°’ê³¼ ì´ë™í‰ê· ì˜ ì°¨ì´
            'return_diff_ma': trade['return_rate'] - np.mean(past_returns[-5:]) if len(past_returns) >= 5 else 0,
            'holding_diff_ma': trade['holding_days'] - np.mean(past_holdings[-5:]) if len(past_holdings) >= 5 else 0,
            
            # Z-score (í‘œì¤€í™”ëœ ê±°ë¦¬)
            'return_zscore': (trade['return_rate'] - np.mean(past_returns)) / (np.std(past_returns) + 1e-6),
            'holding_zscore': (trade['holding_days'] - np.mean(past_holdings)) / (np.std(past_holdings) + 1e-6),
        })
        
        # 3. ì‹œê³„ì—´ íŠ¹ì§•
        # ì´ì „ ê±°ë˜ì™€ì˜ ê°„ê²© ê³„ì‚°
        prev_trades = all_trades[all_trades['timestamp'] < trade['buy_date']]
        if not prev_trades.empty:
            last_trade_time = prev_trades['timestamp'].max()
            days_since_last = (trade['buy_date'] - last_trade_time).days
        else:
            days_since_last = 30  # ì²« ê±°ë˜ì¸ ê²½ìš°
        
        features.update({
            'days_since_last_trade': days_since_last,
            'log_days_since': np.log1p(days_since_last),
            
            # ê±°ë˜ ë¹ˆë„ (ìµœê·¼ 30ì¼)
            'trades_last_30d': len([t for t in all_trades['timestamp'] 
                                   if (trade['buy_date'] - t).days <= 30 and t < trade['buy_date']]),
            
            # ì‹œê°„ì  íŠ¹ì§•
            'buy_month': trade['buy_date'].month,
            'buy_dayofweek': trade['buy_date'].dayofweek,
            'sell_month': trade['sell_date'].month,
            'sell_dayofweek': trade['sell_date'].dayofweek,
        })
        
        # 4. ì—°ì†ì„± íŠ¹ì§•
        if len(past_results) >= 2:
            # ì—°ì† ìˆ˜ìµ/ì†ì‹¤
            last_two_returns = [r['return_rate'] for r in past_results[-2:]]
            features['consecutive_profit'] = all(r > 0 for r in last_two_returns)
            features['consecutive_loss'] = all(r < 0 for r in last_two_returns)
            features['return_momentum'] = last_two_returns[-1] - last_two_returns[-2]
        
        # 5. ìƒëŒ€ì  í¬ê¸°
        features['trade_size_ratio'] = trade['trade_value'] / np.mean([r['trade_value'] for r in past_results])
        
        # 6. ë³€ë™ì„±
        if len(past_returns) >= 5:
            features['return_volatility'] = np.std(past_returns[-5:])
            features['holding_volatility'] = np.std(past_holdings[-5:])
        
        # 7. ì¶”ê°€ ë¹„ì„ í˜• íŠ¹ì§•
        features['return_holding_interaction'] = trade['return_rate'] * trade['holding_days']
        features['return_per_sqrt_days'] = trade['return_rate'] / np.sqrt(trade['holding_days'] + 1)
        
        return features
    
    def discover_patterns(self, n_components=20):
        """PCA + K-meansë¡œ íŒ¨í„´ ë°œê²¬"""
        print("\nğŸ” íŒ¨í„´ ë°œê²¬ ì¤‘...")
        
        # íŠ¹ì§• ì •ê·œí™”
        feature_cols = [col for col in self.features_df.columns 
                       if col not in ['customer_id', 'isin', 'buy_date', 'sell_date']]
        
        X = self.features_df[feature_cols].fillna(0)
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # PCAë¡œ ì°¨ì› ì¶•ì†Œ
        pca = PCA(n_components=min(n_components, X_scaled.shape[1]))
        X_pca = pca.fit_transform(X_scaled)
        
        print(f"PCA ì„¤ëª… ë¶„ì‚°: {pca.explained_variance_ratio_.sum():.2%}")
        print("ì£¼ìš” ì„±ë¶„ë³„ ê¸°ì—¬ë„:")
        for i in range(min(5, len(pca.components_))):
            print(f"  PC{i+1}: {pca.explained_variance_ratio_[i]:.2%}")
        
        # ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ì°¾ê¸°
        silhouette_scores = []
        K = range(3, min(20, len(X) // 50))
        
        for k in K:
            kmeans = KMeans(n_clusters=k, random_state=42)
            labels = kmeans.fit_predict(X_pca)
            score = silhouette_score(X_pca, labels)
            silhouette_scores.append(score)
            print(f"k={k}: ì‹¤ë£¨ì—£ ì ìˆ˜ = {score:.3f}")
        
        # ìµœì  k ì„ íƒ
        optimal_k = K[np.argmax(silhouette_scores)]
        print(f"\nìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜: {optimal_k}")
        
        # ìµœì¢… í´ëŸ¬ìŠ¤í„°ë§
        kmeans = KMeans(n_clusters=optimal_k, random_state=42)
        self.features_df['cluster'] = kmeans.fit_predict(X_pca)
        
        # í´ëŸ¬ìŠ¤í„° ë¶„ì„
        self._analyze_discovered_patterns(X_pca, pca, scaler, feature_cols)
        
        return self.features_df
    
    def _analyze_discovered_patterns(self, X_pca, pca, scaler, feature_cols):
        """ë°œê²¬ëœ íŒ¨í„´ ë¶„ì„ (ì‚¬í›„ í•´ì„)"""
        print("\nğŸ“Š ë°œê²¬ëœ íŒ¨í„´ ë¶„ì„...")
        
        for cluster_id in sorted(self.features_df['cluster'].unique()):
            cluster_data = self.features_df[self.features_df['cluster'] == cluster_id]
            
            print(f"\n=== í´ëŸ¬ìŠ¤í„° {cluster_id} ({len(cluster_data)}ê°œ ê±°ë˜) ===")
            
            # 1. ê¸°ë³¸ í†µê³„
            print("\nê¸°ë³¸ í†µê³„:")
            print(f"  í‰ê·  ìˆ˜ìµë¥ : {cluster_data['return_rate'].mean():.2f}%")
            print(f"  í‰ê·  ë³´ìœ ê¸°ê°„: {cluster_data['holding_days'].mean():.1f}ì¼")
            print(f"  ìˆ˜ìµë¥  ë¶„í¬: Q1={cluster_data['return_rate'].quantile(0.25):.1f}%, "
                  f"ì¤‘ì•™ê°’={cluster_data['return_rate'].median():.1f}%, "
                  f"Q3={cluster_data['return_rate'].quantile(0.75):.1f}%")
            
            # 2. ì£¼ìš” íŠ¹ì§• ì°¾ê¸° (í‰ê· ê³¼ì˜ ì°¨ì´)
            print("\níŠ¹ì§•ì ì¸ ì†ì„± (ì „ì²´ í‰ê·  ëŒ€ë¹„):")
            overall_means = self.features_df[feature_cols].mean()
            cluster_means = cluster_data[feature_cols].mean()
            
            # í‘œì¤€í™”ëœ ì°¨ì´ ê³„ì‚°
            std_diff = (cluster_means - overall_means) / (self.features_df[feature_cols].std() + 1e-6)
            top_features = std_diff.abs().nlargest(5)
            
            for feature in top_features.index:
                cluster_val = cluster_means[feature]
                overall_val = overall_means[feature]
                diff_pct = (cluster_val - overall_val) / (abs(overall_val) + 1e-6) * 100
                
                if abs(diff_pct) > 20:  # 20% ì´ìƒ ì°¨ì´ë‚˜ëŠ” íŠ¹ì§•ë§Œ
                    print(f"  - {feature}: {cluster_val:.2f} (ì „ì²´: {overall_val:.2f}, "
                          f"ì°¨ì´: {diff_pct:+.1f}%)")
            
            # 3. PCA ê³µê°„ì—ì„œì˜ ìœ„ì¹˜
            cluster_indices = cluster_data.index
            cluster_pca_points = X_pca[cluster_indices]
            centroid = cluster_pca_points.mean(axis=0)
            
            print(f"\nPCA ê³µê°„ ì¤‘ì‹¬ì : PC1={centroid[0]:.2f}, PC2={centroid[1]:.2f}")
            
            # 4. í–‰ë™ íŒ¨í„´ í•´ì„ (ë°ì´í„° ê¸°ë°˜)
            interpretation = self._interpret_cluster(cluster_data, overall_means)
            print(f"\ní•´ì„: {interpretation}")
    
    def _interpret_cluster(self, cluster_data, overall_means):
        """í´ëŸ¬ìŠ¤í„°ì˜ ìì—°ìŠ¤ëŸ¬ìš´ í•´ì„ (ë°ì´í„° ê¸°ë°˜)"""
        interpretations = []
        
        # ìˆ˜ìµë¥  ê¸°ë°˜
        avg_return = cluster_data['return_rate'].mean()
        if avg_return > overall_means['return_rate'] * 1.5:
            interpretations.append("ë†’ì€ ìˆ˜ìµë¥ ")
        elif avg_return < overall_means['return_rate'] * 0.5:
            interpretations.append("ë‚®ì€ ìˆ˜ìµë¥ ")
        
        # ë³´ìœ ê¸°ê°„ ê¸°ë°˜
        avg_holding = cluster_data['holding_days'].mean()
        if avg_holding > overall_means['holding_days'] * 1.5:
            interpretations.append("ì¥ê¸° ë³´ìœ ")
        elif avg_holding < overall_means['holding_days'] * 0.5:
            interpretations.append("ë‹¨ê¸° ê±°ë˜")
        
        # ë³€ë™ì„± ê¸°ë°˜
        if 'return_volatility' in cluster_data.columns:
            if cluster_data['return_volatility'].mean() > overall_means['return_volatility'] * 1.3:
                interpretations.append("ë†’ì€ ë³€ë™ì„±")
        
        # Z-score ê¸°ë°˜
        if cluster_data['return_zscore'].abs().mean() > 1.5:
            interpretations.append("ê·¹ë‹¨ì  ê±°ë˜")
        
        return " + ".join(interpretations) if interpretations else "ì¶”ê°€ ë¶„ì„ í•„ìš”"
    
    def visualize_patterns(self, output_dir='output'):
        """íŒ¨í„´ ì‹œê°í™”"""
        os.makedirs(output_dir, exist_ok=True)
        
        # t-SNEë¡œ 2D ì‹œê°í™”
        print("\nğŸ“ˆ íŒ¨í„´ ì‹œê°í™” ì¤‘...")
        
        feature_cols = [col for col in self.features_df.columns 
                       if col not in ['customer_id', 'isin', 'buy_date', 'sell_date', 'cluster']]
        
        X = self.features_df[feature_cols].fillna(0)
        X_scaled = StandardScaler().fit_transform(X)
        
        # ìƒ˜í”Œë§ (ë„ˆë¬´ ë§ìœ¼ë©´)
        if len(X) > 5000:
            sample_idx = np.random.choice(len(X), 5000, replace=False)
            X_scaled_sample = X_scaled[sample_idx]
            clusters_sample = self.features_df['cluster'].iloc[sample_idx]
        else:
            X_scaled_sample = X_scaled
            clusters_sample = self.features_df['cluster']
        
        # t-SNE
        tsne = TSNE(n_components=2, random_state=42, perplexity=30)
        X_tsne = tsne.fit_transform(X_scaled_sample)
        
        # ì‹œê°í™”
        plt.figure(figsize=(12, 8))
        scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], 
                            c=clusters_sample, cmap='tab10', alpha=0.6)
        plt.colorbar(scatter)
        plt.title('ê±°ë˜ íŒ¨í„´ í´ëŸ¬ìŠ¤í„° (t-SNE ì‹œê°í™”)')
        plt.xlabel('t-SNE 1')
        plt.ylabel('t-SNE 2')
        
        # í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ í‘œì‹œ
        for cluster_id in clusters_sample.unique():
            cluster_points = X_tsne[clusters_sample == cluster_id]
            center = cluster_points.mean(axis=0)
            plt.annotate(f'C{cluster_id}', center, fontsize=12, fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'pattern_clusters_tsne.png'))
        plt.close()
        
        print(f"âœ… ì‹œê°í™” ì €ì¥: {output_dir}/pattern_clusters_tsne.png")
    
    def save_results(self, output_dir='output'):
        """ê²°ê³¼ ì €ì¥"""
        os.makedirs(output_dir, exist_ok=True)
        
        # 1. íŒ¨í„´ ë°ì´í„° ì €ì¥
        patterns_path = os.path.join(output_dir, 'discovered_patterns.csv')
        self.features_df.to_csv(patterns_path, index=False, encoding='utf-8-sig')
        
        # 2. í´ëŸ¬ìŠ¤í„° ìš”ì•½ ì €ì¥
        summary = []
        for cluster_id in sorted(self.features_df['cluster'].unique()):
            cluster_data = self.features_df[self.features_df['cluster'] == cluster_id]
            summary.append({
                'cluster': cluster_id,
                'size': len(cluster_data),
                'avg_return': cluster_data['return_rate'].mean(),
                'std_return': cluster_data['return_rate'].std(),
                'avg_holding': cluster_data['holding_days'].mean(),
                'return_q25': cluster_data['return_rate'].quantile(0.25),
                'return_median': cluster_data['return_rate'].median(),
                'return_q75': cluster_data['return_rate'].quantile(0.75)
            })
        
        summary_df = pd.DataFrame(summary)
        summary_path = os.path.join(output_dir, 'cluster_summary.csv')
        summary_df.to_csv(summary_path, index=False, encoding='utf-8-sig')
        
        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥:")
        print(f"  - íŒ¨í„´ ë°ì´í„°: {patterns_path}")
        print(f"  - í´ëŸ¬ìŠ¤í„° ìš”ì•½: {summary_path}")


def main():
    # ë°ì´í„° ê²½ë¡œ
    transactions_path = "/Users/inter4259/Downloads/FAR-Trans/transactions.csv"
    customers_path = "/Users/inter4259/Downloads/FAR-Trans/customer_information.csv"
    
    # ìˆœìˆ˜ ë¹„ì§€ë„í•™ìŠµ ì‹¤í–‰
    discoverer = PureUnsupervisedDiscovery(transactions_path, customers_path)
    
    # 1. íŠ¹ì§• ì¶”ì¶œ
    features = discoverer.extract_pure_features()
    
    # 2. íŒ¨í„´ ë°œê²¬
    patterns = discoverer.discover_patterns()
    
    # 3. ì‹œê°í™”
    discoverer.visualize_patterns()
    
    # 4. ì €ì¥
    discoverer.save_results()
    
    print("\nâœ… ìˆœìˆ˜ ë¹„ì§€ë„í•™ìŠµ ì™„ë£Œ!")
    print("ê·œì¹™ ì—†ì´ ë°ì´í„°ì—ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ë°œê²¬ëœ íŒ¨í„´ì„ í™•ì¸í•˜ì„¸ìš”.")


if __name__ == "__main__":
    main()