import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import Ridge, ElasticNet
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import GridSearchCV
import xgboost as xgb
import lightgbm as lgb
import joblib
import warnings
warnings.filterwarnings('ignore')

def create_quality_score_label(df, w1=0.4, w2=0.3, w3=0.3):
    """
    ê°œì„ ëœ í’ˆì§ˆ ì ìˆ˜ ë¼ë²¨ ìƒì„± (return_pctì™€ ì–‘ì˜ ìƒê´€ê´€ê³„ í™•ë³´)
    """
    df = df.copy()
    
    # 1. ì´ìƒì¹˜ ì²˜ë¦¬ (1%~99% ë²”ìœ„)
    for col in ['return_pct', 'holding_period_days', 'entry_momentum_20d', 'entry_volatility_20d']:
        if col in df.columns:
            lower = df[col].quantile(0.01)
            upper = df[col].quantile(0.99)
            df[col] = df[col].clip(lower, upper)
    
    # 2. ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    required_cols = ['return_pct', 'holding_period_days', 'entry_momentum_20d', 'entry_volatility_20d']
    available_cols = [col for col in required_cols if col in df.columns]
    df = df.dropna(subset=available_cols)
    
    if len(df) == 0:
        print("âš ï¸ Warning: No valid data after preprocessing")
        return df, {}
    
    # 3. ê°œì„ ëœ ì»´í¬ë„ŒíŠ¸ ê³„ì‚°
    
    # (1) ë¦¬ìŠ¤í¬ ì¡°ì • ìˆ˜ìµë¥ : ë³€ë™ì„±ìœ¼ë¡œ ë¦¬ìŠ¤í¬ ì¡°ì •
    if 'entry_volatility_20d' in df.columns:
        risk_adj_return = df['return_pct'] / (df['entry_volatility_20d'] + 0.01)
    else:
        # fallback: ë‹¨ìˆœ ìˆ˜ìµë¥ 
        risk_adj_return = df['return_pct']
    
    # (2) ì‹œì¥ ì¡°ê±´ ì ìˆ˜: ë³´ìœ ê¸°ê°„ì— ë§ëŠ” ë™ì  ì§€í‘œ ì„ íƒ
    def get_appropriate_indicators(row):
        holding_days = row['holding_period_days']
        
        # ë³´ìœ ê¸°ê°„ì— ë§ëŠ” ì§€í‘œ ì„ íƒ
        if holding_days <= 7:  # ë‹¨ê¸°: 5ì¼ ì§€í‘œ
            momentum = row.get('entry_momentum_5d', 0)
            volatility = row.get('entry_volatility_5d', 0.01)
        elif holding_days <= 30:  # ì¤‘ê¸°: 20ì¼ ì§€í‘œ  
            momentum = row.get('entry_momentum_20d', 0)
            volatility = row.get('entry_volatility_20d', 0.01)
        else:  # ì¥ê¸°: 60ì¼ ì§€í‘œ
            momentum = row.get('entry_momentum_60d', 0)
            volatility = row.get('entry_volatility_60d', 0.01)
        
        return momentum / (volatility + 0.01)
    
    print("ğŸ”„ ë³´ìœ ê¸°ê°„ë³„ ë™ì  ì§€í‘œ ê³„ì‚° ì¤‘...")
    market_condition_score = df.apply(get_appropriate_indicators, axis=1)
    
    # (3) ë³´ìœ  ê¸°ê°„ ì ìˆ˜: ë³´ìœ  ê¸°ê°„ ëŒ€ë¹„ ìˆ˜ìµë¥  (ì¼ì¼ í‰ê·  ìˆ˜ìµë¥ )
    holding_period_score = np.maximum(0, df['return_pct'] / (df['holding_period_days'] + 0.01))
    
    # 4. ë¬´í•œëŒ€ì™€ NaN ì²˜ë¦¬
    risk_adj_return = np.nan_to_num(risk_adj_return, nan=0.0, posinf=1.0, neginf=-1.0)
    market_condition_score = np.nan_to_num(market_condition_score, nan=0.0, posinf=1.0, neginf=-1.0)
    holding_period_score = np.nan_to_num(holding_period_score, nan=0.0, posinf=1.0, neginf=-1.0)
    
    # 5. RobustScalerë¡œ ì •ê·œí™” (ì´ìƒì¹˜ ì˜í–¥ ìµœì†Œí™”)
    from sklearn.preprocessing import RobustScaler
    
    scaler_risk = RobustScaler()
    scaler_market = RobustScaler()
    scaler_holding = RobustScaler()
    
    # ì •ê·œí™”í•  ë•Œë„ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
    try:
        normalized_risk_adj_return = scaler_risk.fit_transform(risk_adj_return.reshape(-1, 1)).flatten()
    except:
        normalized_risk_adj_return = np.zeros_like(risk_adj_return)
    
    try:
        normalized_market_condition = scaler_market.fit_transform(market_condition_score.reshape(-1, 1)).flatten()
    except:
        normalized_market_condition = np.zeros_like(market_condition_score)
    
    try:
        normalized_holding_period = scaler_holding.fit_transform(holding_period_score.reshape(-1, 1)).flatten()
    except:
        normalized_holding_period = np.zeros_like(holding_period_score)
    
    # 6. ìµœì¢… í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
    quality_score = (w1 * normalized_risk_adj_return + 
                    w2 * normalized_market_condition + 
                    w3 * normalized_holding_period)
    
    # ìµœì¢… NaN ì²˜ë¦¬
    quality_score = np.nan_to_num(quality_score, nan=0.5)
    df['quality_score'] = quality_score
    
    # 7. ì»´í¬ë„ŒíŠ¸ë³„ ìƒê´€ê´€ê³„ ë¶„ì„
    correlations = {}
    if len(df) > 10:  # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ
        correlations['risk_return_corr'] = np.corrcoef(risk_adj_return, df['return_pct'])[0, 1]
        correlations['market_return_corr'] = np.corrcoef(market_condition_score, df['return_pct'])[0, 1]
        correlations['holding_return_corr'] = np.corrcoef(holding_period_score, df['return_pct'])[0, 1]
        correlations['quality_return_corr'] = np.corrcoef(quality_score, df['return_pct'])[0, 1]
    
    print(f"\nğŸ“Š ê°œì„ ëœ Quality Score ìƒì„±:")
    print(f"  ê°€ì¤‘ì¹˜: Risk({w1:.1f}), Market({w2:.1f}), Holding({w3:.1f})")
    print(f"  í‰ê· : {quality_score.mean():.4f}")
    print(f"  í‘œì¤€í¸ì°¨: {quality_score.std():.4f}")
    print(f"  ë²”ìœ„: {quality_score.min():.4f} ~ {quality_score.max():.4f}")
    
    if correlations:
        print(f"\nğŸ” ì»´í¬ë„ŒíŠ¸-ìˆ˜ìµë¥  ìƒê´€ê´€ê³„:")
        print(f"  Risk ì¡°ì • ìˆ˜ìµë¥ : {correlations.get('risk_return_corr', 0):.4f}")
        print(f"  Market ì¡°ê±´: {correlations.get('market_return_corr', 0):.4f}")
        print(f"  Holding íš¨ìœ¨ì„±: {correlations.get('holding_return_corr', 0):.4f}")
        print(f"  ìµœì¢… Quality Score: {correlations.get('quality_return_corr', 0):.4f}")
    
    # ì •ê·œí™” ê°ì²´ë“¤ë„ ë°˜í™˜ (ì˜ˆì¸¡ì‹œ í•„ìš”)
    scalers = {
        'risk': scaler_risk,
        'market': scaler_market, 
        'holding': scaler_holding
    }
    
    return df, scalers

def prepare_advanced_features(df):
    """
    ì •êµí•œ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ (ë°ì´í„° ë¦¬í‚¤ì§€ ì œê±°)
    Quality Score ê³„ì‚°ì— ì‚¬ìš©ëœ ë³€ìˆ˜ë“¤ ì œì™¸:
    - return_pct, position_size_pct, holding_period_days
    - entry_momentum_5d, entry_volatility_5d (ê¸°ë³¸)
    - entry_momentum_20d, entry_volatility_20d (ë™ì  ì§€í‘œì—ì„œ ì‚¬ìš©)
    - entry_momentum_60d, entry_volatility_60d (ë™ì  ì§€í‘œì—ì„œ ì‚¬ìš©)
    """
    
    # ê¸°ë³¸ íŠ¹ì§•ë“¤ (Quality Scoreì— ì‚¬ìš©ëœ ëª¨ë“  ë³€ìˆ˜ ì œì™¸)
    features = [
        # === ì§„ì… ì‹œì  ê¸°ìˆ ì  ì§€í‘œ (ëª¨ë©˜í…€/ë³€ë™ì„± ì œì™¸) ===
        # entry_momentum_5d, entry_momentum_20d, entry_momentum_60d ëª¨ë‘ ì œì™¸
        # entry_volatility_5d, entry_volatility_20d, entry_volatility_60d ëª¨ë‘ ì œì™¸
        'entry_ma_dev_5d', 'entry_ma_dev_20d', 'entry_ma_dev_60d',
        'entry_vol_change_5d', 'entry_vol_change_20d', 'entry_vol_change_60d',
        'entry_ratio_52w_high',
        
        # === ì²­ì‚° ì‹œì  ê¸°ìˆ ì  ì§€í‘œ ===
        'exit_momentum_5d', 'exit_momentum_20d', 'exit_momentum_60d',
        'exit_volatility_5d', 'exit_volatility_20d', 'exit_volatility_60d',
        'exit_ma_dev_5d', 'exit_ma_dev_20d', 'exit_ma_dev_60d',
        'exit_vol_change_5d', 'exit_vol_change_20d', 'exit_vol_change_60d',
        'exit_ratio_52w_high',
        
        # === ì‹œì¥ ìƒí™© ì§€í‘œ ===
        'market_entry_ma_return_5d', 'market_entry_ma_return_20d',
        'market_entry_cum_return_5d', 'market_entry_cum_return_20d',
        'market_entry_volatility_20d', 'market_return_during_holding',
        
        # === ì¬ë¬´ ì§€í‘œ ===
        'entry_pe_ratio', 'entry_pb_ratio', 'entry_roe',
        'entry_operating_margin', 'entry_debt_equity_ratio',
        
        # === ë³€í™”ëŸ‰ ì§€í‘œ (ëª¨ë©˜í…€/ë³€ë™ì„± ê´€ë ¨ ì œì™¸) ===
        # 'change_momentum_5d', 'change_momentum_20d', 'change_momentum_60d', ì œì™¸
        # 'change_volatility_5d', 'change_volatility_20d', 'change_volatility_60d', ì œì™¸
        'change_ma_dev_5d', 'change_ma_dev_20d', 'change_ma_dev_60d',
        'change_ratio_52w_high'
    ]
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ íŠ¹ì§•ë§Œ ì„ íƒ
    available_features = [f for f in features if f in df.columns]
    X = df[available_features].copy()
    
    # === ê³ ê¸‰ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ (ë°ì´í„° ë¦¬í‚¤ì§€ ì œê±°) ===
    
    # 1. ë¦¬ìŠ¤í¬ ì¡°ì • ì§€í‘œë“¤ (ëª¨ë©˜í…€/ë³€ë™ì„± ì‚¬ìš© ê¸ˆì§€)
    # X['momentum_volatility_ratio_20d'] ì œê±° (ë°ì´í„° ë¦¬í‚¤ì§€)
    # X['momentum_volatility_ratio_60d'] ì œê±° (ë°ì´í„° ë¦¬í‚¤ì§€)
    
    # 2. ì‹œì¥ ëŒ€ë¹„ ìƒëŒ€ ì„±ê³¼ (ëª¨ë©˜í…€/ë³€ë™ì„± ì œê±°)
    # X['relative_volatility'] ì œê±° (entry_volatility_20d ì‚¬ìš© ê¸ˆì§€)
    
    # 3. íƒ€ì´ë° ì§€í‘œë“¤ (ëª¨ë©˜í…€/ë³€ë™ì„± ì œê±°)  
    # X['entry_timing_score'] ì œê±° (entry_momentum_20d, entry_volatility_20d ì‚¬ìš© ê¸ˆì§€)
    # X['exit_timing_score'] ì œê±° (exit_volatility_20d ì‚¬ìš©)
    X['exit_timing_score'] = (df['exit_momentum_20d'] * df['exit_ratio_52w_high']) / (df['exit_ma_dev_20d'].abs() + 0.01)
    
    # 4. ë³€í™”ìœ¨ ì§€í‘œë“¤ (ëª¨ë©˜í…€/ë³€ë™ì„± ì œê±°)
    # X['momentum_change_ratio'] ì œê±° (change_momentum_20d, entry_momentum_20d ì‚¬ìš© ê¸ˆì§€)  
    # X['volatility_stability'] ì œê±° (entry_volatility_20d, change_volatility_20d ì‚¬ìš© ê¸ˆì§€)
    
    # 6. ì‹œì¥ ì¡°ê±´ ì¢…í•© ì ìˆ˜
    if all(col in df.columns for col in ['market_entry_ma_return_20d', 'market_entry_volatility_20d']):
        X['market_condition_score'] = df['market_entry_ma_return_20d'] / (df['market_entry_volatility_20d'] + 0.01)
    
    # 7. ì¬ë¬´ ê±´ì „ì„± ì¢…í•© ì ìˆ˜
    if all(col in df.columns for col in ['entry_roe', 'entry_debt_equity_ratio', 'entry_operating_margin']):
        X['financial_health_score'] = (df['entry_roe'] * df['entry_operating_margin']) / (df['entry_debt_equity_ratio'] + 0.01)
    
    # 5. ê¸°ìˆ ì  ê°•ë„ ì§€í‘œ (ëª¨ë©˜í…€ ì œê±°)
    X['technical_strength'] = df['entry_ratio_52w_high'] / (df['entry_ma_dev_20d'].abs() + 0.01)
    
    # 6. ì§„ì…-ì²­ì‚° ë¹„êµ ì§€í‘œ (ëª¨ë©˜í…€/ë³€ë™ì„± ì œê±°)
    # X['momentum_consistency'] ì œê±° (entry_momentum_20d, exit_momentum_20d ì‚¬ìš© ê¸ˆì§€)
    # X['volatility_trend'] ì œê±° (entry_volatility_20d, exit_volatility_20d ì‚¬ìš© ê¸ˆì§€)
    X['ma_dev_trend'] = df['exit_ma_dev_20d'] / (df['entry_ma_dev_20d'] + 0.01)
    
    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    X = X.fillna(0)
    
    # ë¬´í•œëŒ€ ê°’ ì²˜ë¦¬
    X = X.replace([np.inf, -np.inf], 0)
    
    print(f"\nğŸ”§ ê³ ê¸‰ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ (ë°ì´í„° ë¦¬í‚¤ì§€ ì œê±°):")
    print(f"  ê¸°ë³¸ íŠ¹ì§•: {len(available_features)}ê°œ")
    print(f"  ì—”ì§€ë‹ˆì–´ë§ëœ íŠ¹ì§•: {len(X.columns) - len(available_features)}ê°œ")
    print(f"  ì´ íŠ¹ì§• ìˆ˜: {len(X.columns)}ê°œ")
    print(f"  ì œì™¸ëœ ë¦¬í‚¤ì§€ ë³€ìˆ˜: return_pct, position_size_pct, holding_period_days, entry_momentum_5d, entry_volatility_5d")
    
    return X

def evaluate_regression_model(model, X, y, dataset_name):
    """íšŒê·€ ëª¨ë¸ í‰ê°€"""
    
    y_pred = model.predict(X)
    
    # ê¸°ë³¸ ë©”íŠ¸ë¦­
    mse = mean_squared_error(y, y_pred)
    mae = mean_absolute_error(y, y_pred)
    r2 = r2_score(y, y_pred)
    rmse = np.sqrt(mse)
    
    # ìƒê´€ê³„ìˆ˜
    correlation = np.corrcoef(y, y_pred)[0, 1]
    
    metrics = {
        'mse': mse,
        'rmse': rmse,
        'mae': mae,
        'r2': r2,
        'correlation': correlation
    }
    
    print(f"\nğŸ“Š {dataset_name} ì„±ëŠ¥:")
    print(f"  RÂ² Score: {r2:.4f}")
    print(f"  RMSE: {rmse:.4f}")
    print(f"  MAE: {mae:.4f}")
    print(f"  ìƒê´€ê³„ìˆ˜: {correlation:.4f}")
    
    return metrics, y_pred

def optimize_quality_score_weights():
    """ê°€ì¤‘ì¹˜ ìµœì í™”ë¥¼ í†µí•´ ìƒ/í•˜ìœ„ 20% êµ¬ê°„ ê°„ ìˆ˜ìµë¥  ì°¨ì´ ìµœëŒ€í™”"""
    
    print("ğŸ¯ Quality Score ê°€ì¤‘ì¹˜ ìµœì í™”")
    print("=" * 60)
    
    # 1. ë°ì´í„° ë¡œë“œ
    print("\nğŸ“‚ ë°ì´í„° ë¡œë”© ì¤‘...")
    df = pd.read_csv('../results/final/enriched_trading_episodes_with_fundamentals.csv')
    
    # Train/Val ë¶„í•  (ê²€ì¦ ë°ì´í„°ë¡œ ìµœì í™”)
    n_total = len(df)
    n_train = int(n_total * 0.7)
    n_val = int(n_total * 0.1)
    
    val_df = df.iloc[n_train:n_train+n_val].copy()
    print(f"ê²€ì¦ ë°ì´í„°: {len(val_df):,}ê°œ")
    
    # ì—¬ëŸ¬ ì´ˆê¸°ê°’ í›„ë³´ë“¤ (Market ì¡°ê±´ ìŒìˆ˜ ìƒê´€ê´€ê³„ ê³ ë ¤)
    initial_weight_candidates = [
        [0.4, 0.3, 0.3],   # í˜„ì¬
        [0.7, 0.2, 0.1],   # Risk ìœ„ì£¼
        [0.8, 0.1, 0.1],   # Risk ê·¹ëŒ€í™”
        [0.3, 0.1, 0.6],   # Holding ìœ„ì£¼ (ìƒê´€ê³„ìˆ˜ 0.28)
        [0.5, 0.0, 0.5],   # Market ì œì™¸
        [0.6, 0.05, 0.35], # Risk + Holding
        [0.33, 0.33, 0.34] # ê· ë“±
    ]
    
    # ê¸°ë³¸ ì´ˆê¸°ê°’
    initial_weights = [0.4, 0.3, 0.3]
    
    def calculate_bucket_return_difference(weights):
        """ê°€ì¤‘ì¹˜ì— ë”°ë¥¸ ìƒ/í•˜ìœ„ 20% êµ¬ê°„ ìˆ˜ìµë¥  ì°¨ì´ ê³„ì‚°"""
        if len(weights) != 3:
            print(f"Warning: weights length is {len(weights)}, expected 3. Weights: {weights}")
            return 0
        w1, w2, w3 = weights
        
        # Quality score ê³„ì‚°
        quality_scores, _ = create_quality_score_label(val_df, w1, w2, w3)
        
        # ìƒ/í•˜ìœ„ 20% êµ¬ê°„ ë¶„í• 
        val_df_sorted = quality_scores.sort_values('quality_score')
        n = len(val_df_sorted)
        bottom_20 = val_df_sorted.iloc[:int(n*0.2)]
        top_20 = val_df_sorted.iloc[int(n*0.8):]
        
        # ìˆ˜ìµë¥  ì°¨ì´ (ìƒìœ„ - í•˜ìœ„)
        return_diff = top_20['return_pct'].mean() - bottom_20['return_pct'].mean()
        
        return return_diff
    
    def objective_function(weights):
        """ìµœì í™” ëª©ì í•¨ìˆ˜ (ì°¨ì´ë¥¼ ìµœëŒ€í™”í•˜ë¯€ë¡œ ìŒìˆ˜ ë°˜í™˜)"""
        return -calculate_bucket_return_difference(weights)
    
    # 2. ì œì•½ì¡°ê±´ ì„¤ì • (SLSQP ì‚¬ìš©)
    from scipy.optimize import minimize
    
    # ê° ê°€ì¤‘ì¹˜ëŠ” 0 ì´ìƒ 1 ì´í•˜
    bounds = [(0.0, 1.0), (0.0, 1.0), (0.0, 1.0)]
    
    # ê°€ì¤‘ì¹˜ í•©ì´ 1ì´ ë˜ë„ë¡ ì œì•½
    constraints = {'type': 'eq', 'fun': lambda w: w[0] + w[1] + w[2] - 1.0}
    
    # í˜„ì¬ ì„±ëŠ¥ í™•ì¸
    current_diff = calculate_bucket_return_difference(initial_weights)
    print(f"\nğŸ“Š í˜„ì¬ ê°€ì¤‘ì¹˜ ì„±ëŠ¥:")
    print(f"  ê°€ì¤‘ì¹˜: Risk({initial_weights[0]:.1f}), Market({initial_weights[1]:.1f}), Holding({initial_weights[2]:.1f})")
    print(f"  ìˆ˜ìµë¥  ì°¨ì´: {current_diff:.4f} (ìƒìœ„ 20% - í•˜ìœ„ 20%)")
    
    # 3. ì—¬ëŸ¬ ì´ˆê¸°ê°’ìœ¼ë¡œ ìµœì í™” ì‹œë„
    print(f"\nğŸ” ê°€ì¤‘ì¹˜ ìµœì í™” ì¤‘ (ì—¬ëŸ¬ ì´ˆê¸°ê°’ìœ¼ë¡œ ì‹œë„)...")
    
    best_result = None
    best_diff = current_diff
    best_weights = initial_weights
    
    for i, candidate in enumerate(initial_weight_candidates):
        print(f"\n--- SLSQP ì‹œë„ {i+1}/{len(initial_weight_candidates)}: {candidate} ---")
        
        try:
            # ê° í›„ë³´ë¡œ ì„±ëŠ¥ í™•ì¸
            candidate_diff = calculate_bucket_return_difference(candidate)
            print(f"ì´ˆê¸° ì„±ëŠ¥: {candidate_diff:.4f}")
            
            # SLSQP ìµœì í™” ì‹¤í–‰
            result = minimize(
                objective_function,
                candidate,  # ì´ˆê¸°ê°’
                method='SLSQP',
                bounds=bounds,
                constraints=constraints,
                options={'disp': False, 'maxiter': 100}
            )
            
            if result.success:
                opt_diff = calculate_bucket_return_difference(result.x)
                print(f"ìµœì í™” ê²°ê³¼: {result.x} â†’ ì„±ëŠ¥: {opt_diff:.4f}")
                print(f"ê°œì„ í­: {opt_diff - candidate_diff:.4f}")
                
                if opt_diff > best_diff:
                    best_result = result
                    best_diff = opt_diff
                    best_weights = result.x
                    print(f"âœ… ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥!")
            else:
                print(f"âŒ ìµœì í™” ì‹¤íŒ¨: {result.message}")
                
        except Exception as e:
            print(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")
            continue
    
    # ìµœì¢… ê²°ê³¼ ì„¤ì •
    if best_result and best_result.success:
        result = best_result
        optimal_weights = best_weights
        optimal_diff = best_diff
    else:
        print(f"\nğŸ”„ ëª¨ë“  ìµœì í™” ì‹œë„ ì‹¤íŒ¨ - ìˆ˜ë™ìœ¼ë¡œ í›„ë³´ ì¤‘ ìµœê³  ì„ íƒ")
        # í›„ë³´ë“¤ ì¤‘ ê°€ì¥ ì¢‹ì€ ê²ƒ ì„ íƒ
        best_candidate = initial_weights
        for candidate in initial_weight_candidates:
            candidate_diff = calculate_bucket_return_difference(candidate)
            if candidate_diff > best_diff:
                best_diff = candidate_diff
                best_candidate = candidate
        
        optimal_weights = best_candidate
        optimal_diff = best_diff
        result = type('obj', (object,), {'success': best_diff > current_diff})()
    
    print(f"\nğŸ† ìµœì¢… ìµœì í™” ê²°ê³¼:")
    print(f"  ìµœì  ê°€ì¤‘ì¹˜: Risk({optimal_weights[0]:.3f}), Market({optimal_weights[1]:.3f}), Holding({optimal_weights[2]:.3f})")
    print(f"  ìµœì í™”ëœ ìˆ˜ìµë¥  ì°¨ì´: {optimal_diff:.4f}")
    print(f"  ê°œì„ í­: {optimal_diff - current_diff:.4f}")
    
    return optimal_weights, current_diff, optimal_diff, result.success if hasattr(result, 'success') else True

def train_quality_score_model_with_optimized_weights(optimal_weights):
    """ìµœì í™”ëœ ê°€ì¤‘ì¹˜ë¡œ ê±°ë˜ í’ˆì§ˆ ì ìˆ˜ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ"""
    
    print("ğŸ¯ ê±°ë˜ í’ˆì§ˆ ì ìˆ˜ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ V2 (ìµœì í™”ëœ ê°€ì¤‘ì¹˜)")
    print("=" * 60)
    
    # 1. ë°ì´í„° ë¡œë“œ
    print("\nğŸ“‚ ë°ì´í„° ë¡œë”© ì¤‘...")
    df = pd.read_csv('../results/final/enriched_trading_episodes_with_fundamentals.csv')
    print(f"ì „ì²´ ë°ì´í„°: {len(df):,}ê°œ")
    
    # 2. Quality Score ë¼ë²¨ ìƒì„± (ìµœì í™”ëœ ê°€ì¤‘ì¹˜ ì‚¬ìš©)
    print(f"\nğŸ·ï¸ Quality Score ë¼ë²¨ ìƒì„± ì¤‘ (ìµœì í™”ëœ ê°€ì¤‘ì¹˜)...")
    print(f"ğŸ” ì „ë‹¬í•  ê°€ì¤‘ì¹˜ í™•ì¸: Risk({optimal_weights[0]:.3f}), Market({optimal_weights[1]:.3f}), Holding({optimal_weights[2]:.3f})")
    df, scalers = create_quality_score_label(df, optimal_weights[0], optimal_weights[1], optimal_weights[2])
    
    # NaN ì²´í¬
    print(f"Quality Score NaN í™•ì¸: {df['quality_score'].isna().sum()}ê°œ")
    
    # 3. ê³ ê¸‰ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§
    print("\nâš™ï¸ ê³ ê¸‰ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ ì¤‘...")
    X = prepare_advanced_features(df)
    y = df['quality_score']
    
    # íŠ¹ì§•ê³¼ ë¼ë²¨ ìµœì¢… í™•ì¸
    print(f"\nğŸ” ìµœì¢… ë°ì´í„° í™•ì¸:")
    print(f"  X shape: {X.shape}")
    print(f"  y shape: {y.shape}")
    print(f"  Xì— NaN: {X.isna().sum().sum()}ê°œ")
    print(f"  Xì— Inf: {np.isinf(X).sum().sum()}ê°œ")
    print(f"  yì— NaN: {y.isna().sum()}ê°œ")
    print(f"  yì— Inf: {np.isinf(y).sum()}ê°œ")
    
    # 4. Train/Val/Test ë¶„í•  (ì‹œê°„ìˆœ)
    print("\nğŸ“Š ë°ì´í„° ë¶„í•  ì¤‘...")
    n_total = len(X)
    n_train = int(n_total * 0.7)
    n_val = int(n_total * 0.1)
    
    X_train = X.iloc[:n_train]
    X_val = X.iloc[n_train:n_train+n_val]
    X_test = X.iloc[n_train+n_val:]
    
    y_train = y.iloc[:n_train]
    y_val = y.iloc[n_train:n_train+n_val]
    y_test = y.iloc[n_train+n_val:]
    
    print(f"  í•™ìŠµìš©: {len(X_train):,}ê°œ")
    print(f"  ê²€ì¦ìš©: {len(X_val):,}ê°œ")
    print(f"  í…ŒìŠ¤íŠ¸ìš©: {len(X_test):,}ê°œ")
    
    # Quality Score ë¶„í¬ í™•ì¸
    print(f"\nğŸ“Š Quality Score ë¶„í¬:")
    for name, data in [('Train', y_train), ('Val', y_val), ('Test', y_test)]:
        print(f"  {name}: í‰ê·  {data.mean():.4f}, í‘œì¤€í¸ì°¨ {data.std():.4f}")
    
    # 5. íŠ¹ì§• ìŠ¤ì¼€ì¼ë§
    print("\nğŸ”§ íŠ¹ì§• ìŠ¤ì¼€ì¼ë§ ì¤‘...")
    feature_scaler = StandardScaler()
    X_train_scaled = feature_scaler.fit_transform(X_train)
    X_val_scaled = feature_scaler.transform(X_val)
    X_test_scaled = feature_scaler.transform(X_test)
    
    # 6. íšŒê·€ ëª¨ë¸ë“¤ ì •ì˜ (ê°„ì†Œí™”ëœ íŒŒë¼ë¯¸í„°)
    models_params = {
        'Random Forest': {
            'model': RandomForestRegressor(random_state=42, n_jobs=-1),
            'params': {
                'n_estimators': [300],
                'max_depth': [15],
                'min_samples_split': [5],
                'min_samples_leaf': [2]
            },
            'use_scaled': False
        },
        'XGBoost': {
            'model': xgb.XGBRegressor(random_state=42),
            'params': {
                'n_estimators': [300],
                'max_depth': [6],
                'learning_rate': [0.1],
                'subsample': [0.9],
                'colsample_bytree': [0.9]
            },
            'use_scaled': False
        },
        'Ridge': {
            'model': Ridge(random_state=42),
            'params': {
                'alpha': [1.0]
            },
            'use_scaled': True
        }
    }
    
    # 7. GridSearchë¡œ ê° ëª¨ë¸ ìµœì í™”
    print("\nğŸ” ëª¨ë¸ í•™ìŠµ ì¤‘...")
    
    best_models = {}
    results = {}
    
    for name, config in models_params.items():
        print(f"\n{'='*50}")
        print(f"ğŸ” {name} í•™ìŠµ ì¤‘...")
        
        # GridSearch
        grid_search = GridSearchCV(
            estimator=config['model'],
            param_grid=config['params'],
            cv=3,
            scoring='neg_mean_squared_error',
            n_jobs=-1,
            verbose=1
        )
        
        # ë°ì´í„° ì„ íƒ
        X_train_use = X_train_scaled if config['use_scaled'] else X_train
        X_val_use = X_val_scaled if config['use_scaled'] else X_val
        X_test_use = X_test_scaled if config['use_scaled'] else X_test
        
        # í•™ìŠµ
        grid_search.fit(X_train_use, y_train)
        best_model = grid_search.best_estimator_
        
        print(f"\nğŸ† {name} ìµœì  íŒŒë¼ë¯¸í„°:")
        for param, value in grid_search.best_params_.items():
            print(f"  {param}: {value}")
        
        # í‰ê°€
        val_metrics, val_pred = evaluate_regression_model(best_model, X_val_use, y_val, f"{name} ê²€ì¦")
        test_metrics, test_pred = evaluate_regression_model(best_model, X_test_use, y_test, f"{name} í…ŒìŠ¤íŠ¸")
        
        results[name] = {
            'cv_score': -grid_search.best_score_,
            'best_params': grid_search.best_params_,
            'val_metrics': val_metrics,
            'test_metrics': test_metrics,
            'model': best_model,
            'use_scaled': config['use_scaled']
        }
        
        # Feature Importance
        if hasattr(best_model, 'feature_importances_'):
            print(f"\nğŸ“Š ìƒìœ„ 10ê°œ ì¤‘ìš” íŠ¹ì„±:")
            feature_importance = pd.DataFrame({
                'feature': X.columns,
                'importance': best_model.feature_importances_
            }).sort_values('importance', ascending=False)
            
            for idx, row in feature_importance.head(10).iterrows():
                print(f"  {row['feature']}: {row['importance']:.4f}")
    
    # 8. ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ íƒ
    best_model_name = max(results.keys(), key=lambda k: results[k]['val_metrics']['r2'])
    best_result = results[best_model_name]
    
    print(f"\n{'='*60}")
    print(f"ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model_name}")
    print(f"   ê²€ì¦ RÂ²: {best_result['val_metrics']['r2']:.4f}")
    print(f"   í…ŒìŠ¤íŠ¸ RÂ²: {best_result['test_metrics']['r2']:.4f}")
    print(f"   í…ŒìŠ¤íŠ¸ ìƒê´€ê³„ìˆ˜: {best_result['test_metrics']['correlation']:.4f}")
    
    # 9. ëª¨ë“  ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
    print(f"\nğŸ“Š ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ (í…ŒìŠ¤íŠ¸ ë°ì´í„°):")
    print(f"{'Model':<15} {'RÂ²':<8} {'RMSE':<8} {'ìƒê´€ê³„ìˆ˜':<8}")
    print("-" * 45)
    for name, result in results.items():
        metrics = result['test_metrics']
        print(f"{name:<15} {metrics['r2']:<8.4f} {metrics['rmse']:<8.4f} {metrics['correlation']:<8.4f}")
    
    # 10. í’ˆì§ˆ ì ìˆ˜ ë¶„í¬ ë¶„ì„
    best_model = best_result['model']
    use_scaled = best_result['use_scaled']
    X_test_final = X_test_scaled if use_scaled else X_test
    
    pred_scores = best_model.predict(X_test_final)
    
    print(f"\nğŸ“ˆ ì˜ˆì¸¡ í’ˆì§ˆ ì ìˆ˜ ë¶„ì„:")
    print(f"  ì‹¤ì œ ì ìˆ˜ ë²”ìœ„: {y_test.min():.4f} ~ {y_test.max():.4f}")
    print(f"  ì˜ˆì¸¡ ì ìˆ˜ ë²”ìœ„: {pred_scores.min():.4f} ~ {pred_scores.max():.4f}")
    
    # ì ìˆ˜ êµ¬ê°„ë³„ ë¶„ì„
    test_df = df.iloc[n_train+n_val:].copy()
    test_df['predicted_score'] = pred_scores
    test_df['score_bin'] = pd.qcut(pred_scores, q=5, labels=['ìµœí•˜', 'í•˜', 'ì¤‘', 'ìƒ', 'ìµœìƒ'])
    
    score_analysis = test_df.groupby('score_bin').agg({
        'return_pct': ['mean', 'std'],
        'quality_score': ['mean'],
        'predicted_score': ['mean']
    })
    
    print("\nğŸ“Š ì˜ˆì¸¡ ì ìˆ˜ë³„ ì‹¤ì œ ì„±ê³¼:")
    print(score_analysis)
    
    # 11. ëª¨ë¸ ì €ì¥
    print(f"\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
    
    # ìµœê³  ëª¨ë¸ ì €ì¥
    joblib.dump(best_model, 'quality_model_v2.pkl')
    joblib.dump(feature_scaler, 'feature_scaler_v2.pkl')
    joblib.dump(scalers, 'label_scalers_v2.pkl')
    
    # íŠ¹ì„± ì •ë³´ ì €ì¥
    feature_info = {
        'feature_names': list(X.columns),
        'model_name': best_model_name,
        'best_params': best_result['best_params'],
        'use_scaled_data': use_scaled,
        'test_r2': best_result['test_metrics']['r2'],
        'test_correlation': best_result['test_metrics']['correlation'],
        'optimized_weights': optimal_weights.tolist()
    }
    
    with open('model_metadata_v2.json', 'w') as f:
        import json
        json.dump(feature_info, f, indent=2)
    
    print(f"âœ… ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤:")
    print(f"  - quality_model_v2.pkl")
    print(f"  - feature_scaler_v2.pkl") 
    print(f"  - label_scalers_v2.pkl")
    print(f"  - model_metadata_v2.json")
    
    return results, best_model, scalers

if __name__ == "__main__":
    # 1. ê°€ì¤‘ì¹˜ ìµœì í™”
    optimal_weights, current_diff, optimal_diff, success = optimize_quality_score_weights()
    
    # 2. ìµœì í™” ê²°ê³¼ ì €ì¥
    optimization_results = {
        'original_weights': [0.4, 0.3, 0.3],
        'optimal_weights': optimal_weights.tolist() if hasattr(optimal_weights, 'tolist') else list(optimal_weights),
        'original_return_diff': float(current_diff),
        'optimal_return_diff': float(optimal_diff),
        'improvement': float(optimal_diff - current_diff),
        'optimization_success': bool(success)
    }
    
    import json
    with open('weight_optimization_results_v2.json', 'w') as f:
        json.dump(optimization_results, f, indent=2)
    
    print(f"\nâœ… ê°€ì¤‘ì¹˜ ìµœì í™” ê²°ê³¼ê°€ weight_optimization_results_v2.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # 3. ìµœì í™”ëœ ê°€ì¤‘ì¹˜ë¡œ ëª¨ë¸ í•™ìŠµ
    results, model, scalers = train_quality_score_model_with_optimized_weights(optimal_weights)