import pandas as pd
import numpy as np
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import RobustScaler
from sklearn.metrics import r2_score, mean_squared_error
import xgboost as xgb
from itertools import product, combinations
import json
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

class MegaLabelingOptimizer:
    """
    ì´ˆëŒ€ê·œëª¨ ë¼ë²¨ë§ ë°©ì‹ í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ
    
    ëª©í‘œ: ìˆ˜ë°± ê°€ì§€ ë¼ë²¨ë§ ê³µì‹ê³¼ ìˆ˜ì²œ ê°€ì§€ ê°€ì¤‘ì¹˜ ì¡°í•© í…ŒìŠ¤íŠ¸
    """
    
    def __init__(self, data_path=None):
        self.data_path = data_path or '../results/final/trading_episodes_with_rebuilt_market_component.csv'
        self.df = None
        self.test_results = []
        
    def load_data(self, sample_size=None):
        """ë°ì´í„° ë¡œë“œ"""
        print("ðŸ“Š ë°ì´í„° ë¡œë“œ ì¤‘...")
        self.df = pd.read_csv(self.data_path)
        
        if sample_size and len(self.df) > sample_size:
            self.df = self.df.sample(sample_size, random_state=42)
        
        self.df = self.df.fillna(0)
        print(f"  ì´ ë°ì´í„°: {len(self.df):,}ê°œ")
        return self.df

    def create_mega_labeling_variants(self):
        """ìˆ˜ë°± ê°€ì§€ ë¼ë²¨ë§ ë°©ì‹ë“¤ ìƒì„±"""
        
        df = self.df
        
        # ê¸°ë³¸ ì„±ë¶„ë“¤
        components = {
            # ìˆ˜ìµë¥  ë³€í˜•ë“¤
            'return_raw': df['return_pct'],
            'return_log': np.log(1 + np.abs(df['return_pct'])/100) * np.sign(df['return_pct']),
            'return_sqrt': np.sqrt(np.abs(df['return_pct'])) * np.sign(df['return_pct']),
            'return_squared': df['return_pct'] ** 2 * np.sign(df['return_pct']),
            'return_cubed': df['return_pct'] ** 3,
            
            # ë¦¬ìŠ¤í¬ ì¡°ì •ë“¤
            'risk_adj_vol': df['return_pct'] / np.maximum(df['entry_volatility_20d'], 0.01),
            'risk_adj_vol2': df['return_pct'] / np.maximum(df['entry_volatility_20d']**2, 0.01),
            'risk_adj_vol_sqrt': df['return_pct'] / np.maximum(np.sqrt(df['entry_volatility_20d']), 0.01),
            'risk_adj_exit_vol': df['return_pct'] / np.maximum(df['exit_volatility_20d'], 0.01),
            'risk_adj_avg_vol': df['return_pct'] / np.maximum((df['entry_volatility_20d'] + df['exit_volatility_20d'])/2, 0.01),
            
            # ì‹œê°„ íš¨ìœ¨ì„±ë“¤
            'time_eff_linear': df['return_pct'] / np.maximum(df['holding_period_days'], 1),
            'time_eff_log': df['return_pct'] / np.maximum(np.log(df['holding_period_days'] + 1), 0.01),
            'time_eff_sqrt': df['return_pct'] / np.maximum(np.sqrt(df['holding_period_days']), 0.01),
            'time_eff_squared': df['return_pct'] / np.maximum(df['holding_period_days']**2, 0.01),
            'time_decay': df['return_pct'] * np.exp(-df['holding_period_days']/30),
            
            # ì‹œìž¥ ìƒëŒ€ì„±ë“¤
            'market_rel_simple': df['return_pct'] - df['market_return_during_holding'],
            'market_rel_ratio': df['return_pct'] / np.maximum(np.abs(df['market_return_during_holding']), 0.01),
            'market_beta_adj': df['return_pct'] - df['market_return_during_holding'] * 1.2,
            'market_outperf': np.maximum(df['return_pct'] - df['market_return_during_holding'], 0),
            'market_correlation': df['return_pct'] * (1 - np.abs(df['market_return_during_holding'])/10),
            
            # í¬ì§€ì…˜ ë° ê°€ê²© ìœ„ì¹˜ë“¤
            'position_adj': df['return_pct'] * np.log(df['position_size_pct'] + 1),
            'price_pos_adj': df['return_pct'] * (100 - df['entry_ratio_52w_high']) / 100,
            'price_momentum_adj': df['return_pct'] * (1 + df['entry_momentum_20d']/100),
            'vix_opportunity': df['return_pct'] * (1 + np.clip(df['entry_vix'] - 20, 0, 30)/100),
            'vol_regime': df['return_pct'] * np.where(df['entry_volatility_20d'] > 30, 0.8, 1.2),
            
            # ê³ ê¸‰ ì§€í‘œë“¤
            'sharpe_like': df['return_pct'] / np.maximum(df['entry_volatility_20d'], 0.01),
            'sortino_like': df['return_pct'] / np.maximum(np.where(df['return_pct'] < 0, np.abs(df['return_pct']), 1), 0.01),
            'calmar_like': df['return_pct'] / np.maximum(np.abs(np.minimum(df['return_pct'], 0)), 0.01),
            'sterling_like': (df['return_pct'] + 10) / np.maximum(np.abs(np.minimum(df['return_pct'], 0)), 0.01),
            'martin_like': df['return_pct'] / np.maximum(np.sqrt(np.abs(np.minimum(df['return_pct'], 0))), 0.01),
            
            # ë³€ë™ì„± ê¸°ë°˜ë“¤
            'vol_scaled_return': df['return_pct'] * (20 / np.maximum(df['entry_volatility_20d'], 1)),
            'vol_momentum': df['return_pct'] * (df['entry_volatility_5d'] / np.maximum(df['entry_volatility_20d'], 0.01)),
            'vol_change_adj': df['return_pct'] * (1 - np.abs(df['change_volatility_5d'])/10),
            'vol_efficiency': df['return_pct'] / (df['entry_volatility_20d'] * df['holding_period_days']),
            
            # ëª¨ë©˜í…€ ê¸°ë°˜ë“¤  
            'momentum_weighted': df['return_pct'] * (1 + df['entry_momentum_20d']/50),
            'momentum_contrarian': df['return_pct'] * (1 - df['entry_momentum_20d']/50),
            'momentum_change': df['return_pct'] * (1 + df['change_momentum_20d']/100),
            'momentum_strength': df['return_pct'] * np.abs(df['entry_momentum_20d'])/10,
            
            # ê¸°ìˆ ì  ì§€í‘œë“¤
            'ma_position': df['return_pct'] * (1 + df['entry_ma_dev_20d']/10),
            'ma_trend_follow': df['return_pct'] * np.where(df['entry_ma_dev_20d'] > 0, 1.1, 0.9),
            'ma_mean_revert': df['return_pct'] * np.where(df['entry_ma_dev_20d'] < -5, 1.2, 0.8),
            'ma_volatility': df['return_pct'] / np.maximum(np.abs(df['entry_ma_dev_20d']), 0.01),
            
            # ì‹œìž¥ í™˜ê²½ ì¡°ì •ë“¤
            'vix_adjusted': df['return_pct'] * (30 / np.maximum(df['entry_vix'], 1)),
            'vix_contrarian': df['return_pct'] * (df['entry_vix'] / 20),
            'yield_adjusted': df['return_pct'] * (1 + df['entry_tnx_yield']/10),
            'macro_score': df['return_pct'] * (1 + (df['entry_vix'] - df['exit_vix'])/10),
            
            # ì†ì‹¤ íŽ˜ë„í‹°ë“¤
            'downside_penalty_light': np.where(df['return_pct'] < 0, df['return_pct'] * 1.2, df['return_pct']),
            'downside_penalty_medium': np.where(df['return_pct'] < 0, df['return_pct'] * 1.5, df['return_pct']),
            'downside_penalty_heavy': np.where(df['return_pct'] < 0, df['return_pct'] * 2.0, df['return_pct']),
            'asymmetric_utility': np.where(df['return_pct'] < 0, df['return_pct'] * 2.5, df['return_pct'] * 0.8),
            
            # ìˆ˜ìµ ê°€ì†ë“¤
            'upside_boost_light': np.where(df['return_pct'] > 0, df['return_pct'] * 1.1, df['return_pct']),
            'upside_boost_medium': np.where(df['return_pct'] > 0, df['return_pct'] * 1.3, df['return_pct']),
            'upside_boost_heavy': np.where(df['return_pct'] > 0, df['return_pct'] * 1.5, df['return_pct']),
            
            # ë³µí•© ë¦¬ìŠ¤í¬ ì§€í‘œë“¤
            'comprehensive_risk': df['return_pct'] / (df['entry_volatility_20d'] * np.sqrt(df['holding_period_days']) * (df['entry_ratio_52w_high']/100 + 0.1)),
            'multi_factor_alpha': (df['return_pct'] - df['market_return_during_holding']) / np.maximum(df['entry_volatility_20d'], 1),
            'risk_parity_score': df['return_pct'] / np.maximum(df['entry_volatility_20d'] * df['position_size_pct']/100, 0.01),
            'kelly_approx': df['return_pct'] * (df['return_pct'] / np.maximum(df['entry_volatility_20d']**2, 0.01)),
            
            # í–‰ë™ìž¬ë¬´í•™ ê¸°ë°˜ë“¤
            'overconfidence_adj': df['return_pct'] * np.where(df['position_size_pct'] > 5, 0.9, 1.1),
            'disposition_effect': df['return_pct'] * np.where(df['holding_period_days'] < 5, 0.8, 1.0),
            'anchoring_bias': df['return_pct'] * (1 - df['entry_ratio_52w_high']/200),
            'herding_penalty': df['return_pct'] * np.where(df['entry_vix'] < 15, 0.9, 1.0),
            
            # ì •ë³´ ë¹„ìœ¨ë“¤
            'info_ratio_simple': (df['return_pct'] - df['market_return_during_holding']) / np.maximum(np.std(df['return_pct'] - df['market_return_during_holding']), 0.01),
            'tracking_error_adj': df['return_pct'] / np.maximum(np.abs(df['return_pct'] - df['market_return_during_holding']), 0.01),
            'active_return': np.maximum(df['return_pct'] - df['market_return_during_holding'], 0) * 2,
        }
        
        # ìˆ˜ë°± ê°€ì§€ ë³µí•© ì§€í‘œ ìƒì„±
        labeling_methods = {}
        
        # 1. ë‹¨ì¼ ì„±ë¶„ë“¤
        for name, formula in components.items():
            labeling_methods[f"single_{name}"] = {
                'formula': lambda df, f=formula: f,
                'description': f'ë‹¨ì¼ ì„±ë¶„: {name}',
                'components': [name],
                'weights': [1.0]
            }
        
        # 2. 2ê°œ ì„±ë¶„ ì¡°í•©ë“¤ (í•µì‹¬ ì¡°í•©ë“¤ë§Œ)
        key_components = [
            'return_raw', 'risk_adj_vol', 'time_eff_linear', 'market_rel_simple', 
            'price_pos_adj', 'sharpe_like', 'downside_penalty_medium', 'momentum_weighted'
        ]
        
        weight_combos_2 = [
            [0.1, 0.9], [0.2, 0.8], [0.3, 0.7], [0.4, 0.6], [0.5, 0.5],
            [0.6, 0.4], [0.7, 0.3], [0.8, 0.2], [0.9, 0.1]
        ]
        
        for i, comp1 in enumerate(key_components):
            for j, comp2 in enumerate(key_components[i+1:], i+1):
                for weights in weight_combos_2:
                    name = f"combo2_{comp1[:8]}_{comp2[:8]}_{weights[0]:.1f}_{weights[1]:.1f}"
                    labeling_methods[name] = {
                        'formula': lambda df, c1=comp1, c2=comp2, w=weights: components[c1] * w[0] + components[c2] * w[1],
                        'description': f'2ì„±ë¶„: {comp1}({weights[0]}) + {comp2}({weights[1]})',
                        'components': [comp1, comp2],
                        'weights': weights
                    }
        
        # 3. 3ê°œ ì„±ë¶„ ì¡°í•©ë“¤
        weight_combos_3 = [
            [0.33, 0.33, 0.34], [0.5, 0.3, 0.2], [0.6, 0.25, 0.15], [0.2, 0.3, 0.5],
            [0.4, 0.4, 0.2], [0.7, 0.2, 0.1], [0.1, 0.2, 0.7], [0.45, 0.35, 0.2]
        ]
        
        triple_combos = [
            ('return_raw', 'risk_adj_vol', 'time_eff_linear'),
            ('return_raw', 'market_rel_simple', 'price_pos_adj'),
            ('sharpe_like', 'time_eff_linear', 'momentum_weighted'),
            ('risk_adj_vol', 'downside_penalty_medium', 'vix_opportunity'),
            ('return_raw', 'vol_scaled_return', 'ma_position'),
            ('market_rel_simple', 'momentum_weighted', 'time_eff_log'),
            ('comprehensive_risk', 'multi_factor_alpha', 'kelly_approx'),
            ('sortino_like', 'calmar_like', 'sterling_like')
        ]
        
        for comp_triple in triple_combos:
            for weights in weight_combos_3:
                name = f"combo3_{'_'.join([c[:6] for c in comp_triple])}_{'-'.join([f'{w:.2f}' for w in weights])}"
                labeling_methods[name] = {
                    'formula': lambda df, comps=comp_triple, w=weights: sum(components[c] * w[i] for i, c in enumerate(comps)),
                    'description': f'3ì„±ë¶„: {" + ".join([f"{c}({w:.2f})" for c, w in zip(comp_triple, weights)])}',
                    'components': list(comp_triple),
                    'weights': weights
                }
        
        # 4. 4ê°œ ì„±ë¶„ ì¡°í•©ë“¤ (ì‚¬ìš©ìž ìš”ì²­!)
        weight_combos_4 = [
            [0.25, 0.25, 0.25, 0.25],  # ê· ë“±
            [0.4, 0.3, 0.2, 0.1],      # í”¼ë¼ë¯¸ë“œ
            [0.1, 0.2, 0.3, 0.4],      # ì—­í”¼ë¼ë¯¸ë“œ  
            [0.35, 0.35, 0.15, 0.15],  # 2+2
            [0.6, 0.2, 0.1, 0.1],      # ê·¹ë‹¨ì§‘ì¤‘
            [0.5, 0.3, 0.15, 0.05],    # ì²´ê°
            [0.2, 0.2, 0.3, 0.3],      # í›„ë°˜ì§‘ì¤‘
            [0.3, 0.25, 0.25, 0.2],    # ì•½ê°„ë¶ˆê· ë“±
        ]
        
        quad_combos = [
            ('return_raw', 'risk_adj_vol', 'time_eff_linear', 'market_rel_simple'),
            ('sharpe_like', 'sortino_like', 'calmar_like', 'sterling_like'),
            ('return_raw', 'downside_penalty_medium', 'upside_boost_medium', 'time_eff_linear'),
            ('risk_adj_vol', 'momentum_weighted', 'vix_opportunity', 'price_pos_adj'),
            ('comprehensive_risk', 'multi_factor_alpha', 'kelly_approx', 'info_ratio_simple'),
            ('vol_scaled_return', 'time_decay', 'market_correlation', 'ma_position'),
            ('return_log', 'risk_adj_vol2', 'time_eff_log', 'momentum_contrarian'),
            ('asymmetric_utility', 'vol_regime', 'macro_score', 'anchoring_bias')
        ]
        
        for comp_quad in quad_combos:
            for weights in weight_combos_4:
                name = f"combo4_{'_'.join([c[:5] for c in comp_quad])}_{'-'.join([f'{w:.2f}' for w in weights])}"
                labeling_methods[name] = {
                    'formula': lambda df, comps=comp_quad, w=weights: sum(components[c] * w[i] for i, c in enumerate(comps)),
                    'description': f'4ì„±ë¶„: {" + ".join([f"{c}({w:.2f})" for c, w in zip(comp_quad, weights)])}',
                    'components': list(comp_quad),
                    'weights': weights
                }
        
        # 5. 5ê°œ+ ì„±ë¶„ ì¡°í•©ë“¤
        weight_combos_5 = [
            [0.2, 0.2, 0.2, 0.2, 0.2],    # ê· ë“±
            [0.3, 0.25, 0.2, 0.15, 0.1],   # ì²´ê°
            [0.4, 0.25, 0.15, 0.1, 0.1],   # ì§‘ì¤‘
            [0.15, 0.15, 0.25, 0.25, 0.2], # ì¤‘í›„ë°˜ì§‘ì¤‘
        ]
        
        penta_combos = [
            ('return_raw', 'risk_adj_vol', 'time_eff_linear', 'market_rel_simple', 'momentum_weighted'),
            ('sharpe_like', 'sortino_like', 'calmar_like', 'sterling_like', 'martin_like'),
            ('comprehensive_risk', 'multi_factor_alpha', 'kelly_approx', 'info_ratio_simple', 'tracking_error_adj'),
            ('downside_penalty_medium', 'upside_boost_medium', 'vol_regime', 'vix_contrarian', 'yield_adjusted'),
        ]
        
        for comp_penta in penta_combos:
            for weights in weight_combos_5:
                name = f"combo5_{'_'.join([c[:4] for c in comp_penta])}_{'-'.join([f'{w:.2f}' for w in weights])}"
                labeling_methods[name] = {
                    'formula': lambda df, comps=comp_penta, w=weights: sum(components[c] * w[i] for i, c in enumerate(comps)),
                    'description': f'5ì„±ë¶„: {" + ".join([f"{c}({w:.2f})" for c, w in zip(comp_penta, weights)])}',
                    'components': list(comp_penta),
                    'weights': weights
                }
        
        # 6. ì´ˆë³µìž¡ ìˆ˜ì‹ë“¤ (ì°½ì˜ì  ì¡°í•©)
        creative_formulas = {
            'ultimate_risk_adj': lambda df: (df['return_pct'] / np.maximum(df['entry_volatility_20d'], 0.01)) * (1 + df['entry_vix']/50) * np.exp(-df['holding_period_days']/30),
            'momentum_risk_time': lambda df: (df['return_pct'] * (1 + df['entry_momentum_20d']/100)) / (np.sqrt(df['entry_volatility_20d']) * np.log(df['holding_period_days'] + 1)),
            'market_adaptive': lambda df: df['return_pct'] * np.where(df['entry_vix'] > 25, 1 + (df['entry_vix']-25)/100, 1 - (25-df['entry_vix'])/200),
            'behavioral_composite': lambda df: df['return_pct'] * (1 - df['entry_ratio_52w_high']/300) * np.where(df['holding_period_days'] > 10, 1.1, 0.9),
            'volatility_regime_adaptive': lambda df: df['return_pct'] / np.where(df['entry_volatility_20d'] > 30, df['entry_volatility_20d'], np.sqrt(df['entry_volatility_20d'])),
            'macro_momentum_fusion': lambda df: (df['return_pct'] - df['market_return_during_holding']) * (1 + df['change_vix']/20) * (1 + df['entry_momentum_20d']/50),
            'asymmetric_risk_reward': lambda df: np.where(df['return_pct'] > 0, df['return_pct'] / np.sqrt(df['entry_volatility_20d']), df['return_pct'] / df['entry_volatility_20d']),
            'time_weighted_alpha': lambda df: (df['return_pct'] - df['market_return_during_holding']) / np.sqrt(df['holding_period_days']) / np.maximum(df['entry_volatility_20d'], 1),
            'contrarian_momentum_blend': lambda df: df['return_pct'] * (0.7 * (1 - df['entry_momentum_20d']/100) + 0.3 * (1 + df['entry_momentum_20d']/100)),
            'kelly_sortino_hybrid': lambda df: (df['return_pct'] / np.maximum(np.where(df['return_pct'] < 0, np.abs(df['return_pct']), 1), 0.01)) * (df['return_pct'] / np.maximum(df['entry_volatility_20d']**2, 0.01)),
        }
        
        for name, formula in creative_formulas.items():
            labeling_methods[f"creative_{name}"] = {
                'formula': formula,
                'description': f'ì°½ì˜ì  ê³µì‹: {name}',
                'components': ['complex_formula'],
                'weights': [1.0]
            }
        
        print(f"ðŸŽ¯ ìƒì„±ëœ ë¼ë²¨ë§ ë°©ì‹: {len(labeling_methods):,}ê°œ")
        return labeling_methods
    
    def evaluate_labeling_method(self, y_labels, method_name, n_folds=3):
        """ë¼ë²¨ë§ ë°©ì‹ í‰ê°€"""
        
        # ì§„ìž… ì‹œì  í”¼ì²˜ë“¤ë§Œ ì‚¬ìš©
        feature_cols = [
            'entry_momentum_5d', 'entry_momentum_20d', 'entry_momentum_60d',
            'entry_ma_dev_5d', 'entry_ma_dev_20d', 'entry_ma_dev_60d',
            'entry_volatility_5d', 'entry_volatility_60d', 
            'entry_vix', 'entry_tnx_yield', 'position_size_pct',
            'entry_vol_change_5d', 'entry_vol_change_20d'
        ]
        
        available_features = [col for col in feature_cols if col in self.df.columns]
        
        if len(available_features) < 5:
            return {'error': 'Insufficient features'}
        
        X = self.df[available_features].fillna(0)
        y = y_labels
        
        # NaN/ë¬´í•œê°’ ì œê±°
        mask = ~(np.isnan(y) | np.isinf(y) | (np.abs(y) > 1e6))
        X = X[mask]
        y = y[mask]
        
        if len(X) < 200:
            return {'error': 'Insufficient data'}
        
        try:
            # ë¹ ë¥¸ ëª¨ë¸ë¡œ í‰ê°€ (ì†ë„ ìµœì í™”)
            model = RandomForestRegressor(
                n_estimators=50,
                max_depth=8,
                random_state=42,
                n_jobs=1  # ë³‘ë ¬ì²˜ë¦¬ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
            )
            
            scores = cross_val_score(model, X, y, cv=n_folds, scoring='r2')
            
            return {
                'mean_r2': scores.mean(),
                'std_r2': scores.std(),
                'min_r2': scores.min(),
                'max_r2': scores.max(),
                'method_name': method_name,
                'n_samples': len(X),
                'n_features': len(available_features),
                'y_mean': float(np.mean(y)),
                'y_std': float(np.std(y)),
                'y_range': float(np.max(y) - np.min(y))
            }
            
        except Exception as e:
            return {'error': str(e)}
    
    def run_mega_optimization(self, sample_size=15000, max_methods=None):
        """ë©”ê°€ ìµœì í™” ì‹¤í–‰"""
        
        print("ðŸš€ ë©”ê°€ ë¼ë²¨ë§ ìµœì í™” ì‹œìž‘")
        print("=" * 60)
        
        # ë°ì´í„° ë¡œë“œ
        self.load_data(sample_size)
        
        # ëª¨ë“  ë¼ë²¨ë§ ë°©ì‹ ìƒì„±
        print("\nðŸ”§ ë¼ë²¨ë§ ë°©ì‹ ìƒì„± ì¤‘...")
        labeling_methods = self.create_mega_labeling_variants()
        
        if max_methods:
            # ë¬´ìž‘ìœ„ë¡œ ì¼ë¶€ë§Œ ì„ íƒ (í…ŒìŠ¤íŠ¸ìš©)
            import random
            method_names = random.sample(list(labeling_methods.keys()), min(max_methods, len(labeling_methods)))
            labeling_methods = {k: labeling_methods[k] for k in method_names}
        
        print(f"í…ŒìŠ¤íŠ¸í•  ë°©ì‹: {len(labeling_methods):,}ê°œ")
        
        # ë°°ì¹˜ë³„ë¡œ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ê´€ë¦¬)
        batch_size = 100
        all_results = []
        method_names = list(labeling_methods.keys())
        
        for i in range(0, len(method_names), batch_size):
            batch_names = method_names[i:i+batch_size]
            batch_results = []
            
            print(f"\nðŸ“Š ë°°ì¹˜ {i//batch_size + 1}/{(len(method_names)-1)//batch_size + 1} ì²˜ë¦¬ ì¤‘... ({len(batch_names)}ê°œ ë°©ì‹)")
            
            for j, method_name in enumerate(batch_names):
                if j % 20 == 0:
                    print(f"  ì§„í–‰: {j+1}/{len(batch_names)}")
                
                try:
                    method_info = labeling_methods[method_name]
                    y_labels = method_info['formula'](self.df)
                    
                    result = self.evaluate_labeling_method(y_labels, method_name)
                    
                    if 'error' not in result:
                        result.update({
                            'description': method_info['description'],
                            'components': method_info.get('components', []),
                            'weights': method_info.get('weights', [])
                        })
                        batch_results.append(result)
                    
                except Exception as e:
                    continue
            
            all_results.extend(batch_results)
            print(f"  ë°°ì¹˜ ì™„ë£Œ: {len(batch_results)}ê°œ ì„±ê³µ")
        
        # ê²°ê³¼ ì •ë ¬
        self.test_results = sorted(all_results, key=lambda x: x.get('mean_r2', -999), reverse=True)
        
        # ìƒìœ„ ê²°ê³¼ ì¶œë ¥
        print(f"\nðŸ† ìµœê³  ì„±ëŠ¥ ë¼ë²¨ë§ ë°©ì‹ë“¤ (ìƒìœ„ 20ê°œ)")
        print("=" * 100)
        
        for i, result in enumerate(self.test_results[:20]):
            components_str = ' + '.join([f"{c}({w:.2f})" for c, w in zip(result.get('components', []), result.get('weights', []))])
            if len(components_str) > 60:
                components_str = components_str[:57] + "..."
                
            print(f"{i+1:2d}. RÂ²={result['mean_r2']:.4f}Â±{result['std_r2']:.4f} | {result['method_name'][:25]:25s} | {components_str}")
            
            if i < 5:  # ìƒìœ„ 5ê°œëŠ” ìƒì„¸ ì •ë³´
                print(f"    ðŸ“Š ìƒ˜í”Œ: {result['n_samples']:,}, í”¼ì²˜: {result['n_features']}, Yë²”ìœ„: {result['y_range']:.2f}")
        
        print(f"\nâœ… ì´ í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {len(self.test_results):,}ê°œ ë°©ì‹")
        return self.test_results
    
    def save_mega_results(self, filename=None, top_n=100):
        """ë©”ê°€ ê²°ê³¼ ì €ìž¥"""
        if not filename:
            filename = f"mega_labeling_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        # ìƒìœ„ Nê°œë§Œ ì €ìž¥ (íŒŒì¼ í¬ê¸° ê´€ë¦¬)
        top_results = self.test_results[:top_n]
        
        serializable_results = []
        for result in top_results:
            clean_result = {}
            for key, value in result.items():
                if isinstance(value, (list, str, int, float, bool)):
                    clean_result[key] = value
                elif isinstance(value, np.ndarray):
                    clean_result[key] = value.tolist()
                else:
                    clean_result[key] = str(value)
            serializable_results.append(clean_result)
        
        with open(filename, 'w') as f:
            json.dump({
                'timestamp': datetime.now().isoformat(),
                'total_tested': len(self.test_results),
                'saved_top_n': len(serializable_results),
                'best_r2': self.test_results[0]['mean_r2'] if self.test_results else 0,
                'results': serializable_results
            }, f, indent=2, ensure_ascii=False)
        
        print(f"ðŸ’¾ ë©”ê°€ ê²°ê³¼ ì €ìž¥: {filename} (ìƒìœ„ {top_n}ê°œ)")
        return filename

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("ðŸš€ ë©”ê°€ ë¼ë²¨ë§ ìµœì í™” ì‹œìŠ¤í…œ")
    print("=" * 50)
    
    optimizer = MegaLabelingOptimizer()
    
    # ë©”ê°€ ìµœì í™” ì‹¤í–‰
    results = optimizer.run_mega_optimization(
        sample_size=20000,  # ì ë‹¹í•œ ìƒ˜í”Œ í¬ê¸°
        max_methods=1000    # í…ŒìŠ¤íŠ¸í•  ìµœëŒ€ ë°©ì‹ ìˆ˜ (None = ì „ì²´)
    )
    
    # ê²°ê³¼ ì €ìž¥
    optimizer.save_mega_results(top_n=100)
    
    print("\nâœ… ë©”ê°€ ë¼ë²¨ë§ ìµœì í™” ì™„ë£Œ!")

if __name__ == "__main__":
    main()