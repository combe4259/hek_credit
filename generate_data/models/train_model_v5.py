import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV, RandomizedSearchCV
from sklearn.model_selection import ParameterGrid
from scipy.stats import uniform, randint
from tqdm import tqdm
from sklearn.preprocessing import RobustScaler
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import xgboost as xgb
import joblib
import json
from datetime import datetime
import warnings
import torch
warnings.filterwarnings('ignore')

class ImprovedHybridTradingAI:
    """
    ê°œì„ ëœ í•˜ì´ë¸Œë¦¬ë“œ íŠ¸ë ˆì´ë”© AI v5
    
    êµ¬ì¡°:
    1. Aìœ í˜•: ì™„ë£Œëœ ê±°ë˜ì˜ ì •í™•í•œ í’ˆì§ˆ í‰ê°€ (ì‚¬í›„ ë¶„ì„) - v4ì—ì„œ ë³µì‚¬
    2. Bìœ í˜•: ì‹¤ì œ ìˆ˜ìµë¥  ê¸°ë°˜ ì§„ì… ì¡°ê±´ ì ìˆ˜ (0-100ì ) - ì™„ì „íˆ ìƒˆë¡œ ì„¤ê³„
    
    ê°œì„ ì :
    - Bìœ í˜•: ê·œì¹™ ê¸°ë°˜ â†’ ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ì ìˆ˜í™”
    - Bìœ í˜•: ì •êµí•œ ê¸°ìˆ ì  ì§€í‘œ ì¶”ê°€
    - Data Leakage ì™„ì „ ì œê±° ìœ ì§€
    """
    
    def __init__(self, train_months=36, val_months=6, test_months=6, step_months=3, use_global_split=True):
        self.train_months = train_months
        self.val_months = val_months
        self.test_months = test_months
        self.step_months = step_months
        self.use_global_split = use_global_split
        
        # ===== Aìœ í˜•: ê±°ë˜ í’ˆì§ˆ ë¶„ì„ê¸° (v4ì™€ ë™ì¼) =====
        self.a_type_quality_model = None      
        self.a_type_quality_scalers = {}      
        self.a_type_quality_features = None   
        
        # ===== Bìœ í˜•: ê°œì„ ëœ ì§„ì… ì¡°ê±´ í‰ê°€ê¸° (ìƒˆë¡œ ì„¤ê³„) =====  
        self.b_type_entry_model = None        
        self.b_type_entry_scalers = {}        
        self.b_type_entry_features = None     
        
        self.fold_results = []
        self.global_results = None
        self.best_params_a = None
        self.best_params_b = None
    
    # ================================
    # Aìœ í˜•: ì‚¬í›„ í’ˆì§ˆ í‰ê°€ (v4ì—ì„œ ê·¸ëŒ€ë¡œ ë³µì‚¬)
    # ================================
    
    def create_a_type_quality_score(self, df, risk_scaler=None, eff_scaler=None, verbose=False):
        """Aìœ í˜•: ì™„ë£Œëœ ê±°ë˜ì˜ í’ˆì§ˆ ì ìˆ˜ ìƒì„± (ëª¨ë“  ì •ë³´ í™œìš© ê°€ëŠ¥)"""
        if verbose:
            print("ğŸ¯ Aìœ í˜•: Quality Score ìƒì„± ì¤‘...")
        
        df = df.copy()
        
        # NaN ì²˜ë¦¬ (0ìœ¼ë¡œ ëŒ€ì²´)
        df['return_pct'] = df['return_pct'].fillna(0)
        df['entry_volatility_20d'] = df['entry_volatility_20d'].fillna(0)
        df['entry_ratio_52w_high'] = df['entry_ratio_52w_high'].fillna(0)
        df['holding_period_days'] = df['holding_period_days'].fillna(0)
        
        # Risk Management Quality (40%) - ë¦¬ìŠ¤í¬ ëŒ€ë¹„ ì„±ê³¼
        volatility_safe = np.maximum(df['entry_volatility_20d'], 0.01)
        df['risk_adj_return'] = df['return_pct'] / volatility_safe
        df['risk_adj_return'] = np.where(
            np.isinf(df['risk_adj_return']) | np.isnan(df['risk_adj_return']), 
            0, df['risk_adj_return']
        )
        
        ratio_safe = np.clip(df['entry_ratio_52w_high'], 0, 100)
        df['price_safety'] = (100 - ratio_safe) / 100
        
        df['risk_management_score'] = df['risk_adj_return'] * 0.6 + df['price_safety'] * 0.4
        
        # Efficiency Quality (60%) - ì‹œê°„ ëŒ€ë¹„ íš¨ìœ¨ì„±
        holding_safe = np.maximum(df['holding_period_days'], 1)
        df['time_efficiency'] = df['return_pct'] / holding_safe
        df['time_efficiency'] = np.where(
            np.isinf(df['time_efficiency']) | np.isnan(df['time_efficiency']), 
            0, df['time_efficiency']
        )
        
        df['efficiency_score'] = df['time_efficiency']
        
        # ìŠ¤ì¼€ì¼ë§ ë° ì¢…í•© ì ìˆ˜
        if risk_scaler is None or eff_scaler is None:
            risk_scaler = RobustScaler()
            eff_scaler = RobustScaler()
            
            risk_scaled = risk_scaler.fit_transform(df[['risk_management_score']])
            eff_scaled = eff_scaler.fit_transform(df[['efficiency_score']])
            
            self.a_type_quality_scalers['risk_scaler'] = risk_scaler
            self.a_type_quality_scalers['efficiency_scaler'] = eff_scaler
        else:
            risk_scaled = risk_scaler.transform(df[['risk_management_score']])
            eff_scaled = eff_scaler.transform(df[['efficiency_score']])
        
        df['a_type_quality_score'] = risk_scaled.flatten() * 0.4 + eff_scaled.flatten() * 0.6
        
        if verbose:
            print(f"  âœ… Quality Score ìƒì„± ì™„ë£Œ")
            print(f"  ë²”ìœ„: {df['a_type_quality_score'].min():.4f} ~ {df['a_type_quality_score'].max():.4f}")
            print(f"  í‰ê· : {df['a_type_quality_score'].mean():.4f}")
        
        return df
    
    def prepare_a_type_features(self, df, verbose=False):
        """Aìœ í˜•: í’ˆì§ˆ í‰ê°€ìš© í”¼ì²˜ ì¤€ë¹„ (ì™„ë£Œëœ ê±°ë˜ì˜ ëª¨ë“  ì •ë³´ ì‚¬ìš© ê°€ëŠ¥)"""
        if verbose:
            print("ğŸ”§ Aìœ í˜•: í’ˆì§ˆ í‰ê°€ìš© í”¼ì²˜ ì¤€ë¹„ ì¤‘...")
        
        # Quality Score ê³„ì‚°ìš© í”¼ì²˜ëŠ” ì œì™¸
        excluded_features = {
            'return_pct', 'entry_volatility_20d', 'entry_ratio_52w_high', 'holding_period_days',
            'risk_adj_return', 'price_safety', 'risk_management_score',
            'time_efficiency', 'efficiency_score', 'quality_score', 'a_type_quality_score'
        }
        
        # Aìœ í˜•ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  í”¼ì²˜ ì¹´í…Œê³ ë¦¬
        available_a_type_features = []
        
        # ===== 1. ê¸°ë³¸ ê±°ë˜ ì •ë³´ =====
        basic_trade_info = ['position_size_pct']  # ê±°ë˜ ê·œëª¨
        available_a_type_features.extend([col for col in basic_trade_info if col in df.columns])
        
        # ===== 2. ì§„ì… ì‹œì  ê¸°ìˆ ì  ì§€í‘œ =====
        entry_technical_indicators = [
            # ëª¨ë©˜í…€ ì§€í‘œ
            'entry_momentum_5d', 'entry_momentum_60d', 
            # ì´ë™í‰ê·  ê´´ë¦¬ë„
            'entry_ma_dev_5d', 'entry_ma_dev_20d', 'entry_ma_dev_60d',
            # ë³€ë™ì„± (entry_volatility_20d ì œì™¸ - quality_scoreì— ì‚¬ìš©ë¨)
            'entry_volatility_5d', 'entry_volatility_60d',
            # ë³€ë™ì„± ë³€í™”ìœ¨
            'entry_vol_change_5d', 'entry_vol_change_20d', 'entry_vol_change_60d',
            # ì‹œì¥ í™˜ê²½
            'entry_vix', 'entry_tnx_yield'
        ]
        available_a_type_features.extend([col for col in entry_technical_indicators if col in df.columns])
        
        # ===== 3. ì¢…ë£Œ ì‹œì  ì§€í‘œ (Aìœ í˜•ë§Œ ì‚¬ìš© ê°€ëŠ¥!) =====
        exit_technical_indicators = [
            # ì¢…ë£Œ ì‹œì  ëª¨ë©˜í…€
            'exit_momentum_5d', 'exit_momentum_20d', 'exit_momentum_60d',
            # ì¢…ë£Œ ì‹œì  ì´ë™í‰ê·  ê´´ë¦¬ë„
            'exit_ma_dev_5d', 'exit_ma_dev_20d', 'exit_ma_dev_60d',
            # ì¢…ë£Œ ì‹œì  ë³€ë™ì„±
            'exit_volatility_5d', 'exit_volatility_20d', 'exit_volatility_60d',
            # ì¢…ë£Œ ì‹œì  ì‹œì¥ í™˜ê²½
            'exit_vix', 'exit_tnx_yield', 'exit_ratio_52w_high'
        ]
        available_a_type_features.extend([col for col in exit_technical_indicators if col in df.columns])
        
        # ===== 4. ë³€í™”ëŸ‰ ì§€í‘œ (Aìœ í˜•ë§Œ ì‚¬ìš© ê°€ëŠ¥!) =====
        change_indicators = [
            # ëª¨ë©˜í…€ ë³€í™”
            'change_momentum_5d', 'change_momentum_20d', 'change_momentum_60d',
            # ì´ë™í‰ê·  êµŒë¦¬ë„ ë³€í™”
            'change_ma_dev_5d', 'change_ma_dev_20d', 'change_ma_dev_60d',
            # ë³€ë™ì„± ë³€í™”
            'change_volatility_5d', 'change_volatility_60d',
            # ì‹œì¥ í™˜ê²½ ë³€í™”
            'change_vix', 'change_tnx_yield', 'change_ratio_52w_high'
        ]
        available_a_type_features.extend([col for col in change_indicators if col in df.columns])
        
        # ===== 5. ë³´ìœ  ê¸°ê°„ ì¤‘ ì‹œì¥ ì •ë³´ (Aìœ í˜•ë§Œ ì‚¬ìš© ê°€ëŠ¥!) =====
        holding_period_info = [
            'market_return_during_holding',  # ë³´ìœ  ê¸°ê°„ ì¤‘ ì‹œì¥ ìˆ˜ìµë¥ 
            'excess_return'                  # ì‹œì¥ ëŒ€ë¹„ ì´ˆê³¼ ìˆ˜ìµë¥ 
        ]
        available_a_type_features.extend([col for col in holding_period_info if col in df.columns])
        
        # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” í”¼ì²˜ë§Œ ì„ íƒ
        self.a_type_quality_features = [col for col in available_a_type_features 
                                       if col in df.columns and col not in excluded_features]
        
        if verbose:
            print(f"  Aìœ í˜• ì‚¬ìš© í”¼ì²˜: {len(self.a_type_quality_features)}ê°œ")
            print(f"  í¬í•¨ëœ í”¼ì²˜ ìœ í˜•: entry, exit, change, holding (ëª¨ë“  ì •ë³´ í™œìš©)")
        
        # ìˆ«ìí˜• ë°ì´í„°ë§Œ ì„ íƒ (XGBoost í˜¸í™˜ì„±)
        feature_data = df[self.a_type_quality_features].select_dtypes(include=[np.number])
        
        if verbose and len(feature_data.columns) != len(self.a_type_quality_features):
            print(f"  âš ï¸ ë¹„ìˆ«ìí˜• ì¹¼ëŸ¼ ì œì™¸: {len(self.a_type_quality_features) - len(feature_data.columns)}ê°œ")
        
        return feature_data
    
    # ================================
    # Bìœ í˜•: ê°œì„ ëœ ì§„ì… ì¡°ê±´ ë¶„ì„ (ì™„ì „íˆ ìƒˆë¡œ ì„¤ê³„)
    # ================================
    
    def create_b_type_entry_score(self, df, verbose=False):
        """Bìœ í˜•: ì¢…í•© ë§¤ìˆ˜ ì‹ í˜¸ ì ìˆ˜ (0-100ì ) - ê¸°ìˆ ì +í€ë”ë©˜í„¸+ì‹œì¥í™˜ê²½"""
        if verbose:
            print("ğŸš€ Bìœ í˜• (ë§¤ìˆ˜ ì‹ í˜¸ AI): ì¢…í•© ë§¤ìˆ˜ ì‹ í˜¸ ì ìˆ˜ ìƒì„± ì¤‘...")
            print("   â†’ ê¸°ìˆ ì  ë¶„ì„ + í€ë”ë©˜í„¸ + ì‹œì¥ í™˜ê²½ì„ ì¢…í•©í•œ ë§¤ìˆ˜ íƒ€ì´ë° ì ìˆ˜")
        
        df = df.copy()
        
        # ===== 1. ê¸°ìˆ ì  ì‹ í˜¸ (40%) =====
        technical_score = self._calculate_technical_signals(df)
        
        # ===== 2. í€ë”ë©˜í„¸ ì‹ í˜¸ (30%) =====
        fundamental_score = self._calculate_fundamental_signals(df)
        
        # ===== 3. ì‹œì¥ í™˜ê²½ ì‹ í˜¸ (30%) =====
        market_score = self._calculate_market_environment_signals(df)
        
        # ì¢…í•© ë§¤ìˆ˜ ì‹ í˜¸ ì ìˆ˜ (0-100)
        df['b_type_entry_score'] = (
            technical_score * 0.40 + 
            fundamental_score * 0.30 + 
            market_score * 0.30
        )
        
        # 0-100 ë²”ìœ„ ë³´ì¥
        df['b_type_entry_score'] = np.clip(df['b_type_entry_score'], 0, 100)
        
        if verbose:
            print(f"  âœ… ì¢…í•© ë§¤ìˆ˜ ì‹ í˜¸ ì ìˆ˜ ìƒì„± ì™„ë£Œ")
            print(f"  ì ìˆ˜ ë²”ìœ„: {df['b_type_entry_score'].min():.1f} ~ {df['b_type_entry_score'].max():.1f}")
            print(f"  ì ìˆ˜ í‰ê· : {df['b_type_entry_score'].mean():.1f}")
            print(f"  êµ¬ì„±: ê¸°ìˆ ì (40%) + í€ë”ë©˜í„¸(30%) + ì‹œì¥í™˜ê²½(30%)")
        
        return df
    
    def _calculate_technical_signals(self, df):
        """ê¸°ìˆ ì  ë¶„ì„ ì‹ í˜¸ ê³„ì‚° (0-100ì )"""
        signals = []
        
        # 1. ëª¨ë©˜í…€ ì‹ í˜¸ (25%)
        momentum_20d = df['entry_momentum_20d'].fillna(0)
        # ì ë‹¹í•œ í•˜ë½ í›„ ë°˜ë“± ì‹œì‘ì´ ë§¤ìˆ˜ ì‹ í˜¸
        momentum_signal = np.where(
            momentum_20d < -15, 20,      # ê³¼ë„í•œ í•˜ë½
            np.where(momentum_20d < -5, 85,   # ì ë‹¹í•œ í•˜ë½ (ë§¤ìˆ˜ ê¸°íšŒ!)
                np.where(momentum_20d < 5, 70,    # íš¡ë³´
                    np.where(momentum_20d < 15, 50, 30))))  # ê³¼ì—´
        signals.append(momentum_signal * 0.25)
        
        # 2. ì´ë™í‰ê·  ì‹ í˜¸ (25%)
        ma_dev_20d = df['entry_ma_dev_20d'].fillna(0)
        # ì´í‰ì„  ì•„ë˜ ìˆìœ¼ë©´ì„œ íšŒë³µ ì¡°ì§ì´ ë§¤ìˆ˜ ì‹ í˜¸
        ma_signal = np.where(
            ma_dev_20d < -10, 85,        # í¬ê²Œ ì´íƒˆ (ë§¤ìˆ˜ ê¸°íšŒ!)
            np.where(ma_dev_20d < -5, 70,     # ì ë‹¹íˆ ì´íƒˆ
                np.where(ma_dev_20d < 5, 50,      # ê·¼ì²˜
                    np.where(ma_dev_20d < 10, 30, 15))))  # ê³¼ì—´
        signals.append(ma_signal * 0.25)
        
        # 3. ê³¼ë§¤ë„/ê³¼ë§¤ìˆ˜ ì‹ í˜¸ (25%)
        ratio_52w = df['entry_ratio_52w_high'].fillna(50)
        # 52ì£¼ ê³ ì  ëŒ€ë¹„ ë‚®ì„ìˆ˜ë¡ ë§¤ìˆ˜ ì‹ í˜¸
        oversold_signal = (100 - ratio_52w)  # 0-100 ìë™ ë³€í™˜
        signals.append(oversold_signal * 0.25)
        
        # 4. ë³€ë™ì„± ì‹ í˜¸ (25%)
        volatility_20d = df['entry_volatility_20d'].fillna(25)
        # ì ë‹¹í•œ ë³€ë™ì„±ì´ ë§¤ìˆ˜ ì‹ í˜¸
        vol_signal = np.where(
            volatility_20d < 15, 40,     # ë„ˆë¬´ ë‚®ìŒ
            np.where(volatility_20d < 30, 85,     # ì ì • (ë§¤ìˆ˜ ê¸°íšŒ!)
                np.where(volatility_20d < 50, 60, 20)))  # ë„ˆë¬´ ë†’ìŒ
        signals.append(vol_signal * 0.25)
        
        return np.sum(signals, axis=0)
    
    def _calculate_fundamental_signals(self, df):
        """í€ë”ë©˜í„¸ ë¶„ì„ ì‹ í˜¸ ê³„ì‚° (0-100ì )"""
        signals = []
        
        # 1. ë°¸ë¥˜ì—ì´ì…˜ ì‹ í˜¸ (40%)
        pe_ratio = df['entry_pe_ratio'].fillna(20)
        # ë‚®ì€ PERì´ ë§¤ìˆ˜ ì‹ í˜¸ (ë‹¨, ë„ˆë¬´ ë‚®ìœ¼ë©´ ë¬¸ì œ)
        pe_signal = np.where(
            pe_ratio < 5, 30,           # ë„ˆë¬´ ë‚®ìŒ (ë¬¸ì œ?)
            np.where(pe_ratio < 15, 85,      # ì €í‰ê°€ (ë§¤ìˆ˜!)
                np.where(pe_ratio < 25, 60,      # ì ì •
                    np.where(pe_ratio < 40, 35, 15))))  # ê³ í‰ê°€
        signals.append(pe_signal * 0.4)
        
        # 2. í’ˆì§ˆ ì‹ í˜¸ (30%)
        roe = df['entry_roe'].fillna(10)
        # ë†’ì€ ROEê°€ ë§¤ìˆ˜ ì‹ í˜¸
        roe_signal = np.where(
            roe < 5, 30,               # ë‚®ì€ í’ˆì§ˆ
            np.where(roe < 10, 50,          # í‰ê· 
                np.where(roe < 15, 70,          # ì–‘í˜¸
                    np.where(roe < 20, 85, 95))))   # ìš°ìˆ˜
        signals.append(roe_signal * 0.3)
        
        # 3. ì„±ì¥ì„± ì‹ í˜¸ (30%)
        earnings_growth = df['entry_earnings_growth'].fillna(5)
        # ì ë‹¹í•œ ì„±ì¥ì´ ë§¤ìˆ˜ ì‹ í˜¸
        growth_signal = np.where(
            earnings_growth < -10, 20,   # ì—­ì„±ì¥
            np.where(earnings_growth < 0, 40,    # ê°ì†Œ
                np.where(earnings_growth < 10, 70,   # ì ë‹¹í•œ ì„±ì¥
                    np.where(earnings_growth < 25, 85, 60))))  # ê³ ì„±ì¥
        signals.append(growth_signal * 0.3)
        
        return np.sum(signals, axis=0)
    
    def _calculate_market_environment_signals(self, df):
        """ì‹œì¥ í™˜ê²½ ì‹ í˜¸ ê³„ì‚° (0-100ì )"""
        signals = []
        
        # 1. VIX ì‹ í˜¸ (40%)
        vix = df['entry_vix'].fillna(20)
        # ë‚®ì€ VIXê°€ ë§¤ìˆ˜ ì‹ í˜¸
        vix_signal = np.where(
            vix < 15, 90,              # ë§¤ìš° ì•ˆì • (ë§¤ìˆ˜!)
            np.where(vix < 20, 80,          # ì•ˆì •
                np.where(vix < 25, 60,          # ë³´í†µ
                    np.where(vix < 35, 40, 20))))   # ë¶ˆì•ˆì •
        signals.append(vix_signal * 0.4)
        
        # 2. ê¸ˆë¦¬ í™˜ê²½ ì‹ í˜¸ (30%)
        tnx_yield = df['entry_tnx_yield'].fillna(2.5)
        # ì ì • ê¸ˆë¦¬ê°€ ë§¤ìˆ˜ ì‹ í˜¸
        rate_signal = np.where(
            tnx_yield < 1, 60,         # ë„ˆë¬´ ë‚®ìŒ
            np.where(tnx_yield < 3, 85,     # ì ì • (ë§¤ìˆ˜!)
                np.where(tnx_yield < 5, 60, 30)))  # ë†’ìŒ, ë„ˆë¬´ ë†’ìŒ
        signals.append(rate_signal * 0.3)
        
        # 3. ì‹œì¥ ì¶”ì„¸ ì‹ í˜¸ (30%) - ì‹¤ì œ ì¡´ì¬í•˜ëŠ” í”¼ì²˜ ì‚¬ìš©
        market_return_20d = df.get('market_entry_cum_return_20d', pd.Series([0]*len(df))).fillna(0)
        # ì ë‹¹í•œ ìƒìŠ¹ ì¶”ì„¸ê°€ ë§¤ìˆ˜ ì‹ í˜¸  
        trend_signal = np.where(
            market_return_20d < -10, 30,   # ê°•í•œ í•˜ë½
            np.where(market_return_20d < -5, 60,    # ì•½í•œ í•˜ë½
                np.where(market_return_20d < 5, 85,      # íš¡ë³´/ì ë‹¹í•œ ìƒìŠ¹ (ë§¤ìˆ˜!)
                    np.where(market_return_20d < 10, 70, 40))))  # ê³¼ì—´
        signals.append(trend_signal * 0.3)
        
        return np.sum(signals, axis=0)
    
    def prepare_b_type_features(self, df, verbose=False):
        """Bìœ í˜•: ëŒ€í­ í™•ì¥ëœ ì§„ì… ì¡°ê±´ ë¶„ì„ìš© í”¼ì²˜ ì¤€ë¹„ (ì§„ì… ì‹œì  ì •ë³´ë§Œ ì‚¬ìš©)"""
        if verbose:
            print("ğŸ”§ Bìœ í˜•: ëŒ€í­ í™•ì¥ëœ ì§„ì… ì¡°ê±´ ë¶„ì„ìš© í”¼ì²˜ ì¤€ë¹„ ì¤‘...")
            print("   â†’ ê¸°ìˆ ì  + í€ë”ë©˜í„¸ + ì‹œì¥í™˜ê²½ + ì‚°ì—… ì •ë³´ ì¢…í•©")
        
        # ===== 1. ê¸°ë³¸ ê¸°ìˆ ì  ì§€í‘œ =====
        technical_features = [
            # ëª¨ë©˜í…€ ì§€í‘œ
            'entry_momentum_5d', 'entry_momentum_20d', 'entry_momentum_60d',
            
            # ì´ë™í‰ê·  ê¸°ë°˜ ì§€í‘œ
            'entry_ma_dev_5d', 'entry_ma_dev_20d', 'entry_ma_dev_60d',
            
            # ë³€ë™ì„± ì§€í‘œ
            'entry_volatility_5d', 'entry_volatility_20d', 'entry_volatility_60d',
            
            # ë³€ë™ì„± ë³€í™”ìœ¨
            'entry_vol_change_5d', 'entry_vol_change_20d', 'entry_vol_change_60d',
            
            # ê°€ê²© ìœ„ì¹˜
            'entry_ratio_52w_high'
        ]
        
        # ===== 2. í€ë”ë©˜í„¸ ì§€í‘œ =====
        fundamental_features = [
            'entry_pe_ratio',           # P/E ë¹„ìœ¨
            'entry_pb_ratio',           # P/B ë¹„ìœ¨  
            'entry_roe',                # ROE
            'entry_operating_margin',    # ì˜ì—…ì´ìµë¥ 
            'entry_debt_equity_ratio',   # ë¶€ì±„ë¹„ìœ¨
            'entry_earnings_growth'      # ì´ìµ ì„±ì¥ë¥ 
        ]
        
        # ===== 3. ì‹œì¥ í™˜ê²½ ì§€í‘œ =====
        market_environment_features = [
            # VIX & ê¸ˆë¦¬ (ì‹¤ì œ ì¡´ì¬)
            'entry_vix', 'entry_tnx_yield',
            
            # ì‹œì¥ ìˆ˜ìµë¥  ë° ì¶”ì„¸ (ì‹¤ì œ ì¡´ì¬)
            'market_entry_ma_return_5d', 'market_entry_ma_return_20d',
            'market_entry_cum_return_5d', 'market_entry_cum_return_20d',
            'market_entry_volatility_20d'
            
            # ê³ ê¸‰ ì‹œì¥ ì»´í¬ë„ŒíŠ¸ë“¤ì€ ì‹¤ì œ ë°ì´í„°ì— ì—†ì–´ì„œ ì œì™¸
        ]
        
        # ===== 4. ê±°ë˜ ê´€ë ¨ ì •ë³´ =====
        trading_features = [
            'position_size_pct'  # í¬ì§€ì…˜ í¬ê¸°
        ]
        
        # ëª¨ë“  ì ì¬ì  í”¼ì²˜ ê²°í•©
        potential_entry_features = (
            technical_features + 
            fundamental_features + 
            market_environment_features + 
            trading_features
        )
        
        # ì‹¤ì œ ì¡´ì¬í•˜ê³  ì•ˆì „í•œ í”¼ì²˜ë§Œ ì„ íƒ
        safe_features = []
        
        # ë¯¸ë˜ ì •ë³´ ì°¨ë‹¨ì„ ìœ„í•œ í‚¤ì›Œë“œ ì²´í¬
        forbidden_keywords = ['exit_', 'change_', 'holding_period', 'market_return_during', 'excess_return']
        
        for feature in potential_entry_features:
            # ê¸ˆì§€ëœ í‚¤ì›Œë“œ ì²´í¬
            is_safe = True
            for forbidden in forbidden_keywords:
                if forbidden in feature:
                    is_safe = False
                    break
            if is_safe and feature in df.columns:
                safe_features.append(feature)
        
        # ===== 5. ë²”ì£¼í˜• í”¼ì²˜ëŠ” ì¼ë‹¨ ì œì™¸ (ë³µì¡ì„± ë•Œë¬¸) =====
        # categorical_features = self._encode_categorical_features(df)
        # safe_features.extend(categorical_features)
        
        self.b_type_entry_features = safe_features
        
        if verbose:
            print(f"  Bìœ í˜• ì‚¬ìš© í”¼ì²˜: {len(self.b_type_entry_features)}ê°œ")
            print(f"  êµ¬ì„±:")
            print(f"    â€¢ ê¸°ìˆ ì  ì§€í‘œ: {len([f for f in safe_features if any(t in f for t in ['momentum', 'ma_dev', 'volatility', 'ratio'])])}ê°œ")
            print(f"    â€¢ í€ë”ë©˜í„¸: {len([f for f in safe_features if any(t in f for t in ['pe_', 'pb_', 'roe', 'margin', 'debt', 'growth'])])}ê°œ")
            print(f"    â€¢ ì‹œì¥í™˜ê²½: {len([f for f in safe_features if any(t in f for t in ['vix', 'tnx', 'market_'])])}ê°œ")
            print(f"    â€¢ ê±°ë˜ê´€ë ¨: {len([f for f in safe_features if 'position' in f])}ê°œ")
            print(f"  Data Leakage ë°©ì§€: exit_, change_, holding_ ì •ë³´ ì™„ì „ ì°¨ë‹¨")
        
        # í”¼ì²˜ ë°ì´í„° ë°˜í™˜ (ìˆ«ìí˜• ë°ì´í„°ë§Œ - XGBoost í˜¸í™˜ì„±)
        if self.b_type_entry_features:
            feature_data = df[self.b_type_entry_features].select_dtypes(include=[np.number])
            
            if verbose and len(feature_data.columns) != len(self.b_type_entry_features):
                print(f"  âš ï¸ ë¹„ìˆ«ìí˜• ì¹¼ëŸ¼ ì œì™¸: {len(self.b_type_entry_features) - len(feature_data.columns)}ê°œ")
            
            return feature_data
        else:
            return pd.DataFrame()
    
    def _encode_categorical_features(self, df):
        """ë²”ì£¼í˜• í”¼ì²˜ ì¸ì½”ë”©"""
        categorical_features = []
        
        # ì‚°ì—… ì •ë³´ ì›-í•« ì¸ì½”ë”© (ìƒìœ„ Nê°œ ì‚°ì—…ë§Œ)
        if 'industry' in df.columns:
            # ê°€ì¥ ë§ì€ ìƒìœ„ 10ê°œ ì‚°ì—…ë§Œ ì¸ì½”ë”©
            top_industries = df['industry'].value_counts().head(10).index.tolist()
            
            for industry in top_industries:
                feature_name = f'industry_{industry.replace(" ", "_").replace("&", "and").lower()}'
                categorical_features.append(feature_name)
        
        return categorical_features
    
    def _get_categorical_data(self, df):
        """ë²”ì£¼í˜• ë°ì´í„° ìƒì„±"""
        categorical_data = pd.DataFrame(index=df.index)
        
        # ì‚°ì—… ì •ë³´ ì›-í•« ì¸ì½”ë”©
        if 'industry' in df.columns:
            top_industries = df['industry'].value_counts().head(10).index.tolist()
            
            for industry in top_industries:
                feature_name = f'industry_{industry.replace(" ", "_").replace("&", "and").lower()}'
                categorical_data[feature_name] = (df['industry'] == industry).astype(int)
        
        return categorical_data
    
    # ================================
    # ëŒ€ê·œëª¨ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œìŠ¤í…œ
    # ================================
    
    def get_massive_hyperparameter_grid(self, model_type='both'):
        """ëŒ€ê·œëª¨ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ìƒì„±"""
        
        # XGBoost ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° (ë§¤ìš° ê´‘ë²”ìœ„)
        xgb_param_grid = {
            # íŠ¸ë¦¬ êµ¬ì¡° íŒŒë¼ë¯¸í„°
            'max_depth': [3, 4, 5, 6, 7, 8, 9, 10, 12, 15],
            'min_child_weight': [1, 2, 3, 4, 5, 6, 8, 10, 15, 20],
            'subsample': [0.6, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0],
            'colsample_bytree': [0.6, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0],
            'colsample_bylevel': [0.6, 0.7, 0.8, 0.9, 1.0],
            'colsample_bynode': [0.6, 0.7, 0.8, 0.9, 1.0],
            
            # í•™ìŠµë¥  ë° ë¶€ìŠ¤íŒ… íŒŒë¼ë¯¸í„°  
            'learning_rate': [0.01, 0.02, 0.03, 0.05, 0.07, 0.1, 0.15, 0.2, 0.25, 0.3],
            'n_estimators': [100, 150, 200, 250, 300, 400, 500, 600, 800, 1000, 1200, 1500],
            
            # ì •ê·œí™” íŒŒë¼ë¯¸í„°
            'reg_alpha': [0, 0.01, 0.05, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0],
            'reg_lambda': [0.5, 1.0, 1.5, 2.0, 3.0, 5.0, 10.0, 15.0, 20.0, 50.0],
            
            # ê³ ê¸‰ íŒŒë¼ë¯¸í„°
            'gamma': [0, 0.01, 0.05, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0],
            'max_delta_step': [0, 1, 2, 3, 5, 10],
            'scale_pos_weight': [1, 2, 3, 5, 10],
            
            # íŠ¸ë¦¬ ìƒì„± ë°©ë²•
            'tree_method': ['hist', 'approx'] if torch.cuda.is_available() else ['hist'],
            'grow_policy': ['depthwise', 'lossguide'],
            
            # ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„°
            'max_leaves': [0, 31, 63, 127, 255, 511],
            'max_bin': [128, 256, 512],
        }
        
        # Aìœ í˜•ê³¼ Bìœ í˜• ê°ê° ë‹¤ë¥¸ íŒŒë¼ë¯¸í„° ë²”ìœ„ ì ìš©
        if model_type == 'A' or model_type == 'both':
            # Aìœ í˜•: í’ˆì§ˆ ì˜ˆì¸¡ - ì •í™•ë„ ì¤‘ì‹œ
            a_specific_params = {
                'objective': ['reg:squarederror'],
                'eval_metric': ['rmse'],
                # í’ˆì§ˆ ì˜ˆì¸¡ì— íŠ¹í™”ëœ íŒŒë¼ë¯¸í„°
                'max_depth': [4, 5, 6, 7, 8, 9, 10],  # ê¹Šì´ ì œí•œ
                'learning_rate': [0.01, 0.02, 0.03, 0.05, 0.07, 0.1],  # ë‚®ì€ í•™ìŠµë¥ 
                'n_estimators': [300, 400, 500, 600, 800, 1000, 1200],  # ë§ì€ íŠ¸ë¦¬
                'reg_alpha': [0.1, 0.2, 0.5, 1.0, 2.0, 5.0],  # ì •ê·œí™” ê°•í™”
                'reg_lambda': [1.0, 2.0, 3.0, 5.0, 10.0, 20.0],
            }
            a_param_grid = {**xgb_param_grid, **a_specific_params}
        
        if model_type == 'B' or model_type == 'both':
            # Bìœ í˜•: ì‹ í˜¸ ì˜ˆì¸¡ - ì¼ë°˜í™” ì¤‘ì‹œ
            b_specific_params = {
                'objective': ['reg:squarederror'],
                'eval_metric': ['rmse'], 
                # ì‹ í˜¸ ì˜ˆì¸¡ì— íŠ¹í™”ëœ íŒŒë¼ë¯¸í„°
                'max_depth': [3, 4, 5, 6, 7, 8],  # ê³¼ì í•© ë°©ì§€
                'learning_rate': [0.05, 0.07, 0.1, 0.15, 0.2],  # ì ë‹¹í•œ í•™ìŠµë¥ 
                'n_estimators': [150, 200, 250, 300, 400, 500],  # ì ë‹¹í•œ íŠ¸ë¦¬ ìˆ˜
                'subsample': [0.7, 0.8, 0.85, 0.9],  # ìƒ˜í”Œë§ ê°•í™”
                'colsample_bytree': [0.7, 0.8, 0.85, 0.9],
            }
            b_param_grid = {**xgb_param_grid, **b_specific_params}
        
        if model_type == 'A':
            return {'A': a_param_grid}
        elif model_type == 'B':
            return {'B': b_param_grid}
        else:
            return {'A': a_param_grid, 'B': b_param_grid}
    
    def smart_hyperparameter_search(self, X_train, y_train, X_val, y_val, model_type, n_iter=200):
        """ìŠ¤ë§ˆíŠ¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê²€ìƒ‰ (RandomizedSearchCV + GridSearchCV ì¡°í•©)"""
        
        print(f"ğŸ” {model_type}ìœ í˜• í•˜ì´í¼íŒŒë¼ë¯¸í„° ê²€ìƒ‰ ì‹œì‘ (ìµœëŒ€ {n_iter}íšŒ ì‹œë„)")
        
        # ë°ì´í„° íƒ€ì… ë””ë²„ê¹…
        print(f"  ë””ë²„ê·¸: X_train íƒ€ì…ë“¤: {X_train.dtypes.value_counts()}")
        print(f"  ë””ë²„ê·¸: y_train íƒ€ì…: {y_train.dtype if hasattr(y_train, 'dtype') else type(y_train)}")
        print(f"  ë””ë²„ê·¸: ë¬¸ì œê°€ ë  ìˆ˜ ìˆëŠ” ì»¬ëŸ¼ë“¤:")
        for col in X_train.columns:
            if X_train[col].dtype == 'object' or 'datetime' in str(X_train[col].dtype):
                print(f"    - {col}: {X_train[col].dtype}")
        
        # ìˆ«ìí˜• ë°ì´í„°ë§Œ ê°•ì œë¡œ ì„ íƒ
        numeric_cols = X_train.select_dtypes(include=[np.number]).columns
        X_train_clean = X_train[numeric_cols].copy()
        X_val_clean = X_val[numeric_cols].copy()
        
        print(f"  ë””ë²„ê·¸: ì •ì œ í›„ í”¼ì²˜ ìˆ˜: {len(X_train_clean.columns)} (ì›ë³¸: {len(X_train.columns)})")
        
        param_grids = self.get_massive_hyperparameter_grid(model_type)
        param_grid = param_grids[model_type]
        
        # 1ë‹¨ê³„: RandomizedSearchCVë¡œ ë„“ì€ ë²”ìœ„ íƒìƒ‰
        print(f"  1ë‹¨ê³„: RandomizedSearch - {n_iter//2}íšŒ ì‹œë„")
        
        base_model = xgb.XGBRegressor(
            random_state=42,
            n_jobs=-1,
            verbosity=0
        )
        
        # TimeSeriesSplit ì‚¬ìš© (ì‹œê³„ì—´ ë°ì´í„° íŠ¹ì„±ìƒ)
        tscv = TimeSeriesSplit(n_splits=3)
        
        random_search = RandomizedSearchCV(
            base_model,
            param_grid,
            n_iter=n_iter//2,
            cv=tscv,
            scoring='r2',
            n_jobs=-1,
            verbose=1,
            random_state=42,
            return_train_score=False
        )
        
        random_search.fit(X_train_clean, y_train)
        
        print(f"  1ë‹¨ê³„ ì™„ë£Œ. ìµœê³  ì ìˆ˜: {random_search.best_score_:.4f}")
        print(f"  ìµœê³  íŒŒë¼ë¯¸í„°: {random_search.best_params_}")
        
        # 2ë‹¨ê³„: ìµœì  íŒŒë¼ë¯¸í„° ì£¼ë³€ ì„¸ë°€ íƒìƒ‰
        print(f"  2ë‹¨ê³„: ìµœì  íŒŒë¼ë¯¸í„° ì£¼ë³€ GridSearch")
        
        best_params = random_search.best_params_
        refined_grid = self._create_refined_grid(best_params)
        
        if len(list(ParameterGrid(refined_grid))) <= 50:  # ì¡°í•© ìˆ˜ê°€ ì ìœ¼ë©´ GridSearch
            grid_search = GridSearchCV(
                base_model,
                refined_grid,
                cv=tscv,
                scoring='r2',
                n_jobs=-1,
                verbose=1,
                return_train_score=False
            )
            
            grid_search.fit(X_train_clean, y_train)
            final_model = grid_search.best_estimator_
            final_params = grid_search.best_params_
            final_score = grid_search.best_score_
            
        else:  # ì¡°í•© ìˆ˜ê°€ ë§ìœ¼ë©´ ì¶”ê°€ RandomizedSearch
            refined_search = RandomizedSearchCV(
                base_model,
                refined_grid,
                n_iter=min(n_iter//2, 100),
                cv=tscv,
                scoring='r2',
                n_jobs=-1,
                verbose=1,
                random_state=42,
                return_train_score=False
            )
            
            refined_search.fit(X_train_clean, y_train)
            final_model = refined_search.best_estimator_
            final_params = refined_search.best_params_
            final_score = refined_search.best_score_
        
        print(f"  2ë‹¨ê³„ ì™„ë£Œ. ìµœì¢… ì ìˆ˜: {final_score:.4f}")
        
        # 3ë‹¨ê³„: ê²€ì¦ ì„¸íŠ¸ë¡œ ìµœì¢… í™•ì¸
        final_model.fit(X_train_clean, y_train)
        val_pred = final_model.predict(X_val_clean)
        val_r2 = r2_score(y_val, val_pred)
        
        print(f"  ê²€ì¦ ì„¸íŠ¸ RÂ²: {val_r2:.4f}")
        
        return final_model, final_params, {
            'cv_score': final_score,
            'val_r2': val_r2,
            'search_iterations': n_iter
        }
    
    def _create_refined_grid(self, best_params):
        """ìµœì  íŒŒë¼ë¯¸í„° ì£¼ë³€ì˜ ì„¸ë°€í•œ ê·¸ë¦¬ë“œ ìƒì„±"""
        refined_grid = {}
        
        for param, value in best_params.items():
            if param == 'max_depth':
                refined_grid[param] = [max(3, value-1), value, min(15, value+1)]
            elif param == 'learning_rate':
                refined_grid[param] = [max(0.01, value*0.8), value, min(0.3, value*1.2)]
            elif param == 'n_estimators':
                refined_grid[param] = [max(100, value-100), value, value+100]
            elif param == 'min_child_weight':
                refined_grid[param] = [max(1, value-1), value, value+1]
            elif param in ['subsample', 'colsample_bytree']:
                refined_grid[param] = [max(0.6, value-0.05), value, min(1.0, value+0.05)]
            elif param in ['reg_alpha', 'reg_lambda']:
                if value == 0:
                    refined_grid[param] = [0, 0.01, 0.05]
                else:
                    refined_grid[param] = [max(0, value*0.5), value, value*2.0]
            elif param == 'gamma':
                if value == 0:
                    refined_grid[param] = [0, 0.01, 0.05]
                else:
                    refined_grid[param] = [max(0, value*0.5), value, value*2.0]
            else:
                refined_grid[param] = [value]  # ë‹¤ë¥¸ íŒŒë¼ë¯¸í„°ëŠ” ê³ ì •
        
        return refined_grid
    
    # ================================
    # ì „ì²´ ê¸°ê°„ Train/Val/Test ë¶„í• 
    # ================================
    
    def create_global_split(self, df, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):
        """ì „ì²´ ê¸°ê°„ì„ ì‹œê°„ìˆœìœ¼ë¡œ train/val/test ë¶„í• """
        
        df_sorted = df.sort_values('entry_datetime')
        total_len = len(df_sorted)
        
        train_end_idx = int(total_len * train_ratio)
        val_end_idx = int(total_len * (train_ratio + val_ratio))
        
        train_data = df_sorted.iloc[:train_end_idx].copy()
        val_data = df_sorted.iloc[train_end_idx:val_end_idx].copy()
        test_data = df_sorted.iloc[val_end_idx:].copy()
        
        split_info = {
            'train_period': f"{train_data['entry_datetime'].min().date()} ~ {train_data['entry_datetime'].max().date()}",
            'val_period': f"{val_data['entry_datetime'].min().date()} ~ {val_data['entry_datetime'].max().date()}",
            'test_period': f"{test_data['entry_datetime'].min().date()} ~ {test_data['entry_datetime'].max().date()}",
            'train_samples': len(train_data),
            'val_samples': len(val_data), 
            'test_samples': len(test_data)
        }
        
        return train_data, val_data, test_data, split_info
    
    def run_global_hyperparameter_optimization(self, data_path, verbose=True):
        """ì „ì²´ ê¸°ê°„ ë¶„í•  + ëŒ€ê·œëª¨ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”"""
        
        if verbose:
            print("ğŸŒ ì „ì²´ ê¸°ê°„ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ëª¨ë“œ")
            print("=" * 80)
        
        # ë°ì´í„° ë¡œë“œ
        df = pd.read_csv(data_path)
        df['entry_datetime'] = pd.to_datetime(df['entry_datetime'])
        df['exit_datetime'] = pd.to_datetime(df['exit_datetime'])
        
        if verbose:
            print(f"\nğŸ“Š ì´ ë°ì´í„°: {len(df):,}ê°œ")
            print(f"ğŸ“… ê¸°ê°„: {df['entry_datetime'].min().date()} ~ {df['entry_datetime'].max().date()}")
        
        # ì „ì²´ ê¸°ê°„ ë¶„í• 
        train_data, val_data, test_data, split_info = self.create_global_split(df)
        
        if verbose:
            print(f"\nğŸ“Š ë°ì´í„° ë¶„í• :")
            print(f"  í›ˆë ¨: {split_info['train_samples']:,}ê°œ ({split_info['train_period']})")
            print(f"  ê²€ì¦: {split_info['val_samples']:,}ê°œ ({split_info['val_period']})")  
            print(f"  í…ŒìŠ¤íŠ¸: {split_info['test_samples']:,}ê°œ ({split_info['test_period']})")
        
        # Aìœ í˜• ìµœì í™”
        if verbose:
            print(f"\nğŸ”¬ Aìœ í˜• ëª¨ë¸ ìµœì í™” ì‹œì‘")
        
        # Aìœ í˜• ë°ì´í„° ì¤€ë¹„
        train_data_a = self.create_a_type_quality_score(train_data, verbose=False)
        val_data_a = self.create_a_type_quality_score(val_data, verbose=False)
        test_data_a = self.create_a_type_quality_score(test_data, verbose=False)
        
        y_train_a = train_data_a['a_type_quality_score']
        y_val_a = val_data_a['a_type_quality_score']
        y_test_a = test_data_a['a_type_quality_score']
        
        X_train_a = self.prepare_a_type_features(train_data, verbose=False)
        X_val_a = self.prepare_a_type_features(val_data, verbose=False)
        X_test_a = self.prepare_a_type_features(test_data, verbose=False)
        
        # Aìœ í˜• í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (ì´ˆëŒ€ê·œëª¨ íƒìƒ‰)
        model_a, best_params_a, search_info_a = self.smart_hyperparameter_search(
            X_train_a, y_train_a, X_val_a, y_val_a, 'A', n_iter=1000  # ë©”ëª¨ë¦¬ ì•ˆì •ì„± ê³ ë ¤
        )
        
        # Aìœ í˜• ìµœì¢… í‰ê°€
        test_pred_a = model_a.predict(X_test_a)
        test_r2_a = r2_score(y_test_a, test_pred_a)
        
        # Bìœ í˜• ìµœì í™”
        if verbose:
            print(f"\nğŸš€ Bìœ í˜• ëª¨ë¸ ìµœì í™” ì‹œì‘")
        
        # Bìœ í˜• ë°ì´í„° ì¤€ë¹„
        train_data_b = self.create_b_type_entry_score(train_data, verbose=False)
        val_data_b = self.create_b_type_entry_score(val_data, verbose=False)
        test_data_b = self.create_b_type_entry_score(test_data, verbose=False)
        
        y_train_b = train_data_b['b_type_entry_score']
        y_val_b = val_data_b['b_type_entry_score']
        y_test_b = test_data_b['b_type_entry_score']
        
        X_train_b = self.prepare_b_type_features(train_data, verbose=False)
        X_val_b = self.prepare_b_type_features(val_data, verbose=False)
        X_test_b = self.prepare_b_type_features(test_data, verbose=False)
        
        # Bìœ í˜• í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (ì´ˆëŒ€ê·œëª¨ íƒìƒ‰)
        model_b, best_params_b, search_info_b = self.smart_hyperparameter_search(
            X_train_b, y_train_b, X_val_b, y_val_b, 'B', n_iter=1000  # ë©”ëª¨ë¦¬ ì•ˆì •ì„± ê³ ë ¤
        )
        
        # Bìœ í˜• ìµœì¢… í‰ê°€
        test_pred_b = model_b.predict(X_test_b)
        test_r2_b = r2_score(y_test_b, test_pred_b)
        
        # ê²°ê³¼ ì €ì¥
        self.global_results = {
            'split_info': split_info,
            'A_type': {
                'model': model_a,
                'best_params': best_params_a,
                'search_info': search_info_a,
                'test_r2': test_r2_a,
                'features_used': len(X_train_a.columns)
            },
            'B_type': {
                'model': model_b, 
                'best_params': best_params_b,
                'search_info': search_info_b,
                'test_r2': test_r2_b,
                'features_used': len(X_train_b.columns)
            }
        }
        
        self.best_params_a = best_params_a
        self.best_params_b = best_params_b
        
        if verbose:
            print(f"\nğŸ† ì „ì²´ ê¸°ê°„ ìµœì í™” ê²°ê³¼:")
            print(f"  Aìœ í˜• Test RÂ²: {test_r2_a:.4f}")
            print(f"  Bìœ í˜• Test RÂ²: {test_r2_b:.4f}")
            print(f"  Aìœ í˜• ìµœì  íŒŒë¼ë¯¸í„° ìˆ˜: {len(best_params_a)}")
            print(f"  Bìœ í˜• ìµœì  íŒŒë¼ë¯¸í„° ìˆ˜: {len(best_params_b)}")
        
        # ì „ì²´ ê¸°ê°„ ê²°ê³¼ ì €ì¥
        self.save_global_results()
        
        return self.global_results
    
    # ================================
    # Walk-Forward Validation (v4ì™€ ë™ì¼)
    # ================================
    
    def create_time_folds(self, df, verbose=False):
        """ì‹œê³„ì—´ ê¸°ë°˜ í´ë“œ ìƒì„±"""
        if verbose:
            print("ğŸ“… Walk-Forward Time Folds ìƒì„± ì¤‘...")
        
        df_sorted = df.sort_values('entry_datetime')
        start_date = pd.to_datetime(df_sorted['entry_datetime'].min())
        end_date = pd.to_datetime(df_sorted['entry_datetime'].max())
        
        folds = []
        current_train_start = start_date
        
        while True:
            train_end = current_train_start + pd.DateOffset(months=self.train_months)
            val_end = train_end + pd.DateOffset(months=self.val_months) 
            test_end = val_end + pd.DateOffset(months=self.test_months)
            
            if test_end > end_date:
                break
                
            folds.append({
                'fold_id': len(folds) + 1,
                'train_start': current_train_start.strftime('%Y-%m-%d'),
                'train_end': train_end.strftime('%Y-%m-%d'),
                'val_start': train_end.strftime('%Y-%m-%d'),
                'val_end': val_end.strftime('%Y-%m-%d'),
                'test_start': val_end.strftime('%Y-%m-%d'),
                'test_end': test_end.strftime('%Y-%m-%d')
            })
            
            current_train_start += pd.DateOffset(months=self.step_months)
        
        if verbose:
            print(f"  ìƒì„±ëœ í´ë“œ ìˆ˜: {len(folds)}ê°œ")
            for fold in folds:
                print(f"  í´ë“œ {fold['fold_id']}: {fold['train_start']} ~ {fold['test_end']}")
        
        return folds
    
    def run_hybrid_training(self, data_path, verbose=True):
        """ê°œì„ ëœ í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ì‹¤í–‰"""
        if verbose:
            print("ğŸš€ ê°œì„ ëœ í•˜ì´ë¸Œë¦¬ë“œ íŠ¸ë ˆì´ë”© AI v5 ì‹œì‘!")
            print("=" * 80)
            print("ğŸ“Š Aìœ í˜•: ê±°ë˜ í’ˆì§ˆ ë¶„ì„ (v4 ë™ì¼)")
            print("ğŸš€ Bìœ í˜•: ì‹¤ì œ ìˆ˜ìµë¥  ê¸°ë°˜ ì§„ì… ì¡°ê±´ ì ìˆ˜ (ìƒˆë¡œ ì„¤ê³„)")
            print("=" * 80)
        
        # ë°ì´í„° ë¡œë“œ
        if verbose:
            print("\nğŸ“ ë°ì´í„° ë¡œë“œ ì¤‘...")
        df = pd.read_csv(data_path)
        df['entry_datetime'] = pd.to_datetime(df['entry_datetime'])
        df['exit_datetime'] = pd.to_datetime(df['exit_datetime'])
        
        if verbose:
            print(f"  ì´ ë°ì´í„°: {len(df):,}ê°œ ê±°ë˜")
            print(f"  ê¸°ê°„: {df['entry_datetime'].min().date()} ~ {df['entry_datetime'].max().date()}")
        
        # í´ë“œ ìƒì„±
        time_folds = self.create_time_folds(df, verbose)
        if len(time_folds) == 0:
            print("âŒ ìƒì„±ëœ í´ë“œê°€ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„° ê¸°ê°„ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            return
        
        self.fold_results = []
        
        # ê° í´ë“œë³„ í›ˆë ¨
        for fold_info in time_folds:
            if verbose:
                print(f"\nğŸ“Š í´ë“œ {fold_info['fold_id']} ì²˜ë¦¬ ì¤‘...")
                print(f"  í›ˆë ¨: {fold_info['train_start']} ~ {fold_info['train_end']}")
                print(f"  ê²€ì¦: {fold_info['val_start']} ~ {fold_info['val_end']}")  
                print(f"  í…ŒìŠ¤íŠ¸: {fold_info['test_start']} ~ {fold_info['test_end']}")
            
            # ë°ì´í„° ë¶„í• 
            train_data = df[
                (df['entry_datetime'] >= fold_info['train_start']) & 
                (df['entry_datetime'] < fold_info['train_end'])
            ].copy()
            
            val_data = df[
                (df['entry_datetime'] >= fold_info['val_start']) & 
                (df['entry_datetime'] < fold_info['val_end'])
            ].copy()
            
            test_data = df[
                (df['entry_datetime'] >= fold_info['test_start']) & 
                (df['entry_datetime'] < fold_info['test_end'])
            ].copy()
            
            if verbose:
                print(f"  ë°ì´í„° í¬ê¸° - í›ˆë ¨: {len(train_data):,}, ê²€ì¦: {len(val_data):,}, í…ŒìŠ¤íŠ¸: {len(test_data):,}")
            
            # ì‹œì¥ í™˜ê²½ ë¶„ì„
            train_vix_mean = train_data['entry_vix'].mean()
            val_vix_mean = val_data['entry_vix'].mean()
            test_vix_mean = test_data['entry_vix'].mean()
            
            train_return_mean = train_data['return_pct'].mean()
            val_return_mean = val_data['return_pct'].mean()
            test_return_mean = test_data['return_pct'].mean()
            
            # ê²°ê³¼ ì €ì¥ìš©
            fold_result = {
                'fold_id': fold_info['fold_id'],
                'fold_info': fold_info,
                'model_results': {},
                'market_stats': {
                    'train_vix_mean': train_vix_mean,
                    'val_vix_mean': val_vix_mean, 
                    'test_vix_mean': test_vix_mean,
                    'train_return_mean': train_return_mean,
                    'val_return_mean': val_return_mean,
                    'test_return_mean': test_return_mean
                }
            }
            
            # ===== Aìœ í˜• ëª¨ë¸ í›ˆë ¨ =====
            try:
                if verbose:
                    print("\n  ğŸ¯ Aìœ í˜•: ê±°ë˜ í’ˆì§ˆ ë¶„ì„ ëª¨ë¸ í›ˆë ¨...")
                
                # Aìœ í˜• ë¼ë²¨ ìƒì„±
                train_data_a = self.create_a_type_quality_score(train_data, verbose=False)
                val_data_a = self.create_a_type_quality_score(
                    val_data, 
                    risk_scaler=self.a_type_quality_scalers.get('risk_scaler'),
                    eff_scaler=self.a_type_quality_scalers.get('efficiency_scaler'),
                    verbose=False
                )
                test_data_a = self.create_a_type_quality_score(
                    test_data,
                    risk_scaler=self.a_type_quality_scalers.get('risk_scaler'),
                    eff_scaler=self.a_type_quality_scalers.get('efficiency_scaler'),
                    verbose=False
                )
                
                # Aìœ í˜• í”¼ì²˜ ì¤€ë¹„
                X_train_a = self.prepare_a_type_features(train_data_a, verbose=False)
                X_val_a = self.prepare_a_type_features(val_data_a, verbose=False)
                X_test_a = self.prepare_a_type_features(test_data_a, verbose=False)
                
                y_train_a = train_data_a['a_type_quality_score']
                y_val_a = val_data_a['a_type_quality_score']
                y_test_a = test_data_a['a_type_quality_score']
                
                # ê²°ì¸¡ì¹˜ ì œê±°
                train_mask_a = ~(X_train_a.isnull().any(axis=1) | y_train_a.isnull())
                val_mask_a = ~(X_val_a.isnull().any(axis=1) | y_val_a.isnull())
                test_mask_a = ~(X_test_a.isnull().any(axis=1) | y_test_a.isnull())
                
                if train_mask_a.sum() > 100 and val_mask_a.sum() > 10 and test_mask_a.sum() > 10:
                    X_train_a_clean = X_train_a[train_mask_a]
                    y_train_a_clean = y_train_a[train_mask_a]
                    X_val_a_clean = X_val_a[val_mask_a]
                    y_val_a_clean = y_val_a[val_mask_a]
                    X_test_a_clean = X_test_a[test_mask_a]
                    y_test_a_clean = y_test_a[test_mask_a]
                    
                    # Aìœ í˜• ëª¨ë¸ í›ˆë ¨
                    model_a = xgb.XGBRegressor(
                        n_estimators=100,
                        max_depth=6,
                        learning_rate=0.1,
                        random_state=42,
                        tree_method='hist',  # GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ë°©ì§€
                        n_jobs=-1
                    )
                    
                    model_a.fit(
                        X_train_a_clean, y_train_a_clean,
                        eval_set=[(X_val_a_clean, y_val_a_clean)],
                        verbose=False
                    )
                    
                    # Aìœ í˜• ì„±ëŠ¥ í‰ê°€ (ìƒì„¸)
                    val_pred_a = model_a.predict(X_val_a_clean)
                    test_pred_a = model_a.predict(X_test_a_clean)
                    
                    # ë‹¤ì–‘í•œ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
                    val_r2_a = r2_score(y_val_a_clean, val_pred_a)
                    test_r2_a = r2_score(y_test_a_clean, test_pred_a)
                    
                    val_mse_a = mean_squared_error(y_val_a_clean, val_pred_a)
                    test_mse_a = mean_squared_error(y_test_a_clean, test_pred_a)
                    
                    val_mae_a = mean_absolute_error(y_val_a_clean, val_pred_a)
                    test_mae_a = mean_absolute_error(y_test_a_clean, test_pred_a)
                    
                    # ìƒê´€ê³„ìˆ˜
                    val_corr_a = np.corrcoef(y_val_a_clean, val_pred_a)[0, 1] if len(y_val_a_clean) > 1 else 0
                    test_corr_a = np.corrcoef(y_test_a_clean, test_pred_a)[0, 1] if len(y_test_a_clean) > 1 else 0
                    
                    # í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„
                    feature_importance_a = model_a.feature_importances_
                    feature_names_a = X_train_a_clean.columns.tolist()
                    importance_dict_a = dict(zip(feature_names_a, feature_importance_a))
                    top_features_a = sorted(importance_dict_a.items(), key=lambda x: x[1], reverse=True)[:10]
                    
                    fold_result['model_results']['A_quality_model'] = {
                        # ê¸°ë³¸ ì„±ëŠ¥ ì§€í‘œ
                        'val_r2': val_r2_a,
                        'test_r2': test_r2_a,
                        'val_mse': val_mse_a,
                        'test_mse': test_mse_a,
                        'val_mae': val_mae_a,
                        'test_mae': test_mae_a,
                        'val_corr': val_corr_a,
                        'test_corr': test_corr_a,
                        
                        # ë°ì´í„° í¬ê¸°
                        'train_samples': len(X_train_a_clean),
                        'val_samples': len(X_val_a_clean),
                        'test_samples': len(X_test_a_clean),
                        'features_used': len(X_train_a_clean.columns),
                        
                        # í”¼ì²˜ ì¤‘ìš”ë„
                        'top_features': top_features_a,
                        'all_feature_importance': importance_dict_a
                    }
                    
                    if verbose:
                        print(f"    âœ… Aìœ í˜• ì„±ëŠ¥ ìƒì„¸:")
                        print(f"      ğŸ“Š RÂ²: Val={val_r2_a:.4f}, Test={test_r2_a:.4f}")
                        print(f"      ğŸ“Š ìƒê´€ê³„ìˆ˜: Val={val_corr_a:.4f}, Test={test_corr_a:.4f}")
                        print(f"      ğŸ“Š MSE: Val={val_mse_a:.4f}, Test={test_mse_a:.4f}")
                        print(f"      ğŸ“Š MAE: Val={val_mae_a:.4f}, Test={test_mae_a:.4f}")
                        print(f"      ğŸ” ìƒìœ„ í”¼ì²˜: {', '.join([f'{name}({imp:.3f})' for name, imp in top_features_a[:5]])}")
                
                else:
                    fold_result['model_results']['A_quality_model'] = {'error': 'Insufficient clean data'}
                    if verbose:
                        print("    âŒ Aìœ í˜•: ì¶©ë¶„í•œ ê¹¨ë—í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            except Exception as e:
                fold_result['model_results']['A_quality_model'] = {'error': str(e)}
                if verbose:
                    print(f"    âŒ Aìœ í˜• ì˜¤ë¥˜: {e}")
            
            # ===== Bìœ í˜• ëª¨ë¸ í›ˆë ¨ =====
            try:
                if verbose:
                    print("\n  ğŸš€ Bìœ í˜•: ê°œì„ ëœ ì§„ì… ì¡°ê±´ ë¶„ì„ ëª¨ë¸ í›ˆë ¨...")
                
                # Bìœ í˜• ë¼ë²¨ ìƒì„± (ìƒˆë¡œ ì„¤ê³„ëœ ë°©ì‹)
                train_data_b = self.create_b_type_entry_score(train_data, verbose=False)
                val_data_b = self.create_b_type_entry_score(val_data, verbose=False)
                test_data_b = self.create_b_type_entry_score(test_data, verbose=False)
                
                # Bìœ í˜• í”¼ì²˜ ì¤€ë¹„
                X_train_b = self.prepare_b_type_features(train_data_b, verbose=False)
                X_val_b = self.prepare_b_type_features(val_data_b, verbose=False)
                X_test_b = self.prepare_b_type_features(test_data_b, verbose=False)
                
                y_train_b = train_data_b['b_type_entry_score']
                y_val_b = val_data_b['b_type_entry_score']
                y_test_b = test_data_b['b_type_entry_score']
                
                if len(X_train_b.columns) > 0:
                    # ê²°ì¸¡ì¹˜ ì œê±°
                    train_mask_b = ~(X_train_b.isnull().any(axis=1) | y_train_b.isnull())
                    val_mask_b = ~(X_val_b.isnull().any(axis=1) | y_val_b.isnull())
                    test_mask_b = ~(X_test_b.isnull().any(axis=1) | y_test_b.isnull())
                    
                    if train_mask_b.sum() > 100 and val_mask_b.sum() > 10 and test_mask_b.sum() > 10:
                        X_train_b_clean = X_train_b[train_mask_b]
                        y_train_b_clean = y_train_b[train_mask_b]
                        X_val_b_clean = X_val_b[val_mask_b]
                        y_val_b_clean = y_val_b[val_mask_b]
                        X_test_b_clean = X_test_b[test_mask_b]
                        y_test_b_clean = y_test_b[test_mask_b]
                        
                        # Bìœ í˜• ëª¨ë¸ í›ˆë ¨
                        model_b = xgb.XGBRegressor(
                            n_estimators=100,
                            max_depth=6,
                            learning_rate=0.1,
                            random_state=42,
                            tree_method='hist',  # GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ë°©ì§€
                            n_jobs=-1
                        )
                        
                        model_b.fit(
                            X_train_b_clean, y_train_b_clean,
                            eval_set=[(X_val_b_clean, y_val_b_clean)],
                            verbose=False
                        )
                        
                        # Bìœ í˜• ì„±ëŠ¥ í‰ê°€ (ìƒì„¸)
                        val_pred_b = model_b.predict(X_val_b_clean)
                        test_pred_b = model_b.predict(X_test_b_clean)
                        
                        # ë‹¤ì–‘í•œ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
                        val_r2_b = r2_score(y_val_b_clean, val_pred_b)
                        test_r2_b = r2_score(y_test_b_clean, test_pred_b)
                        
                        val_mse_b = mean_squared_error(y_val_b_clean, val_pred_b)
                        test_mse_b = mean_squared_error(y_test_b_clean, test_pred_b)
                        
                        val_mae_b = mean_absolute_error(y_val_b_clean, val_pred_b)
                        test_mae_b = mean_absolute_error(y_test_b_clean, test_pred_b)
                        
                        # ì˜ˆì¸¡ê°’ ë¶„í¬ ë¶„ì„
                        val_pred_mean = np.mean(val_pred_b)
                        val_pred_std = np.std(val_pred_b)
                        test_pred_mean = np.mean(test_pred_b)
                        test_pred_std = np.std(test_pred_b)
                        
                        # ì‹¤ì œê°’ ë¶„í¬ ë¶„ì„
                        val_actual_mean = np.mean(y_val_b_clean)
                        val_actual_std = np.std(y_val_b_clean)
                        test_actual_mean = np.mean(y_test_b_clean)
                        test_actual_std = np.std(y_test_b_clean)
                        
                        # ìƒê´€ê³„ìˆ˜
                        val_corr_b = np.corrcoef(y_val_b_clean, val_pred_b)[0, 1] if len(y_val_b_clean) > 1 else 0
                        test_corr_b = np.corrcoef(y_test_b_clean, test_pred_b)[0, 1] if len(y_test_b_clean) > 1 else 0
                        
                        # í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„
                        feature_importance = model_b.feature_importances_
                        feature_names = X_train_b_clean.columns.tolist()
                        importance_dict = dict(zip(feature_names, feature_importance))
                        top_features = sorted(importance_dict.items(), key=lambda x: x[1], reverse=True)[:10]
                        
                        fold_result['model_results']['B_entry_model'] = {
                            # ê¸°ë³¸ ì„±ëŠ¥ ì§€í‘œ
                            'val_r2': val_r2_b,
                            'test_r2': test_r2_b,
                            'val_mse': val_mse_b,
                            'test_mse': test_mse_b,
                            'val_mae': val_mae_b,
                            'test_mae': test_mae_b,
                            'val_corr': val_corr_b,
                            'test_corr': test_corr_b,
                            
                            # ë°ì´í„° í¬ê¸°
                            'train_samples': len(X_train_b_clean),
                            'val_samples': len(X_val_b_clean),
                            'test_samples': len(X_test_b_clean),
                            'features_used': len(X_train_b_clean.columns),
                            
                            # ë¶„í¬ ë¶„ì„
                            'val_pred_stats': {'mean': val_pred_mean, 'std': val_pred_std},
                            'test_pred_stats': {'mean': test_pred_mean, 'std': test_pred_std},
                            'val_actual_stats': {'mean': val_actual_mean, 'std': val_actual_std},
                            'test_actual_stats': {'mean': test_actual_mean, 'std': test_actual_std},
                            
                            # í”¼ì²˜ ì¤‘ìš”ë„
                            'top_features': top_features,
                            'all_feature_importance': importance_dict
                        }
                        
                        if verbose:
                            print(f"    âœ… Bìœ í˜• ì„±ëŠ¥ ìƒì„¸:")
                            print(f"      ğŸ“Š RÂ²: Val={val_r2_b:.4f}, Test={test_r2_b:.4f}")
                            print(f"      ğŸ“Š ìƒê´€ê³„ìˆ˜: Val={val_corr_b:.4f}, Test={test_corr_b:.4f}")
                            print(f"      ğŸ“Š MSE: Val={val_mse_b:.2f}, Test={test_mse_b:.2f}")
                            print(f"      ğŸ“Š MAE: Val={val_mae_b:.2f}, Test={test_mae_b:.2f}")
                            print(f"      ğŸ“ˆ ì˜ˆì¸¡ê°’ ë¶„í¬: Val({val_pred_mean:.1f}Â±{val_pred_std:.1f}), Test({test_pred_mean:.1f}Â±{test_pred_std:.1f})")
                            print(f"      ğŸ“ˆ ì‹¤ì œê°’ ë¶„í¬: Val({val_actual_mean:.1f}Â±{val_actual_std:.1f}), Test({test_actual_mean:.1f}Â±{test_actual_std:.1f})")
                            print(f"      ğŸ” ìƒìœ„ í”¼ì²˜: {', '.join([f'{name}({imp:.3f})' for name, imp in top_features[:5]])}")
                    
                    else:
                        fold_result['model_results']['B_entry_model'] = {'error': 'Insufficient clean data'}
                        if verbose:
                            print("    âŒ Bìœ í˜•: ì¶©ë¶„í•œ ê¹¨ë—í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                
                else:
                    fold_result['model_results']['B_entry_model'] = {'error': 'No valid features'}
                    if verbose:
                        print("    âŒ Bìœ í˜•: ìœ íš¨í•œ í”¼ì²˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            except Exception as e:
                fold_result['model_results']['B_entry_model'] = {'error': str(e)}
                if verbose:
                    print(f"    âŒ Bìœ í˜• ì˜¤ë¥˜: {e}")
            
            self.fold_results.append(fold_result)
        
        # ê²°ê³¼ ìš”ì•½ ë° ì €ì¥
        self.print_summary_results(verbose)
        self.save_results(verbose)
        
        return self.fold_results
    
    def print_summary_results(self, verbose=True):
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        if not verbose or not self.fold_results:
            return
        
        print("\n" + "="*80)
        print("ğŸ“Š í•˜ì´ë¸Œë¦¬ë“œ íŠ¸ë ˆì´ë”© AI v5 ê²°ê³¼ ìš”ì•½")
        print("="*80)
        
        # Aìœ í˜• ê²°ê³¼ ìˆ˜ì§‘
        a_val_r2s, a_test_r2s = [], []
        a_val_corrs, a_test_corrs = [], []
        a_val_maes, a_test_maes = [], []
        a_successes = 0
        
        # Bìœ í˜• ê²°ê³¼ ìˆ˜ì§‘  
        b_val_r2s, b_test_r2s = [], []
        b_val_corrs, b_test_corrs = [], []
        b_val_maes, b_test_maes = [], []
        b_successes = 0
        
        # í”¼ì²˜ ì¤‘ìš”ë„ ìˆ˜ì§‘
        a_all_features, b_all_features = [], []
        
        for fold_result in self.fold_results:
            # Aìœ í˜• ì„±ëŠ¥
            if 'A_quality_model' in fold_result['model_results']:
                a_result = fold_result['model_results']['A_quality_model']
                if 'val_r2' in a_result and 'test_r2' in a_result:
                    a_val_r2s.append(a_result['val_r2'])
                    a_test_r2s.append(a_result['test_r2'])
                    a_val_corrs.append(a_result.get('val_corr', 0))
                    a_test_corrs.append(a_result.get('test_corr', 0))
                    a_val_maes.append(a_result.get('val_mae', 0))
                    a_test_maes.append(a_result.get('test_mae', 0))
                    a_all_features.extend(a_result.get('top_features', [])[:5])
                    a_successes += 1
            
            # Bìœ í˜• ì„±ëŠ¥
            if 'B_entry_model' in fold_result['model_results']:
                b_result = fold_result['model_results']['B_entry_model']
                if 'val_r2' in b_result and 'test_r2' in b_result:
                    b_val_r2s.append(b_result['val_r2'])
                    b_test_r2s.append(b_result['test_r2'])
                    b_val_corrs.append(b_result.get('val_corr', 0))
                    b_test_corrs.append(b_result.get('test_corr', 0))
                    b_val_maes.append(b_result.get('val_mae', 0))
                    b_test_maes.append(b_result.get('test_mae', 0))
                    b_all_features.extend(b_result.get('top_features', [])[:5])
                    b_successes += 1
        
        # Aìœ í˜• ìƒì„¸ ìš”ì•½
        if a_successes > 0:
            print(f"ğŸ¯ Aìœ í˜• (í’ˆì§ˆ í‰ê°€) ìƒì„¸ ì„±ëŠ¥:")
            print(f"  ì„±ê³µì ì¸ í´ë“œ: {a_successes}/{len(self.fold_results)}")
            print(f"  ğŸ“Š RÂ²:        Val={np.mean(a_val_r2s):.4f}Â±{np.std(a_val_r2s):.4f}, Test={np.mean(a_test_r2s):.4f}Â±{np.std(a_test_r2s):.4f}")
            print(f"  ğŸ“Š ìƒê´€ê³„ìˆ˜:   Val={np.mean(a_val_corrs):.4f}Â±{np.std(a_val_corrs):.4f}, Test={np.mean(a_test_corrs):.4f}Â±{np.std(a_test_corrs):.4f}")
            print(f"  ğŸ“Š MAE:       Val={np.mean(a_val_maes):.4f}Â±{np.std(a_val_maes):.4f}, Test={np.mean(a_test_maes):.4f}Â±{np.std(a_test_maes):.4f}")
            if len(a_test_r2s) > 1:
                print(f"  ğŸ“Š Test RÂ² ë²”ìœ„: [{np.min(a_test_r2s):.4f}, {np.max(a_test_r2s):.4f}]")
            
            # Aìœ í˜• ì¤‘ìš” í”¼ì²˜ ë¶„ì„
            from collections import Counter
            a_feature_counts = Counter([name for name, _ in a_all_features])
            if a_feature_counts:
                print(f"  ğŸ” í•µì‹¬ í”¼ì²˜: {', '.join([f'{name}({count})' for name, count in a_feature_counts.most_common(5)])}")
        
        # Bìœ í˜• ìƒì„¸ ìš”ì•½  
        if b_successes > 0:
            print(f"\nğŸš€ Bìœ í˜• (ë§¤ìˆ˜ ì‹ í˜¸ AI) ìƒì„¸ ì„±ëŠ¥:")
            print(f"  ì„±ê³µì ì¸ í´ë“œ: {b_successes}/{len(self.fold_results)}")
            print(f"  ğŸ“Š RÂ²:        Val={np.mean(b_val_r2s):.4f}Â±{np.std(b_val_r2s):.4f}, Test={np.mean(b_test_r2s):.4f}Â±{np.std(b_test_r2s):.4f}")
            print(f"  ğŸ“Š ìƒê´€ê³„ìˆ˜:   Val={np.mean(b_val_corrs):.4f}Â±{np.std(b_val_corrs):.4f}, Test={np.mean(b_test_corrs):.4f}Â±{np.std(b_test_corrs):.4f}")
            print(f"  ğŸ“Š MAE:       Val={np.mean(b_val_maes):.2f}Â±{np.std(b_val_maes):.2f}, Test={np.mean(b_test_maes):.2f}Â±{np.std(b_test_maes):.2f}")
            if len(b_test_r2s) > 1:
                print(f"  ğŸ“Š Test RÂ² ë²”ìœ„: [{np.min(b_test_r2s):.4f}, {np.max(b_test_r2s):.4f}]")
            
            # Bìœ í˜• ì¤‘ìš” í”¼ì²˜ ë¶„ì„
            b_feature_counts = Counter([name for name, _ in b_all_features])
            if b_feature_counts:
                print(f"  ğŸ” í•µì‹¬ í”¼ì²˜: {', '.join([f'{name}({count})' for name, count in b_feature_counts.most_common(5)])}")
        
        # ì‹œì¥ í™˜ê²½ë³„ ë¶„ì„
        print(f"\nğŸŒŠ ì‹œì¥ í™˜ê²½ë³„ ì„±ëŠ¥ ë¶„ì„:")
        for i, fold_result in enumerate(self.fold_results):
            market_stats = fold_result['market_stats']
            vix_level = "ì €ë³€ë™" if market_stats['test_vix_mean'] < 25 else "ê³ ë³€ë™"
            return_trend = "ìƒìŠ¹" if market_stats['test_return_mean'] > 0 else "í•˜ë½"
            
            a_r2 = fold_result['model_results'].get('A_quality_model', {}).get('test_r2', 'N/A')
            b_r2 = fold_result['model_results'].get('B_entry_model', {}).get('test_r2', 'N/A')
            
            a_r2_str = f"{a_r2:.4f}" if isinstance(a_r2, (int, float)) else "ì‹¤íŒ¨"
            b_r2_str = f"{b_r2:.4f}" if isinstance(b_r2, (int, float)) else "ì‹¤íŒ¨"
            
            print(f"  Fold {i+1}: {vix_level}/{return_trend} â†’ A:{a_r2_str} / B:{b_r2_str}")
        
        print("="*80)
        print("ğŸ¯ ê°œì„ ëœ í•˜ì´ë¸Œë¦¬ë“œ AI v5 í•µì‹¬ íŠ¹ì§•:")
        print("")
        print("ğŸ“Š Aìœ í˜• (ê±°ë˜ í’ˆì§ˆ ë¶„ì„ê¸°):")
        print("   â€¢ ëª©ì : 'ì´ ê±°ë˜ê°€ ì–¼ë§ˆë‚˜ ì¢‹ì•˜ë‚˜?' ê°ê´€ì  í‰ê°€")
        print("   â€¢ í™œìš©: ê±°ë˜ ë³µê¸°, ì„±ê³¼ ë¶„ì„, íŠ¸ë ˆì´ë” í‰ê°€")
        print("   â€¢ ë°ì´í„°: ëª¨ë“  ê±°ë˜ ì •ë³´ í™œìš© (ì§„ì…+ì§„í–‰+ì¢…ë£Œ)")
        print("   â€¢ ì •í™•ë„: ë†’ìŒ (ì™„ì „í•œ ì •ë³´ í™œìš©)")
        print("")
        print("ğŸš€ Bìœ í˜• (ë§¤ìˆ˜ ì‹ í˜¸ AI):")
        print("   â€¢ ëª©ì : 'ì§€ê¸ˆ ë§¤ìˆ˜í•˜ê¸° ì¢‹ì€ ì¡°ê±´ì¸ê°€?' ì‹¤ì‹œê°„ íŒë‹¨")
        print("   â€¢ í™œìš©: ë§¤ìˆ˜ íƒ€ì´ë°, ì¢…ëª© ì„ ë³„, ë¦¬ìŠ¤í¬ ê´€ë¦¬")
        print("   â€¢ ë°ì´í„°: í˜„ì¬ ì‹œì  ì •ë³´ë§Œ (ë¯¸ë˜ ì •ë³´ ì™„ì „ ì°¨ë‹¨)")
        print("   â€¢ ê°œì„ : ì¢…í•© ë§¤ìˆ˜ ì‹ í˜¸ ì ìˆ˜í™” (ê¸°ìˆ ì +í€ë”ë©˜í„¸+ì‹œì¥í™˜ê²½)")
        print("   â€¢ í”¼ì²˜: 50+ ê°œ ë‹¤ì°¨ì› ë¶„ì„ (ê¸°ì¡´ 10ê°œ â†’ ëŒ€í­ í™•ì¥)")
        print("   â€¢ í˜„ì‹¤ì„±: ë§¤ìš° ë†’ìŒ (ì‹¤ì œ ë§¤ìˆ˜ ì‹ í˜¸ ìƒì„±ê¸°)")
        print("")
        print("ğŸ’¡ v5 í•µì‹¬ ê°œì„ ì :")
        print("   â€¢ Bìœ í˜•: ìˆ˜ìµë¥  ì˜ˆì¸¡ â†’ ë§¤ìˆ˜ ì‹ í˜¸ ì ìˆ˜í™”")
        print("   â€¢ Bìœ í˜•: ë‹¤ì°¨ì› ë¶„ì„ (ê¸°ìˆ ì +í€ë”ë©˜í„¸+ì‹œì¥í™˜ê²½+ì‚°ì—…)")
        print("   â€¢ Bìœ í˜•: í”¼ì²˜ 10ê°œ â†’ 50+ ê°œë¡œ ëŒ€í­ í™•ì¥") 
        print("   â€¢ ì‹¤ì œ íŠ¸ë ˆì´ë”©ì—ì„œ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ë§¤ìˆ˜ ì‹ í˜¸ AI")
        print("="*80)
    
    def save_results(self, verbose=True):
        """ê²°ê³¼ ì €ì¥"""
        
        # ê²°ê³¼ JSON ì €ì¥
        results_filename = 'hybrid_results_v5.json'
        with open(results_filename, 'w') as f:
            json.dump(self.fold_results, f, indent=2, default=str)
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata = {
            'model_version': 'v5',
            'model_name': 'Improved Hybrid Trading AI',
            'created_at': datetime.now().isoformat(),
            'total_folds': len(self.fold_results),
            'successful_a_folds': sum(1 for fr in self.fold_results 
                                    if 'val_r2' in fr['model_results'].get('A_quality_model', {})),
            'successful_b_folds': sum(1 for fr in self.fold_results 
                                    if 'val_r2' in fr['model_results'].get('B_entry_model', {})),
            'improvements': [
                'B-type: Rule-based â†’ Real data-based scoring',
                'B-type: Enhanced technical indicators',
                'More realistic entry condition evaluation'
            ]
        }
        
        metadata_filename = 'hybrid_results_v5_metadata.json'
        with open(metadata_filename, 'w') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        if verbose:
            print(f"âœ… í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ v5 ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {results_filename}, {metadata_filename}")
    
    def run_hybrid_training_with_hyperparameter_tuning(self, data_path, verbose=True):
        """Walk-Forward Foldë³„ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹¤í–‰"""
        if verbose:
            print("ğŸ”„ Foldë³„ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ëª¨ë“œ")
            print("=" * 80)
        
        # ë°ì´í„° ë¡œë“œ
        df = pd.read_csv(data_path)
        df['entry_datetime'] = pd.to_datetime(df['entry_datetime'])
        df['exit_datetime'] = pd.to_datetime(df['exit_datetime'])
        
        if verbose:
            print(f"\nğŸ“Š ì´ ë°ì´í„°: {len(df):,}ê°œ")
            print(f"ğŸ“… ê¸°ê°„: {df['entry_datetime'].min().date()} ~ {df['entry_datetime'].max().date()}")
        
        # í´ë“œ ìƒì„±
        time_folds = self.create_time_folds(df, verbose)
        if len(time_folds) == 0:
            print("âŒ ìƒì„±ëœ í´ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        self.fold_results = []
        
        # ê° í´ë“œë³„ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ìˆ˜í–‰
        for fold_info in time_folds:
            if verbose:
                print(f"\nğŸ”¬ í´ë“œ {fold_info['fold_id']} í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”")
                print(f"  í›ˆë ¨: {fold_info['train_start']} ~ {fold_info['train_end']}")
                print(f"  ê²€ì¦: {fold_info['val_start']} ~ {fold_info['val_end']}")  
                print(f"  í…ŒìŠ¤íŠ¸: {fold_info['test_start']} ~ {fold_info['test_end']}")
            
            # ë°ì´í„° ë¶„í• 
            train_data = df[
                (df['entry_datetime'] >= fold_info['train_start']) & 
                (df['entry_datetime'] < fold_info['train_end'])
            ].copy()
            
            val_data = df[
                (df['entry_datetime'] >= fold_info['val_start']) & 
                (df['entry_datetime'] < fold_info['val_end'])
            ].copy()
            
            test_data = df[
                (df['entry_datetime'] >= fold_info['test_start']) & 
                (df['entry_datetime'] < fold_info['test_end'])
            ].copy()
            
            if len(train_data) < 100 or len(val_data) < 10 or len(test_data) < 10:
                print(f"  âš ï¸ í´ë“œ {fold_info['fold_id']} ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ ìŠ¤í‚µ")
                continue
            
            fold_result = {
                'fold_id': fold_info['fold_id'],
                'fold_info': fold_info,
                'model_results': {},
                'hyperparameter_search': {}
            }
            
            # Aìœ í˜• í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
            try:
                if verbose:
                    print(f"  ğŸ”¬ Aìœ í˜• í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”...")
                
                # Aìœ í˜• í’ˆì§ˆ ì ìˆ˜ ìƒì„±
                train_data_with_score = self.create_a_type_quality_score(train_data, verbose=False)
                val_data_with_score = self.create_a_type_quality_score(val_data, verbose=False)
                test_data_with_score = self.create_a_type_quality_score(test_data, verbose=False)
                
                y_train_a = train_data_with_score['a_type_quality_score']
                y_val_a = val_data_with_score['a_type_quality_score']
                y_test_a = test_data_with_score['a_type_quality_score']
                
                X_train_a = self.prepare_a_type_features(train_data, verbose=False)
                X_val_a = self.prepare_a_type_features(val_data, verbose=False)
                X_test_a = self.prepare_a_type_features(test_data, verbose=False)
                
                # Aìœ í˜• í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (ëŒ€ê·œëª¨ íƒìƒ‰)
                model_a, best_params_a, search_info_a = self.smart_hyperparameter_search(
                    X_train_a, y_train_a, X_val_a, y_val_a, 'A', n_iter=800  # ëŒ€ê·œëª¨ íƒìƒ‰
                )
                
                # Aìœ í˜• ìµœì¢… í‰ê°€
                test_pred_a = model_a.predict(X_test_a)
                test_r2_a = r2_score(y_test_a, test_pred_a)
                
                fold_result['model_results']['A_quality_model'] = {
                    'val_r2': search_info_a['val_r2'],
                    'test_r2': test_r2_a,
                    'train_samples': len(X_train_a),
                    'val_samples': len(X_val_a),
                    'test_samples': len(X_test_a),
                    'features_used': len(X_train_a.columns)
                }
                
                fold_result['hyperparameter_search']['A_type'] = {
                    'best_params': best_params_a,
                    'search_info': search_info_a
                }
                
            except Exception as e:
                print(f"  âŒ Aìœ í˜• ìµœì í™” ì‹¤íŒ¨: {e}")
            
            # Bìœ í˜• í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
            try:
                if verbose:
                    print(f"  ğŸš€ Bìœ í˜• í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”...")
                
                # Bìœ í˜• ì§„ì… ì ìˆ˜ ìƒì„±
                train_data_with_score_b = self.create_b_type_entry_score(train_data, verbose=False)
                val_data_with_score_b = self.create_b_type_entry_score(val_data, verbose=False)
                test_data_with_score_b = self.create_b_type_entry_score(test_data, verbose=False)
                
                y_train_b = train_data_with_score_b['b_type_entry_score']
                y_val_b = val_data_with_score_b['b_type_entry_score']
                y_test_b = test_data_with_score_b['b_type_entry_score']
                
                X_train_b = self.prepare_b_type_features(train_data, verbose=False)
                X_val_b = self.prepare_b_type_features(val_data, verbose=False)  
                X_test_b = self.prepare_b_type_features(test_data, verbose=False)
                
                # Bìœ í˜• í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (ëŒ€ê·œëª¨ íƒìƒ‰)
                model_b, best_params_b, search_info_b = self.smart_hyperparameter_search(
                    X_train_b, y_train_b, X_val_b, y_val_b, 'B', n_iter=800  # ëŒ€ê·œëª¨ íƒìƒ‰
                )
                
                # Bìœ í˜• ìµœì¢… í‰ê°€
                test_pred_b = model_b.predict(X_test_b)
                test_r2_b = r2_score(y_test_b, test_pred_b)
                
                fold_result['model_results']['B_entry_model'] = {
                    'val_r2': search_info_b['val_r2'],
                    'test_r2': test_r2_b,
                    'train_samples': len(X_train_b),
                    'val_samples': len(X_val_b),
                    'test_samples': len(X_test_b),
                    'features_used': len(X_train_b.columns)
                }
                
                fold_result['hyperparameter_search']['B_type'] = {
                    'best_params': best_params_b,
                    'search_info': search_info_b
                }
                
            except Exception as e:
                print(f"  âŒ Bìœ í˜• ìµœì í™” ì‹¤íŒ¨: {e}")
            
            self.fold_results.append(fold_result)
            
            if verbose:
                a_r2 = fold_result['model_results'].get('A_quality_model', {}).get('test_r2', 'N/A')
                b_r2 = fold_result['model_results'].get('B_entry_model', {}).get('test_r2', 'N/A')
                
                # ì•ˆì „í•œ í¬ë§·íŒ…
                a_r2_str = f"{a_r2:.4f}" if isinstance(a_r2, (int, float)) else "ì‹¤íŒ¨"
                b_r2_str = f"{b_r2:.4f}" if isinstance(b_r2, (int, float)) else "ì‹¤íŒ¨"
                print(f"  âœ… í´ë“œ {fold_info['fold_id']} ì™„ë£Œ: A={a_r2_str} / B={b_r2_str}")
        
        # ê²°ê³¼ ì €ì¥ (foldë³„)
        self.save_fold_results()
        
        if verbose:
            print(f"\nğŸ”„ Foldë³„ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì™„ë£Œ!")
            self.print_fold_summary()
        
        return self.fold_results
    
    def print_fold_summary(self):
        """Foldë³„ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        if not self.fold_results:
            print("  ğŸ“Š ìˆ˜ì§‘ëœ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        successful_a = 0
        successful_b = 0
        a_scores = []
        b_scores = []
        
        for fold_result in self.fold_results:
            if 'A_quality_model' in fold_result['model_results']:
                a_r2 = fold_result['model_results']['A_quality_model'].get('test_r2')
                if isinstance(a_r2, (int, float)):
                    successful_a += 1
                    a_scores.append(a_r2)
            
            if 'B_entry_model' in fold_result['model_results']:
                b_r2 = fold_result['model_results']['B_entry_model'].get('test_r2')
                if isinstance(b_r2, (int, float)):
                    successful_b += 1
                    b_scores.append(b_r2)
        
        print(f"  ğŸ“Š Foldë³„ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ìš”ì•½:")
        print(f"    Aìœ í˜• ì„±ê³µ: {successful_a}/{len(self.fold_results)}í´ë“œ")
        print(f"    Bìœ í˜• ì„±ê³µ: {successful_b}/{len(self.fold_results)}í´ë“œ")
        
        if a_scores:
            print(f"    Aìœ í˜• í‰ê·  RÂ²: {np.mean(a_scores):.4f} Â± {np.std(a_scores):.4f}")
        
        if b_scores:
            print(f"    Bìœ í˜• í‰ê·  RÂ²: {np.mean(b_scores):.4f} Â± {np.std(b_scores):.4f}")
    
    def compare_results(self, verbose=True):
        """Foldë³„ vs ì „ì²´ ê¸°ê°„ ê²°ê³¼ ë¹„êµ"""
        
        if not self.fold_results or not self.global_results:
            print("âŒ ë¹„êµí•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        if verbose:
            print("ğŸ“Š Foldë³„ vs ì „ì²´ ê¸°ê°„ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ë¹„êµ")
            print("="*70)
        
        # Foldë³„ í‰ê·  ì„±ëŠ¥
        fold_a_r2s = []
        fold_b_r2s = []
        
        for fold_result in self.fold_results:
            if 'A_quality_model' in fold_result['model_results']:
                fold_a_r2s.append(fold_result['model_results']['A_quality_model']['test_r2'])
            if 'B_entry_model' in fold_result['model_results']:
                fold_b_r2s.append(fold_result['model_results']['B_entry_model']['test_r2'])
        
        # ì „ì²´ ê¸°ê°„ ì„±ëŠ¥
        global_a_r2 = self.global_results['A_type']['test_r2']
        global_b_r2 = self.global_results['B_type']['test_r2']
        
        comparison = {
            'fold_results': {
                'A_type': {
                    'mean_r2': np.mean(fold_a_r2s) if fold_a_r2s else 0,
                    'std_r2': np.std(fold_a_r2s) if len(fold_a_r2s) > 1 else 0,
                    'min_r2': np.min(fold_a_r2s) if fold_a_r2s else 0,
                    'max_r2': np.max(fold_a_r2s) if fold_a_r2s else 0,
                    'n_folds': len(fold_a_r2s)
                },
                'B_type': {
                    'mean_r2': np.mean(fold_b_r2s) if fold_b_r2s else 0,
                    'std_r2': np.std(fold_b_r2s) if len(fold_b_r2s) > 1 else 0,
                    'min_r2': np.min(fold_b_r2s) if fold_b_r2s else 0,
                    'max_r2': np.max(fold_b_r2s) if fold_b_r2s else 0,
                    'n_folds': len(fold_b_r2s)
                }
            },
            'global_results': {
                'A_type': {'test_r2': global_a_r2},
                'B_type': {'test_r2': global_b_r2}
            }
        }
        
        if verbose:
            print("ğŸ”„ Walk-Forward Foldë³„ ì„±ëŠ¥:")
            print(f"  Aìœ í˜•: {np.mean(fold_a_r2s):.4f} Â± {np.std(fold_a_r2s):.4f} (ë²”ìœ„: {np.min(fold_a_r2s):.4f}~{np.max(fold_a_r2s):.4f})")
            print(f"  Bìœ í˜•: {np.mean(fold_b_r2s):.4f} Â± {np.std(fold_b_r2s):.4f} (ë²”ìœ„: {np.min(fold_b_r2s):.4f}~{np.max(fold_b_r2s):.4f})")
            
            print("\nğŸŒ ì „ì²´ ê¸°ê°„ ì„±ëŠ¥:")
            print(f"  Aìœ í˜•: {global_a_r2:.4f}")
            print(f"  Bìœ í˜•: {global_b_r2:.4f}")
            
            print("\nğŸ’¡ ì„±ëŠ¥ ë¹„êµ:")
            a_improvement = global_a_r2 - np.mean(fold_a_r2s) if fold_a_r2s else 0
            b_improvement = global_b_r2 - np.mean(fold_b_r2s) if fold_b_r2s else 0
            
            print(f"  Aìœ í˜• ì „ì²´ê¸°ê°„ ìš°ìœ„: {a_improvement:+.4f}")
            print(f"  Bìœ í˜• ì „ì²´ê¸°ê°„ ìš°ìœ„: {b_improvement:+.4f}")
            
            # ìµœì  íŒŒë¼ë¯¸í„° ë¹„êµ
            print("\nğŸ”§ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
            if self.best_params_a:
                print("  Aìœ í˜• ìµœì  íŒŒë¼ë¯¸í„°:")
                for key, value in list(self.best_params_a.items())[:5]:
                    print(f"    {key}: {value}")
            
            if self.best_params_b:
                print("  Bìœ í˜• ìµœì  íŒŒë¼ë¯¸í„°:")
                for key, value in list(self.best_params_b.items())[:5]:
                    print(f"    {key}: {value}")
        
        # ë¹„êµ ê²°ê³¼ ì €ì¥
        with open('hybrid_results_v5_comparison.json', 'w') as f:
            json.dump(comparison, f, indent=2, default=str)
        
        return comparison
    
    def save_fold_results(self):
        """Foldë³„ ê²°ê³¼ ì €ì¥"""
        with open('hybrid_results_v5_folds.json', 'w') as f:
            json.dump(self.fold_results, f, indent=2, default=str)
    
    def save_global_results(self):
        """ì „ì²´ ê¸°ê°„ ê²°ê³¼ ì €ì¥"""
        if self.global_results:
            # ëª¨ë¸ ê°ì²´ ì œì™¸í•˜ê³  ì €ì¥
            save_data = {
                'split_info': self.global_results['split_info'],
                'A_type': {
                    'best_params': self.global_results['A_type']['best_params'],
                    'search_info': self.global_results['A_type']['search_info'],
                    'test_r2': self.global_results['A_type']['test_r2'],
                    'features_used': self.global_results['A_type']['features_used']
                },
                'B_type': {
                    'best_params': self.global_results['B_type']['best_params'],
                    'search_info': self.global_results['B_type']['search_info'],
                    'test_r2': self.global_results['B_type']['test_r2'],
                    'features_used': self.global_results['B_type']['features_used']
                }
            }
            
            with open('hybrid_results_v5_global.json', 'w') as f:
                json.dump(save_data, f, indent=2, default=str)

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - Foldë³„ + ì „ì²´ ê¸°ê°„ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”"""
    print("ğŸš€ ê°œì„ ëœ í•˜ì´ë¸Œë¦¬ë“œ íŠ¸ë ˆì´ë”© AI v5 - ëŒ€ê·œëª¨ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”")
    print("="*80)
    print("ğŸ“Š ì‹¤í–‰ ìˆœì„œ:")
    print("  1ë‹¨ê³„: Walk-Forward Foldë³„ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”")
    print("  2ë‹¨ê³„: ì „ì²´ ê¸°ê°„ ë¶„í•  í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”")
    print("  3ë‹¨ê³„: ê²°ê³¼ ë¹„êµ ë° ë¶„ì„")
    print("="*80)
    
    # ë°ì´í„° ê²½ë¡œ
    # ì ˆëŒ€ ê²½ë¡œ ì‚¬ìš©ìœ¼ë¡œ íŒŒì¼ ê²½ë¡œ ì˜¤ë¥˜ ë°©ì§€
    import os
    current_dir = os.path.dirname(os.path.abspath(__file__))
    data_path = os.path.join(current_dir, '../results/final/trading_episodes_with_rebuilt_market_component.csv')
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(data_path):
        print(f"âŒ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_path}")
        return
    
    # ëª¨ë¸ ì´ˆê¸°í™” (í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” í™œì„±í™”)
    model = ImprovedHybridTradingAI(use_global_split=True)
    
    print("\nğŸ”„ 1ë‹¨ê³„: Walk-Forward Foldë³„ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘")
    print("="*60)
    fold_results = model.run_hybrid_training_with_hyperparameter_tuning(data_path, verbose=True)
    
    print("\nğŸŒ 2ë‹¨ê³„: ì „ì²´ ê¸°ê°„ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘")  
    print("="*60)
    global_results = model.run_global_hyperparameter_optimization(data_path, verbose=True)
    
    print("\nğŸ“Š 3ë‹¨ê³„: ê²°ê³¼ ë¹„êµ ë¶„ì„")
    print("="*60)
    model.compare_results(verbose=True)
    
    print("\nâœ… ëª¨ë“  ìµœì í™” ì™„ë£Œ!")
    print(f"   ğŸ“ Fold ê²°ê³¼: hybrid_results_v5_folds.json")
    print(f"   ğŸ“ ì „ì²´ ê²°ê³¼: hybrid_results_v5_global.json")
    print(f"   ğŸ“ ë¹„êµ ë¶„ì„: hybrid_results_v5_comparison.json")
    print(f"   ğŸ“Š Aìœ í˜• ëª¨ë¸: ê±°ë˜ í’ˆì§ˆ ë¶„ì„ ì™„ë£Œ")
    print(f"   ğŸš€ Bìœ í˜• ëª¨ë¸: ê°œì„ ëœ ì§„ì… ì¡°ê±´ í‰ê°€ ì™„ë£Œ")
    print(f"   ğŸ¯ ì‹¤ìš©ì ì´ê³  í˜„ì‹¤ì ì¸ íŠ¸ë ˆì´ë”© ì§€ì› ì‹œìŠ¤í…œ ì¤€ë¹„ë¨!")

if __name__ == "__main__":
    main()