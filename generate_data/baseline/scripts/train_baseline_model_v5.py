import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import GridSearchCV
import xgboost as xgb
import joblib
import warnings
warnings.filterwarnings('ignore')

def prepare_features(df):
    """
    V1ê³¼ ë™ì¼í•œ íŠ¹ì„± ì¶”ì¶œ (ì‹œì¥ ì§€í‘œ ì œì™¸)
    """
    
    # V1ì˜ baseline features
    baseline_features = [
        'holding_period_days',
        'position_size_pct',
        'entry_momentum_5d',
        'entry_momentum_20d',
        'entry_volatility_5d', 
        'entry_volatility_20d',
        'entry_ma_dev_5d',
        'entry_ma_dev_20d',
        'entry_vol_change_5d',
        'entry_vol_change_20d',
        'entry_ratio_52w_high'
    ]
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ íŠ¹ì„±ë§Œ ì„ íƒ
    available_features = [f for f in baseline_features if f in df.columns]
    X = df[available_features].copy()
    y = df['label']
    
    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (V1ê³¼ ë™ì¼ - 0ìœ¼ë¡œ ì±„ìš°ê¸°)
    X = X.fillna(0)
    
    # ë°ì´í„° ê²€ì¦
    print(f"âœ… ì„ íƒëœ íŠ¹ì„± ìˆ˜: {len(available_features)}ê°œ")
    print(f"âœ… ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì™„ë£Œ")
    
    return X, y

def apply_percentile_labeling(df, upper_percentile=70, lower_percentile=30):
    """
    ìƒëŒ€ì  í¼ì„¼íƒ€ì¼ ë¼ë²¨ë§ ì ìš©
    ìƒìœ„ 30%ë¥¼ Positive(1), í•˜ìœ„ 30%ë¥¼ Negative(0)ë¡œ ë¼ë²¨ë§
    ì¤‘ê°„ 40%ëŠ” ì œì™¸
    """
    df = df.copy()
    
    # í¼ì„¼íƒ€ì¼ ê¸°ì¤€ê°’ ê³„ì‚°
    upper_threshold = df['return_pct'].quantile(upper_percentile / 100)
    lower_threshold = df['return_pct'].quantile(lower_percentile / 100)
    
    # ë¼ë²¨ ìƒì„± (ì¤‘ê°„ê°’ì€ -1ë¡œ í‘œì‹œ í›„ ì œê±°)
    df['label'] = -1
    df.loc[df['return_pct'] >= upper_threshold, 'label'] = 1  # ìƒìœ„ 30%
    df.loc[df['return_pct'] <= lower_threshold, 'label'] = 0  # í•˜ìœ„ 30%
    
    # ì¤‘ê°„ 40% ì œê±°
    df = df[df['label'] != -1].copy()
    
    print(f"\nğŸ“Š í¼ì„¼íƒ€ì¼ ë¼ë²¨ë§ ì ìš©:")
    print(f"  ìƒìœ„ {100-upper_percentile}% ê¸°ì¤€ê°’: {upper_threshold:.4f}")
    print(f"  í•˜ìœ„ {lower_percentile}% ê¸°ì¤€ê°’: {lower_threshold:.4f}")
    print(f"  Positive ë¹„ìœ¨: {df['label'].mean():.1%}")
    print(f"  ì œì™¸ëœ ì¤‘ê°„ ê±°ë˜: {100-upper_percentile-lower_percentile}%")
    
    return df

def evaluate_trade_quality_model(model, X, y, dataset_name):
    """í‰ê°€ í•¨ìˆ˜"""
    
    y_pred = model.predict(X)
    y_prob = model.predict_proba(X)[:, 1]
    
    # ê¸°ë³¸ ë©”íŠ¸ë¦­
    metrics = {
        'accuracy': accuracy_score(y, y_pred),
        'precision': precision_score(y, y_pred),
        'recall': recall_score(y, y_pred),
        'f1': f1_score(y, y_pred),
        'auc': roc_auc_score(y, y_prob)
    }
    
    print(f"\nğŸ“Š {dataset_name} ì„±ëŠ¥:")
    for metric, value in metrics.items():
        print(f"  {metric}: {value:.4f}")
    
    # Confusion Matrix
    cm = confusion_matrix(y, y_pred)
    print(f"\ní˜¼ë™ í–‰ë ¬:")
    print(f"  TN: {cm[0,0]:,}  FP: {cm[0,1]:,}")
    print(f"  FN: {cm[1,0]:,}  TP: {cm[1,1]:,}")
    
    return metrics, y_prob

def train_trade_quality_baseline():
    """ê°œë³„ ê±°ë˜ í’ˆì§ˆ í‰ê°€ Baseline ëª¨ë¸ í•™ìŠµ - v5 (GridSearch ì¶”ê°€)"""
    
    print("ğŸ¯ ê°œë³„ ê±°ë˜ í’ˆì§ˆ í‰ê°€ ëª¨ë¸ í•™ìŠµ v5 (GridSearch í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹)")
    print("==" * 30)
    
    # 1. ì „ì²´ ë°ì´í„° ë¡œë“œ
    print("\nğŸ“‚ ë°ì´í„° ë¡œë”© ì¤‘...")
    df = pd.read_csv('../../results/final/enriched_trading_episodes_with_fundamentals.csv')
    print(f"ì „ì²´ ë°ì´í„°: {len(df):,}ê°œ")
    
    # 2. í¼ì„¼íƒ€ì¼ ë¼ë²¨ë§ ì ìš© (ìƒìœ„ 30%, í•˜ìœ„ 30%)
    df = apply_percentile_labeling(df)
    
    # 3. Train/Val/Test ë¶„í•  (ì‹œê°„ìˆœ)
    n_total = len(df)
    n_train = int(n_total * 0.7)
    n_val = int(n_total * 0.1)
    
    train_df = df.iloc[:n_train].copy()
    val_df = df.iloc[n_train:n_train+n_val].copy()
    test_df = df.iloc[n_train+n_val:].copy()
    
    print(f"\nğŸ“Š ë°ì´í„° ë¶„í• :")
    print(f"í•™ìŠµìš©: {len(train_df):,}ê°œ ê±°ë˜")
    print(f"ê²€ì¦ìš©: {len(val_df):,}ê°œ ê±°ë˜")
    print(f"í…ŒìŠ¤íŠ¸ìš©: {len(test_df):,}ê°œ ê±°ë˜")
    
    # ë¼ë²¨ ë¶„í¬ í™•ì¸
    print(f"\nğŸ¯ ë¼ë²¨ ë¶„í¬:")
    print(f"í•™ìŠµ: {train_df['label'].mean():.1%}")
    print(f"ê²€ì¦: {val_df['label'].mean():.1%}")
    print(f"í…ŒìŠ¤íŠ¸: {test_df['label'].mean():.1%}")
    
    # 4. Feature ì¤€ë¹„
    print("\nğŸ”§ íŠ¹ì„± ì¤€ë¹„ ì¤‘...")
    X_train, y_train = prepare_features(train_df)
    X_val, y_val = prepare_features(val_df)
    X_test, y_test = prepare_features(test_df)
    
    # 5. ìŠ¤ì¼€ì¼ë§
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)
    X_test_scaled = scaler.transform(X_test)
    
    # 6. GridSearchë¡œ ëª¨ë¸ë³„ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
    print("\nğŸ” GridSearchë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘...")
    
    # GridSearch ì„¤ì •
    cv_folds = 3  # ì‹œê°„ ì ˆì•½ì„ ìœ„í•´ 3-fold
    scoring = 'f1'  # F1 scoreë¡œ ìµœì í™”
    
    models_params = {
        'Logistic Regression': {
            'model': LogisticRegression(random_state=42, max_iter=1000),
            'params': {
                'C': [0.1, 1.0, 10.0],
                'solver': ['liblinear', 'lbfgs'],
                'class_weight': ['balanced']
            },
            'use_scaled': True
        },
        'Random Forest': {
            'model': RandomForestClassifier(random_state=42, n_jobs=-1),
            'params': {
                'n_estimators': [100, 300, 500],
                'max_depth': [10, 15, 20],
                'min_samples_split': [5, 10],
                'min_samples_leaf': [2, 5],
                'class_weight': ['balanced']
            },
            'use_scaled': False
        },
        'XGBoost': {
            'model': xgb.XGBClassifier(random_state=42, scale_pos_weight=sum(y_train==0)/sum(y_train==1)),
            'params': {
                'n_estimators': [100, 300, 500],
                'max_depth': [4, 6, 8],
                'learning_rate': [0.01, 0.1, 0.2],
                'subsample': [0.8, 0.9],
                'colsample_bytree': [0.8, 0.9]
            },
            'use_scaled': False
        }
    }
    
    best_models = {}
    results = {}
    
    for name, config in models_params.items():
        print(f"\n{'='*50}")
        print(f"ğŸ” {name} GridSearch ì§„í–‰ ì¤‘...")
        
        # GridSearch ì‹¤í–‰
        grid_search = GridSearchCV(
            estimator=config['model'],
            param_grid=config['params'],
            cv=cv_folds,
            scoring=scoring,
            n_jobs=-1,
            verbose=1
        )
        
        # ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„° ë˜ëŠ” ì›ë³¸ ë°ì´í„° ì‚¬ìš©
        X_train_use = X_train_scaled if config['use_scaled'] else X_train
        X_val_use = X_val_scaled if config['use_scaled'] else X_val
        X_test_use = X_test_scaled if config['use_scaled'] else X_test
        
        # GridSearch ì‹¤í–‰
        grid_search.fit(X_train_use, y_train)
        
        # ìµœì  ëª¨ë¸
        best_model = grid_search.best_estimator_
        best_models[name] = best_model
        
        print(f"\nğŸ† {name} ìµœì  íŒŒë¼ë¯¸í„°:")
        for param, value in grid_search.best_params_.items():
            print(f"  {param}: {value}")
        
        print(f"\nğŸ“Š êµì°¨ê²€ì¦ ìµœê³  F1 ì ìˆ˜: {grid_search.best_score_:.4f}")
        
        # ê²€ì¦ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ í‰ê°€
        val_metrics, val_prob = evaluate_trade_quality_model(best_model, X_val_use, y_val, "ê²€ì¦ ë°ì´í„°")
        test_metrics, test_prob = evaluate_trade_quality_model(best_model, X_test_use, y_test, "í…ŒìŠ¤íŠ¸ ë°ì´í„°")
        
        results[name] = {
            'cv_score': grid_search.best_score_,
            'best_params': grid_search.best_params_,
            'val_metrics': val_metrics,
            'test_metrics': test_metrics,
            'model': best_model,
            'use_scaled': config['use_scaled']
        }
        
        # Feature Importance (tree ê¸°ë°˜ ëª¨ë¸ë§Œ)
        if hasattr(best_model, 'feature_importances_'):
            print(f"\nğŸ“Š ì¤‘ìš” íŠ¹ì„± ìˆœìœ„:")
            feature_importance = pd.DataFrame({
                'feature': X_train.columns,
                'importance': best_model.feature_importances_
            }).sort_values('importance', ascending=False)
            
            for idx, row in feature_importance.head(5).iterrows():
                print(f"  {row['feature']}: {row['importance']:.4f}")
    
    # 7. ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ íƒ
    best_model_name = max(results.keys(), key=lambda k: results[k]['val_metrics']['f1'])
    best_result = results[best_model_name]
    
    print(f"\n{'='*60}")
    print(f"ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model_name}")
    print(f"   êµì°¨ê²€ì¦ F1: {best_result['cv_score']:.4f}")
    print(f"   ê²€ì¦ F1: {best_result['val_metrics']['f1']:.4f}")
    print(f"   í…ŒìŠ¤íŠ¸ F1: {best_result['test_metrics']['f1']:.4f}")
    
    # 8. ëª¨ë“  ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
    print(f"\nğŸ“Š ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ (í…ŒìŠ¤íŠ¸ ë°ì´í„°):")
    print(f"{'Model':<20} {'F1':<8} {'AUC':<8} {'Precision':<10} {'Recall':<8}")
    print("-" * 55)
    for name, result in results.items():
        metrics = result['test_metrics']
        print(f"{name:<20} {metrics['f1']:<8.4f} {metrics['auc']:<8.4f} {metrics['precision']:<10.4f} {metrics['recall']:<8.4f}")
    
    # 9. ê±°ë˜ í’ˆì§ˆ ì ìˆ˜ ë¶„í¬ ë¶„ì„
    best_model = best_result['model']
    use_scaled = best_result['use_scaled']
    X_test_final = X_test_scaled if use_scaled else X_test
    
    print(f"\nğŸ“ˆ ê±°ë˜ í’ˆì§ˆ ì ìˆ˜ ë¶„í¬ ({best_model_name}):")
    test_df['quality_score'] = best_model.predict_proba(X_test_final)[:, 1]
    
    # ì ìˆ˜ êµ¬ê°„ë³„ ì‹¤ì œ ìˆ˜ìµë¥ 
    test_df['score_bin'] = pd.qcut(test_df['quality_score'], q=5, labels=['ìµœí•˜', 'í•˜', 'ì¤‘', 'ìƒ', 'ìµœìƒ'])
    
    score_analysis = test_df.groupby('score_bin').agg({
        'return_pct': ['mean', 'std', 'count'],
        'label': 'mean'
    })
    
    print("\ní’ˆì§ˆ ì ìˆ˜ë³„ ì‹¤ì œ ì„±ê³¼:")
    print(score_analysis)
    
    # 10. ëª¨ë¸ ì €ì¥
    import os
    os.makedirs('../models', exist_ok=True)
    
    joblib.dump(best_model, '../models/baseline_model_v5.pkl')
    joblib.dump(scaler, '../models/baseline_scaler_v5.pkl')
    
    # ë©”íƒ€ë°ì´í„° ì €ì¥
    metadata = {
        'model_name': best_model_name,
        'best_params': best_result['best_params'],
        'features': list(X_train.columns),
        'cv_f1': best_result['cv_score'],
        'val_f1': best_result['val_metrics']['f1'],
        'test_f1': best_result['test_metrics']['f1'],
        'use_scaled_data': use_scaled,
        'label_type': 'percentile_30_70',
        'training_date': pd.Timestamp.now().strftime('%Y-%m-%d')
    }
    
    with open('../models/baseline_metadata_v5.json', 'w') as f:
        import json
        json.dump(metadata, f, indent=2)
    
    print(f"\nâœ… ëª¨ë¸ì´ ../models/ í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤")
    print(f"âœ… V5ëŠ” GridSearchë¡œ ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‚¬ìš©")
    
    return results, best_model

if __name__ == "__main__":
    results, model = train_trade_quality_baseline()