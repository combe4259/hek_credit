import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.metrics import classification_report, confusion_matrix
import xgboost as xgb
import joblib
import warnings
warnings.filterwarnings('ignore')

def prepare_features(df):
    """
    V1ê³¼ ë™ì¼í•œ íŠ¹ì„± ì¶”ì¶œ (ì‹œì¥ ì§€í‘œ ì œì™¸)
    """
    
    # V1ì˜ baseline features
    baseline_features = [
        'holding_period_days',
        'position_size_pct',
        'entry_momentum_5d',
        'entry_momentum_20d',
        'entry_volatility_5d', 
        'entry_volatility_20d',
        'entry_ma_dev_5d',
        'entry_ma_dev_20d',
        'entry_vol_change_5d',
        'entry_vol_change_20d',
        'entry_ratio_52w_high'
    ]
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ íŠ¹ì„±ë§Œ ì„ íƒ
    available_features = [f for f in baseline_features if f in df.columns]
    X = df[available_features].copy()
    y = df['label']
    
    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (V1ê³¼ ë™ì¼ - 0ìœ¼ë¡œ ì±„ìš°ê¸°)
    X = X.fillna(0)
    
    # ë°ì´í„° ê²€ì¦
    print(f"âœ… ì„ íƒëœ íŠ¹ì„± ìˆ˜: {len(available_features)}ê°œ")
    print(f"âœ… ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì™„ë£Œ")
    
    return X, y

def apply_percentile_labeling(df, upper_percentile=70, lower_percentile=30):
    """
    ìƒëŒ€ì  í¼ì„¼íƒ€ì¼ ë¼ë²¨ë§ ì ìš©
    ìƒìœ„ 30%ë¥¼ Positive(1), í•˜ìœ„ 30%ë¥¼ Negative(0)ë¡œ ë¼ë²¨ë§
    ì¤‘ê°„ 40%ëŠ” ì œì™¸
    """
    df = df.copy()
    
    # í¼ì„¼íƒ€ì¼ ê¸°ì¤€ê°’ ê³„ì‚°
    upper_threshold = df['return_pct'].quantile(upper_percentile / 100)
    lower_threshold = df['return_pct'].quantile(lower_percentile / 100)
    
    # ë¼ë²¨ ìƒì„± (ì¤‘ê°„ê°’ì€ -1ë¡œ í‘œì‹œ í›„ ì œê±°)
    df['label'] = -1
    df.loc[df['return_pct'] >= upper_threshold, 'label'] = 1  # ìƒìœ„ 30%
    df.loc[df['return_pct'] <= lower_threshold, 'label'] = 0  # í•˜ìœ„ 30%
    
    # ì¤‘ê°„ 40% ì œê±°
    df = df[df['label'] != -1].copy()
    
    print(f"\nğŸ“Š í¼ì„¼íƒ€ì¼ ë¼ë²¨ë§ ì ìš©:")
    print(f"  ìƒìœ„ {100-upper_percentile}% ê¸°ì¤€ê°’: {upper_threshold:.4f}")
    print(f"  í•˜ìœ„ {lower_percentile}% ê¸°ì¤€ê°’: {lower_threshold:.4f}")
    print(f"  Positive ë¹„ìœ¨: {df['label'].mean():.1%}")
    print(f"  ì œì™¸ëœ ì¤‘ê°„ ê±°ë˜: {100-upper_percentile-lower_percentile}%")
    
    return df

def evaluate_trade_quality_model(model, X, y, dataset_name):
    """V1ê³¼ ë™ì¼í•œ í‰ê°€ í•¨ìˆ˜"""
    
    y_pred = model.predict(X)
    y_prob = model.predict_proba(X)[:, 1]
    
    # ê¸°ë³¸ ë©”íŠ¸ë¦­
    metrics = {
        'accuracy': accuracy_score(y, y_pred),
        'precision': precision_score(y, y_pred),
        'recall': recall_score(y, y_pred),
        'f1': f1_score(y, y_pred),
        'auc': roc_auc_score(y, y_prob)
    }
    
    print(f"\nğŸ“Š {dataset_name} ì„±ëŠ¥:")
    for metric, value in metrics.items():
        print(f"  {metric}: {value:.4f}")
    
    # Confusion Matrix
    cm = confusion_matrix(y, y_pred)
    print(f"\ní˜¼ë™ í–‰ë ¬:")
    print(f"  TN: {cm[0,0]:,}  FP: {cm[0,1]:,}")
    print(f"  FN: {cm[1,0]:,}  TP: {cm[1,1]:,}")
    
    # ê±°ë˜ í’ˆì§ˆë³„ ë¶„ì„
    print(f"\nğŸ’° ê±°ë˜ í’ˆì§ˆ ë¶„ì„:")
    true_positive_rate = cm[1,1] / (cm[1,0] + cm[1,1])
    false_positive_rate = cm[0,1] / (cm[0,0] + cm[0,1])
    print(f"  ìƒìœ„ ê±°ë˜ ì •í™•íˆ ì‹ë³„: {true_positive_rate:.1%}")
    print(f"  í•˜ìœ„ ê±°ë˜ë¥¼ ìƒìœ„ë¡œ ì˜ëª» ë¶„ë¥˜: {false_positive_rate:.1%}")
    
    return metrics, y_prob

def train_trade_quality_baseline():
    """ê°œë³„ ê±°ë˜ í’ˆì§ˆ í‰ê°€ Baseline ëª¨ë¸ í•™ìŠµ - v4 (í¼ì„¼íƒ€ì¼ ë¼ë²¨ë§)"""
    
    print("ğŸ¯ ê°œë³„ ê±°ë˜ í’ˆì§ˆ í‰ê°€ ëª¨ë¸ í•™ìŠµ v4 (ìƒëŒ€ì  í¼ì„¼íƒ€ì¼ ë¼ë²¨ë§)")
    print("==" * 25)
    
    # 1. ì „ì²´ ë°ì´í„° ë¡œë“œ
    print("\nğŸ“‚ ë°ì´í„° ë¡œë”© ì¤‘...")
    df = pd.read_csv('../../results/final/enriched_trading_episodes_with_fundamentals.csv')
    print(f"ì „ì²´ ë°ì´í„°: {len(df):,}ê°œ")
    
    # 2. í¼ì„¼íƒ€ì¼ ë¼ë²¨ë§ ì ìš© (ìƒìœ„ 30%, í•˜ìœ„ 30%)
    df = apply_percentile_labeling(df)
    
    # 3. Train/Val/Test ë¶„í•  (ì‹œê°„ìˆœ)
    n_total = len(df)
    n_train = int(n_total * 0.7)
    n_val = int(n_total * 0.1)
    
    train_df = df.iloc[:n_train].copy()
    val_df = df.iloc[n_train:n_train+n_val].copy()
    test_df = df.iloc[n_train+n_val:].copy()
    
    print(f"\nğŸ“Š ë°ì´í„° ë¶„í• :")
    print(f"í•™ìŠµìš©: {len(train_df):,}ê°œ ê±°ë˜")
    print(f"ê²€ì¦ìš©: {len(val_df):,}ê°œ ê±°ë˜")
    print(f"í…ŒìŠ¤íŠ¸ìš©: {len(test_df):,}ê°œ ê±°ë˜")
    
    # ë¼ë²¨ ë¶„í¬ í™•ì¸
    print(f"\nğŸ¯ ë¼ë²¨ ë¶„í¬:")
    print(f"í•™ìŠµ: {train_df['label'].mean():.1%}")
    print(f"ê²€ì¦: {val_df['label'].mean():.1%}")
    print(f"í…ŒìŠ¤íŠ¸: {test_df['label'].mean():.1%}")
    
    # 4. Feature ì¤€ë¹„ (V1ê³¼ ë™ì¼)
    print("\nğŸ”§ íŠ¹ì„± ì¤€ë¹„ ì¤‘ (V1ê³¼ ë™ì¼í•œ ê¸°ìˆ ì  ì§€í‘œ)...")
    X_train, y_train = prepare_features(train_df)
    X_val, y_val = prepare_features(val_df)
    X_test, y_test = prepare_features(test_df)
    
    # 5. ìŠ¤ì¼€ì¼ë§
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)
    X_test_scaled = scaler.transform(X_test)
    
    # 6. ëª¨ë¸ ì •ì˜
    models = {
        'Logistic Regression': LogisticRegression(
            random_state=42, 
            max_iter=1000,
            class_weight='balanced'
        ),
        'Random Forest': RandomForestClassifier(
            n_estimators=100,
            random_state=42,
            n_jobs=-1,
            class_weight='balanced'
        ),
        'XGBoost': xgb.XGBClassifier(
            n_estimators=100,
            random_state=42,
            scale_pos_weight=sum(y_train==0)/sum(y_train==1)
        )
    }
    
    results = {}
    best_val_f1 = 0
    best_model = None
    best_model_name = None
    
    for name, model in models.items():
        print(f"\n{'='*50}")
        print(f"ğŸ¤– {name} í•™ìŠµ ì¤‘...")
        
        # í•™ìŠµ
        if name == 'Logistic Regression':
            model.fit(X_train_scaled, y_train)
            val_metrics, val_prob = evaluate_trade_quality_model(model, X_val_scaled, y_val, "ê²€ì¦ ë°ì´í„°")
            test_metrics, test_prob = evaluate_trade_quality_model(model, X_test_scaled, y_test, "í…ŒìŠ¤íŠ¸ ë°ì´í„°")
        else:
            model.fit(X_train, y_train)
            val_metrics, val_prob = evaluate_trade_quality_model(model, X_val, y_val, "ê²€ì¦ ë°ì´í„°")
            test_metrics, test_prob = evaluate_trade_quality_model(model, X_test, y_test, "í…ŒìŠ¤íŠ¸ ë°ì´í„°")
        
        results[name] = {
            'val_metrics': val_metrics,
            'test_metrics': test_metrics,
            'model': model
        }
        
        # ìµœê³  ëª¨ë¸ ì¶”ì 
        if val_metrics['f1'] > best_val_f1:
            best_val_f1 = val_metrics['f1']
            best_model = model
            best_model_name = name
        
        # Feature Importance (tree ê¸°ë°˜ ëª¨ë¸ë§Œ)
        if hasattr(model, 'feature_importances_'):
            print(f"\nğŸ“Š ì¤‘ìš” íŠ¹ì„± ìˆœìœ„:")
            feature_importance = pd.DataFrame({
                'feature': X_train.columns,
                'importance': model.feature_importances_
            }).sort_values('importance', ascending=False)
            
            for idx, row in feature_importance.head(5).iterrows():
                print(f"  {row['feature']}: {row['importance']:.4f}")
    
    # 7. ìµœì¢… ê²°ê³¼ ìš”ì•½
    print(f"\n{'='*50}")
    print(f"ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model_name}")
    print(f"   ê²€ì¦ F1 ì ìˆ˜: {best_val_f1:.4f}")
    print(f"   í…ŒìŠ¤íŠ¸ F1 ì ìˆ˜: {results[best_model_name]['test_metrics']['f1']:.4f}")
    
    # 8. V1ê³¼ ë¹„êµ
    print(f"\nğŸ“Š V1ê³¼ V4 ë¹„êµ:")
    print(f"  V1: ì ˆëŒ€ ê¸°ì¤€ (return > 0.5%)")
    print(f"  V4: ìƒëŒ€ ê¸°ì¤€ (ìƒìœ„ 50%)")
    print(f"  â†’ ì‹œì¥ ìƒí™©ê³¼ ë¬´ê´€í•˜ê²Œ ì¼ì •í•œ ë¼ë²¨ ë¶„í¬ ìœ ì§€")
    
    # 9. ê±°ë˜ í’ˆì§ˆ ì ìˆ˜ ë¶„í¬ ë¶„ì„
    print(f"\nğŸ“ˆ ê±°ë˜ í’ˆì§ˆ ì ìˆ˜ ë¶„í¬:")
    test_df['quality_score'] = best_model.predict_proba(
        X_test_scaled if best_model_name == 'Logistic Regression' else X_test
    )[:, 1]
    
    # ì ìˆ˜ êµ¬ê°„ë³„ ì‹¤ì œ ìˆ˜ìµë¥ 
    test_df['score_bin'] = pd.qcut(test_df['quality_score'], q=5, labels=['ìµœí•˜', 'í•˜', 'ì¤‘', 'ìƒ', 'ìµœìƒ'])
    
    score_analysis = test_df.groupby('score_bin').agg({
        'return_pct': ['mean', 'std', 'count'],
        'label': 'mean'
    })
    
    print("\ní’ˆì§ˆ ì ìˆ˜ë³„ ì‹¤ì œ ì„±ê³¼:")
    print(score_analysis)
    
    # 10. ëª¨ë¸ ì €ì¥
    import os
    os.makedirs('../models', exist_ok=True)
    
    joblib.dump(best_model, '../models/baseline_model_v4.pkl')
    joblib.dump(scaler, '../models/baseline_scaler_v4.pkl')
    
    # ë©”íƒ€ë°ì´í„° ì €ì¥
    metadata = {
        'model_name': best_model_name,
        'features': list(X_train.columns),
        'val_f1': best_val_f1,
        'test_f1': results[best_model_name]['test_metrics']['f1'],
        'label_type': 'percentile_50',
        'training_date': pd.Timestamp.now().strftime('%Y-%m-%d')
    }
    
    with open('../models/baseline_metadata_v4.json', 'w') as f:
        import json
        json.dump(metadata, f, indent=2)
    
    print(f"\nâœ… ëª¨ë¸ì´ ../models/ í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤")
    print(f"âœ… V4ëŠ” ìƒëŒ€ì  í¼ì„¼íƒ€ì¼ ë¼ë²¨ë§ìœ¼ë¡œ ì•ˆì •ì ì¸ ì„±ëŠ¥ ì œê³µ")
    
    return results, best_model

if __name__ == "__main__":
    results, model = train_trade_quality_baseline()